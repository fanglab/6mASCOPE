
"""
Some useful base classes for writing custom tests of pbsmrtpipe output.
"""

from unittest import SkipTest
import operator as OP
import traceback
import logging
import json
import os

from pbcommand.pb_io.report import load_report_from_json
from pbcommand.models import FileTypes

import pbsmrtpipe.testkit.core.base

from pysiv2.io.entrypoints import EntryPoints
from pysiv2.io.datastore import DataStore
from pysiv2.custom import utils as u

log = logging.getLogger(__name__)


def _file_test_id(task_id, file_type):
    return "{t}_{e}".format(t="_".join(task_id.split(".")), e=file_type.ext)


class TestBase(pbsmrtpipe.testkit.core.base.TestBase):
    # XXX subclasses may need to override this, e.g. if they specifically need
    # to retrieve chunked files
    FORCE_USE_LOCAL_DATASTORE = False

    @classmethod
    def setUpClass(cls):
        super(TestBase, cls).setUpClass()
        cls.entrypoints = EntryPoints.from_job_path(cls.job_dir)
        if cls.service_access_layer is None or cls.FORCE_USE_LOCAL_DATASTORE:
            cls.datastore = DataStore.from_job_path(cls.job_dir)
        else:
            ds = cls.service_access_layer.get_analysis_job_datastore(cls.job_id)
            cls.datastore = DataStore(ds)
            ep = cls.service_access_layer.get_analysis_job_entry_points(
                cls.job_id)
            # FIXME
            #cls.entrypoints = EntryPoints(ep)

    def skip_if_not_services_test(self):
        if self.service_access_layer is None:
            raise SkipTest("Not run through SMRT Link services")


class TestFilesWereGenerated(TestBase):

    """
    This is basically just a template for verifying (and potentially
    validating) that expected files were generated, by means of simple
    subclasses that override TASK_IDS and FILE_TYPES (which must be lists of
    the same length).
    """

    TASK_IDS = []
    FILE_TYPES = []

    class __metaclass__(type):

        def __new__(cls, classname, bases, classdict):
            for task_id, file_type in zip(classdict["TASK_IDS"],
                                          classdict["FILE_TYPES"]):
                test_f = TestFilesWereGenerated.make_func(task_id, file_type)
                func_name = "test_{i}".format(
                    i=_file_test_id(task_id, file_type))
                test_f.__name__ = func_name
                test_f.__doc__ = """\
                Test that the datastore contains a file of type {f} generated
                by task {t}.
                """.format(t=task_id, f=file_type)
                classdict[func_name] = test_f
            return type.__new__(cls, classname, bases, classdict)

    @staticmethod
    def make_func(task_id, file_type):
        def test(self):
            fid = "{t}-{f}".format(t=task_id, f=file_type.file_type_id)
            fn = self.datastore._get_task_file(task_id, file_type.file_type_id)
            self.assertTrue(fn is not None, "missing %s" % fid)
            ftid = _file_test_id(task_id, file_type)
            if hasattr(self, "verify_%s_file" % ftid):
                getattr(self, "verify_%s_file" % ftid)(fn)
            return True
        return test


class TestValuesLoader(TestBase):

    """
    Base class for tests that load a JSON dictionary of expected values.
    """
    JSON_FILE = "test_values.json"
    HAVE_TEST_VALUES = False
    _FAILURE_TRACEBACK = None

    @classmethod
    def setUpClass(cls):
        """
        Load expected values from the test's JSON dictionary.
        This is tolerant of errors, which will be re-raised in setUp().
        """
        super(TestValuesLoader, cls).setUpClass()
        json_file = os.path.join(cls.job_dir, cls.JSON_FILE)
        if not os.path.isfile(json_file):
            json_file = os.path.join(os.getcwd(), cls.JSON_FILE)
            if not os.path.isfile(json_file):
                log.warning("Missing %s" % cls.JSON_FILE)
                return
            else:
                log.info("Using test values in %s" % json_file)
        log.info('about to load test_values')
        try:
            with open(json_file, 'r') as f:
                cls.test_values = u.unicode_to_string(json.load(f))
        except (IOError, OSError, ValueError) as e:
            log.error("Error loading %s: %s" % (json_file, e))
            tb = traceback.format_exc()
            cls._FAILURE_TRACEBACK = "%s:\n%s" % (str(e), tb)
        else:
            cls.HAVE_TEST_VALUES = True

    def setUp(self):
        super(TestValuesLoader, self).setUp()
        if self._FAILURE_TRACEBACK is not None:
            self.fail("setUpClass failed:\n%s" % self._FAILURE_TRACEBACK)


class TestStatisticsBase(TestValuesLoader):

    """
    This base class allows us to very quickly add tests that compare actual
    results against expected values.  The assumption is that a testkit job has
    a JSON file that looks something like this:

    {
        'ccs': {
            'number_of_ccs_reads': 100
        },
        'mapping': {
            'number_of_aligned_reads__ge': 95
        }
    }

    which can specify exact values, or use operator suffixes (inspired by
    Django) for looser comparisons.  It is left to subclasses to specify which
    keys are expected in the METRIC_IDS class attribute, and to extract the
    corresponding metrics from the job results.
    """

    JSON_SCOPE = None
    TEST_ID = None
    METRIC_IDS = []
    DEFAULT_VALUES = {}
    SKIP_TESTS = {} # in case we need to bail out for some reason

    # dynamically define test cases at the time of class definition.  in
    # the vanilla unittest framework this could be deferred until later, but
    # for our pbsmrtpipe tests the new methods need to be visible on import
    class __metaclass__(type):

        def __new__(cls, classname, bases, classdict):
            test_id = classdict['TEST_ID']
            # XXX here follows some hackery to make the docstring correspond to
            # the structure of the JSON file - currently this only affects the
            # report-based tests
            json_scope = test_id
            if classdict.get('JSON_SCOPE', None) is not None:
                json_scope = "{s}.{i}".format(s=classdict['JSON_SCOPE'],
                                              i=test_id)
            else:
                for base_cls in bases:
                    if getattr(base_cls, "JSON_SCOPE", None) is not None:
                        json_scope = "{s}.{i}".format(s=base_cls.JSON_SCOPE,
                                                      i=test_id)
                        break
            for stat_key in classdict['METRIC_IDS']:
                test_f = TestStatisticsBase.make_func(stat_key)
                test_f.__doc__ = """\
            Test that the metric ``{s}.{k}`` is within the range of acceptable
            values specified in ``test_values.json``.  Default behavior is to
            check equality, but this can be
            modified by adding a suffix to the key in ``test_values.json``, for
            example ``{k}__lt`` to specify that the metric should be less than
            the given value.
            """.format(s=json_scope, k=stat_key)
                method_name = "test_%s_%s" % (test_id, stat_key)
                test_f.__name__ = method_name
                classdict[method_name] = test_f
            return type.__new__(cls, classname, bases, classdict)


    @staticmethod
    def make_func(stat_key):
        def test(self):
            if stat_key in self.SKIP_TESTS:
                raise SkipTest(self.SKIP_TESTS[stat_key])
            log.info("Test %s" % stat_key)
            return self._compare_stats(stat_key)
        return test

    @classmethod
    def setUpClass(cls):
        """
        Prepare the test class for use, mainly populating various metrics.
        This is tolerant of errors, which will be re-raised in setUp().
        """
        super(TestStatisticsBase, cls).setUpClass()
        cls.expected_values = {}
        cls.metric_dict = {}
        try:
            cls.getMetrics()
        except Exception as e:
            log.error(str(e))
            if cls._FAILURE_TRACEBACK is None:
                cls._FAILURE_TRACEBACK = traceback.format_exc()
        cls.getTestValues()

    @classmethod
    def getTestValues(cls):
        if cls.HAVE_TEST_VALUES:
            cls.expected_values = cls.test_values.get(cls.TEST_ID, {})

    @classmethod
    def getMetrics(cls):
        """
        Subclasses should implement this function to load the appropriate
        metrics for the run, typically by mining the output files.
        """
        raise NotImplementedError("must implement in subclasses")

    def _get_stat(self, stat_id):
        return self.metric_dict.get(stat_id, None)

    def _expected_values_and_operators(self, stat_id):
        if not self.HAVE_TEST_VALUES:
            raise StopIteration()
        elif stat_id in self.expected_values:
            yield self.expected_values[stat_id], OP.eq
        else:
            n_ops = 0
            for key, value in self.expected_values.iteritems():
                for op in ["lt", "gt", "le", "ge", "eq"]:
                    if key == (stat_id + "__" + op):
                        val = self.expected_values[key]
                        n_ops += 1
                        yield val, getattr(OP, op)
            if n_ops == 0 and stat_id in self.DEFAULT_VALUES:
                yield self.DEFAULT_VALUES[stat_id], OP.eq

    def _compare(self, value, expected, operator, stat_id):
        if isinstance(operator, basestring):
            operator = getattr(OP, operator)
        eqn = "%s .%s. %s" % (value, operator.__name__, expected)
        log.info("Comparing values of %s: %s" % (stat_id, eqn))
        self.assertTrue(operator(value, expected), "%s: ! %s" % (stat_id, eqn))

    def _compare_stats(self, stat_id):
        value = self._get_stat(stat_id)
        n_tested = 0
        for expected, operator in self._expected_values_and_operators(stat_id):
            self._compare(value, expected, operator, stat_id)
            n_tested += 1
        if n_tested == 0:
            raise SkipTest(
                "No expected values found for {s} (actual value: {v})".format(
                    v=value, s=stat_id))


class TestReportStatistics(TestStatisticsBase):

    """
    This base class builds on TestStatisticsBase, using the output of pbreports
    instead of diving into the underlying results.
    """

    JSON_SCOPE = "reports"
    REPORT_ID = None
    TEST_ID = None
    METRIC_IDS = []

    @classmethod
    def getTestValues(cls):
        if cls.HAVE_TEST_VALUES:
            cls.expected_values = cls.test_values.get("reports", {}).get(cls.TEST_ID, {})

    @classmethod
    def getMetrics(cls):
        cls.report = cls.getReport()
        cls.report_stats = cls.report.attributes

    @classmethod
    def getReport(cls):
        if cls.service_access_layer is None:
            return cls.datastore.get_report(cls.REPORT_ID)
        else:
            report_id = cls.REPORT_ID
            if isinstance(cls.REPORT_ID, basestring):
                report_id = set([cls.REPORT_ID])
            # load report from services, not raw file
            for rpt_info in cls.service_access_layer.get_analysis_job_reports(
                cls.job_id):
                file_info = rpt_info['dataStoreFile']
                rpt = load_report_from_json(file_info.path)
                if rpt.id in report_id:
                    return rpt
            raise IOError("Can't find report with ID {i}".format(
                          i=" OR ".join(sorted(list(report_id)))))

    def _get_stat(self, stat_id):
        for stat in self.report_stats:
            if stat.id == stat_id:
                return stat.value
        return super(TestReportStatistics, self)._get_stat(stat_id)

    def _get_table_column(self, table_id, column_id):
        for table in self.report.tables:
            if table.id == table_id:
                for column in table.columns:
                    if column.id == column_id:
                        return column.values
        raise KeyError("Table column {r}.{t}.{c} not found.".format(
            r=self.report.id, t=table_id, c=column_id))

    def _compare_table_column(self, table_id, column_id, operator="eq"):
        stat_id = "{t}.{c}".format(t=table_id, c=column_id)
        # FIXME this is deprecated but some tests still rely on it
        expected_values = self.expected_values.get(column_id, None)
        if expected_values is None:
            # keyed by both table ID and column ID
            expected_values = self.expected_values.get(table_id, {}).get(
                column_id, None)
        if expected_values is None:
            raise SkipTest("No expected values defined")
        report_values = self._get_table_column(table_id, column_id)
        for value, expected in zip(report_values, expected_values):
            self._compare(value, expected, operator, stat_id)


def setUpFindAlignments(self):
    """
    Identify the final AlignmentSet from resequencing jobs and save the
    path to the attribute alignment_file_name.
    """
    self.alignment_file_name = None
    self.consolidated_alignment_file_name = None
    pbalign_files = []
    ALIGNED_DS_TYPES = set([
        FileTypes.DS_ALIGN.file_type_id,
        FileTypes.DS_ALIGN_CCS.file_type_id
    ])
    for file_id, file_info in self.datastore.get_file_dict().iteritems():
        if file_info.is_chunked:
            continue
        if file_info.file_type_id in ALIGNED_DS_TYPES:
            if "consolidate_alignments" in file_info.file_id:
                self.consolidated_alignment_file_name = file_info.path
                break
            # FIXME special case, get rid of this eventually
            elif ("pbalign.tasks.pbalign" in file_info.file_id or
                  "pysiv2.tasks.pbalign_unrolled" in file_info.file_id):
                pbalign_files.append(file_info.path)
    if self.consolidated_alignment_file_name is not None:
        self.alignment_file_name = self.consolidated_alignment_file_name
    elif len(pbalign_files) > 0:
        assert len(pbalign_files) == 1
        self.alignment_file_name = pbalign_files[0]


class TestResequencingOutput(TestBase):

    """Base class for tests that inspect resequencing output"""

    def setUp(self):
        setUpFindAlignments(self)
