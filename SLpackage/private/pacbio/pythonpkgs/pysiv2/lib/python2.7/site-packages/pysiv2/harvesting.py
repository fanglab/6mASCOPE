
"""
Harvest testkit job configs (and associated batch .txt files) to produce
various documents describing tests.
"""

from collections import namedtuple, OrderedDict
import logging
import json
import re
from os import path as op

from pbcore.io import SubreadSet, InvalidDataSetIOError

from pbsmrtpipe.loader import load_all_installed_pipelines

log = logging.getLogger(__name__)

testkit_job = namedtuple("testkit_job", ("job_id", "description", "pipeline",
                                         "entry_points", "pysiv2_tests",
                                         "data_type", "data_stats",
                                         "test_metrics"))
testkit_job_p4 = namedtuple("testkit_job_p4",
                            ("job_id", "description", "pipeline",
                             "entry_points", "pysiv2_tests",
                             "data_type", "data_stats", "test_metrics",
                             "p4_path"))
data_stats = namedtuple("data_stats", ("n_zmws", "n_subreads", "n_bases"))

EID_LABELS = {
    "eid_subread": "SubreadSet",
    "eid_barcode": "BarcodeSet",
    "eid_ref_dataset": "ReferenceSet",
    "eid_hdfsubread": "HdfSubreadSet",
    "eid_ref_fasta": "FASTA",
    "rs_movie_xml": "RSII movie metadata XML"
}

OPS = {
    "eq": "==",
    "gt": ">",
    "ge": ">=",
    "lt": "<",
    "le": "<="
}


def _get_test_metrics(test_values_json):
    test_metrics = []
    if op.isfile(test_values_json):
        with open(test_values_json) as f:
            test_values = json.load(f)
            reports = test_values.get("reports", {})
            for report_id, report in reports.iteritems():
                for metric_id, value in report.iteritems():
                    if not isinstance(value, (int, float)):
                        continue
                    fields = metric_id.split("__")
                    operator = "=="
                    if len(fields) == 2:
                        metric_id = fields[0]
                        operator = OPS[fields[1]]
                    test_metrics.append("{r}.{m} {o} {v}".format(
                                        r=report_id, m=metric_id, o=operator,
                                        v=value))
    return test_metrics


def get_testkit_json_data(file_name):
    """
    Extract test metadata from a testkit_cfg.json file.
    """
    with open(file_name) as json_f:
        cfg = json.load(json_f)
        assert cfg.get('jobType', "pbsmrtpipe") == "pbsmrtpipe"
        entry_points = []
        for ep in cfg['entryPoints']:
            entry_points.append((ep['entryId'], ep['path']))
        tests = []
        for module, test_names in cfg["pythonTests"].iteritems():
            tests.extend(test_names)
        data_type = get_sequencing_chemistry(entry_points)
        pipeline_id = cfg['pipelineId']
        pipelines = load_all_installed_pipelines()
        test_values_json = op.join(op.dirname(file_name), "test_values.json")
        test_metrics = _get_test_metrics(test_values_json)
        return testkit_job(
            cfg['testId'],
            cfg['description'],
            pipelines[pipeline_id].display_name,
            entry_points,
            tests,
            get_sequencing_chemistry(entry_points),
            get_data_stats(entry_points),
            test_metrics)


def make_rst_table(rows, headers=None):
    """
    Construct RST syntax for a generic table.
    """
    _rows = list(rows)
    if headers is not None:
        assert len(headers) == len(rows[0])
        _rows.append(headers)
    widths = [max([len(_rows[j][i]) for j in range(len(_rows))])
              for i in range(len(_rows[0]))]
    format_str = "| " + \
        " | ".join(["%-{:d}s".format(x) for x in widths]) + " |"
    sep_str = "+" + "+".join(["-" * (x + 2) for x in widths]) + "+"
    table = [sep_str]
    if headers is not None:
        table.append(format_str % tuple(headers))
        table.append(re.sub("-", "=", sep_str))
    for row in rows:
        table.append(format_str % tuple(row))
        table.append(sep_str)
    return "\n".join(table)


def get_sequencing_chemistry(entry_points, include_system_type=True):
    """
    Given a list of entry points (eid, path), extract the sequencing chemistry
    (and optionally system name) as a human-readable string.
    """
    chemistries = set()
    is_sequel = is_rsii = False
    for eid, path in entry_points:
        if eid == "eid_subread" and op.isfile(path):
            try:
                with SubreadSet(path) as ds:
                    for bam in ds.resourceReaders():
                        for rg in bam.readGroupTable:
                            chemistries.add(rg.SequencingChemistry)
                            if rg.SequencingChemistry.startswith("S"):
                                is_sequel = True
                            else:
                                is_rsii = True
            except InvalidDataSetIOError as e:
                log.error(e)
                return "INVALID DATASET"
    if len(chemistries) == 0:
        return "NA"
    chemistry_str = "; ".join(sorted(list(chemistries)))
    if include_system_type:
        fmt = "{s} ({c})"
        if is_sequel and is_rsii:
            return fmt.format(s="Mixed", c=chemistry_str)
        elif is_sequel:
            return fmt.format(s="Sequel", c=chemistry_str)
        elif is_rsii:
            return fmt.format(s="RSII", c=chemistry_str)
        else:
            raise ValueError("Can't determine system type for {c}".format(
                             c=chemistry_str))
    return chemistry_str


def get_data_stats(entry_points):
    """
    Get basic metrics for input dataset (assumed to be a SubreadSet).
    """
    for eid, path in entry_points:
        if eid == "eid_subread" and op.isfile(path):
            ds = SubreadSet(path)
            n_zmws = 0
            for bam in ds.resourceReaders():
                n_zmws += len(set(bam.pbi.holeNumber))
            return data_stats(n_zmws, ds.numRecords, ds.totalLength)
    return data_stats("NA", "NA", "NA")


def add_p4_path(p4_root, test, cfg_file):
    """
    Utility function to generate the full Perforce repo path for a config file
    given a prefix path.
    """
    p4_path = p4_root + "/" + re.sub("^\.\/", "", cfg_file)
    fields = list(tuple(test)) + [p4_path]
    return testkit_job_p4(*fields)


def _get_subread_set(entry_points):
    for eid, path in entry_points:
        if eid == "eid_subread" and op.isfile(path):
            return path
    return ""


def format_data_stats(s):
    return "{z} ZMWs, {s} subreads, {b} bases".format(z=s.n_zmws,
                                                      s=s.n_subreads,
                                                      b=s.n_bases)


XLS_HEADERS = ["Sub-System", "Component", "Application", "Data Type",
               "Description",
               "Test Requirements", "Automated", "Test Level", "Pass/Fail",
               "Justification", "Dataset(s)", "Input Size", "Test ID", "Perforce Path"]
LOCK_CELLS = set([0, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13])
COL_WIDTHS = [20, 20, 40, 20, 80, 60, 15, 15, 20, 20, 200, 40, 20, 100]


def write_excel_test_plan(tests, file_name, service_tests=()):
    """
    Generate a test plan acceptable to TPTB in Excel format.
    """
    import xlsxwriter
    service_tests = set(service_tests)
    wb = xlsxwriter.Workbook(file_name)
    ws = wb.add_worksheet("Analysis Tests")
    unlocked = wb.add_format({'locked': 0})
    ws.protect()
    for j, width in enumerate(COL_WIDTHS):
        ws.set_column(j, j, width)
    for j, header in enumerate(XLS_HEADERS):
        ws.write(0, j, header)
    i = 1
    for t in tests:
        def _write_row(row_):
            for j, item in enumerate(row_):
                if not j in LOCK_CELLS:
                    ws.write(i, j, item, unlocked)
                else:
                    ws.write(i, j, item)
        level = "CMDLINE"
        if t.job_id in service_tests:
            level = "SERVICES+CMDLINE"
        row = ["SMRT Link - SA", "End-to-end Pipeline", t.pipeline, t.data_type,
               t.description, "Job runs to completion without errors", "YES",
               level, "", "", _get_subread_set(t.entry_points),
               format_data_stats(t.data_stats), t.job_id, t.p4_path]
        _write_row(row)
        i += 1
        for metric in t.test_metrics:
            row = ["SMRT Link - SA", "End-to-end Pipeline", t.pipeline,
                   t.data_type, t.description,
                   "Reports: {m}".format(m=metric), "YES", level, "", "",
                   _get_subread_set(t.entry_points),
                   format_data_stats(t.data_stats), t.job_id, t.p4_path]
            _write_row(row)
            i += 1
    wb.close()
