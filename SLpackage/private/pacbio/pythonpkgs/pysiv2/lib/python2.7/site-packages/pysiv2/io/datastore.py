import logging
import json
import os

from pbcommand.models.common import FileTypes, DataStore as ds
from pbcommand.pb_io.report import load_report_from_json

log = logging.getLogger(__name__)


class DataStore(object):

    def __init__(self, datastore):
        """ Class to handle [job_path]/workflow/datastore.json """
        self.data = datastore

    @classmethod
    def from_job_path(cls, job_path):
        "Use job_path to resolve datastore.json"
        datastore_path = os.path.join(job_path, 'workflow/datastore.json')
        data = ds.load_from_json(datastore_path)
        return cls(data)

    def get_file_dict(self):
        return self.data.files

    def get_files(self):
        """Returns a dictionary of {file_id:file_path}"""
        return {x['fileId']: x['path'] for x in self.data['files']}

    @property
    def created_at(self):
        return self.data['createdAt']

    @property
    def updated_at(self):
        return self.data['updatedAt']

    @property
    def version(self):
        return self.data['version']

    def _get_file(self, file_id):
        for uuid, file_dict in self.data.files.iteritems():
            if file_id == file_dict.file_id:
                return file_dict.path

    # FIXME mostly redundant with files_by_type_and_source
    def _get_task_file(self, task_id, file_type_id):
        if isinstance(task_id, basestring):
            task_ids = {task_id}
        else:
            task_ids = set(task_id)
        files = set()
        for uuid, file_info in self.data.files.iteritems():
            if file_info.is_chunked:
                continue
            if (file_info.file_id.split("-")[0] in task_ids and
                file_info.file_type_id == file_type_id):
                files.add(file_info.path)
        if len(files) == 0:
            raise KeyError("Can't find file of type {f} from task {t}".format(
                           f=file_type_id, t=task_id))
        elif len(files) > 1:
            raise KeyError("Multiple files of type {f} from task {t}".format(
                           f=file_type_id, t=task_id))
        return list(files)[0]

    @property
    def motifs_csv(self):
        return self._get_task_file("motif_maker.tasks.find_motifs",
                                   FileTypes.CSV.file_type_id)

    @property
    def basemods_gff(self):
        return self._get_task_file("kinetics_tools.tasks.ipd_summary",
                                   FileTypes.GFF.file_type_id)

    def get_report(self, report_id):
        """
        Returns one of the pbreports outputs.  report_id can be a sequence if
        the report lives under multiple names (e.g. for subreads/CCS).
        """
        if isinstance(report_id, basestring):
            report_id = set([report_id])
        for uuid, file_info in self.data.files.iteritems():
            if file_info.file_type_id == FileTypes.REPORT.file_type_id:
                rpt = load_report_from_json(file_info.path)
                if rpt.id in report_id:
                    return rpt
        raise IOError("Can't find report with ID {i}".format(
                      i=" OR ".join(sorted(list(report_id)))))

    def files_by_type_and_source(self, file_type_id, task_name):
        """
        Look for files of the specified type, output by the specified (partial)
        task name or gather task name.  Because some tasks may output more
        than one file of the given type, this is implemented as an iterator.
        The decision of how to distinguish between multiple files (if present)
        is left to the code calling this method.
        """
        if isinstance(task_name, basestring):
            task_names = {task_name}
        else:
            task_names = set(task_name)
        gathered_files = []
        raw_files = []
        for file_id, file_info in self.get_file_dict().iteritems():
            if file_info.is_chunked:
                continue
            if file_info.file_type_id == file_type_id:
                for name in task_names:
                    if name in file_info.file_id:
                        raw_files.append(file_info.path)
        if len(raw_files) > 0:
            for file_name in raw_files:
                yield file_name
