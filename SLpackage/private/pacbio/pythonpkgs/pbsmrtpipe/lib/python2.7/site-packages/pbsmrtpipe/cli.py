from __future__ import absolute_import, division, print_function

import copy
import json
import os
import pprint
import sys
import logging
import time
import traceback
import urlparse
import warnings
from collections import OrderedDict

from pbcommand.utils import compose, setup_logger
from pbcommand.common_options import add_log_debug_option, add_base_options
from pbcommand.cli import get_default_argparser
from pbcommand.models.common import BaseChoiceType # FIXME this should be exposed from models
from pbcommand.validators import validate_file

import pbsmrtpipe
from pbsmrtpipe.core import binding_str_is_entry_id
from pbsmrtpipe.constants import (ENV_PRESET, ENTRY_PREFIX, RX_ENTRY, ENV_TC_DIR,
                                  ENV_CHK_OPT_DIR)
import pbsmrtpipe.constants as GlobalConstants
import pbsmrtpipe.tools.utils as TU
from pbsmrtpipe.exceptions import MalformedEntryStrError
from pbsmrtpipe.models import MetaTask, MetaScatterTask, MetaGatherTask
import pbsmrtpipe.pb_io as IO
import pbsmrtpipe.driver as D
from pbsmrtpipe.tools.diagnostics import (run_diagnostics,
                                          run_simple_diagnostics)

log = logging.getLogger(__name__)
slog = logging.getLogger('status.' + __name__)


def _validate_preset_xml(path):

    _, _, _, pipelines = __dynamically_load_all()

    if os.path.exists(path):
        _ = IO.parse_pipeline_template_xml(os.path.abspath(path), pipelines)
        return os.path.abspath(path)
    raise IOError("Unable to find preset '{f}'".format(f=path))


LOG_LEVELS = ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')
LOG_LEVELS_D = {attr: getattr(logging, attr) for attr in LOG_LEVELS}


def add_log_level_option(p):
    p.add_argument('--log-level',
                   default='INFO',
                   choices=LOG_LEVELS, help="Log LEVEL")
    return p


def _add_template_id_option(p):
    p.add_argument('template_id', type=str, help="Show details of Pipeline Template.")
    return p


def _add_task_id_option(p, help_msg="Show details of registered Task by id."):
    p.add_argument('task_id', type=str, help=help_msg)
    return p


def _add_task_id_run_option(p):
    m = "Run a single task by Tool Contract Id (e.g., pbcommand.tasks.dev_txt_hello). " \
        "Use 'pbsmrtpipe show-tasks' to get a complete list of registered tasks. "
    return _add_task_id_option(p, help_msg=m)


def _validate_dir_or_create(p):
    x = os.path.abspath(os.path.expanduser(p))
    if os.path.exists(x):
        return x
    else:
        os.mkdir(x)
        return x


def _add_output_dir_option(p):
    p.add_argument('-o', '--output-dir', default=os.getcwd(),
                   type=_validate_dir_or_create,
                   help="Path to job output directory. Directory will be created if it does not exist.")
    return p


def _add_entry_point_option(p, help_msg="Entry Points using 'entry_idX:/path/to/file.txt' format."):
    p.add_argument('-e', '--entry', dest="entry_points", required=True,
                   action="append",
                   nargs="+", type=_validate_entry, help=help_msg)
    return p


def _add_preset_xml_option(p):
    p.add_argument('--preset-xml', action="append", type=validate_file,
                   default=[],
                   help="Preset/Option XML file.  This option may be "+
                        "repeated if you have multiple preset files.")
    return p


def _add_preset_json_option(p):
    p.add_argument('--preset-json', action="append", type=validate_file,
                   default=[],
                   help="Preset/Option JSON file.  This option may be "+
                        "repeated if you have multiple preset files.")
    return p


def _add_rc_preset_xml_option(p):
    # This should probably just be removed. It doesn't really provide any real value.
    # Users should unset the ENV var to disable this.
    p.add_argument('--preset-json-rc', type=validate_file,
                   help="Skipping loading preset JSON from ENV var '{x}' and Explicitly load the supplied preset.json".format(x=ENV_PRESET))
    return p


def _add_output_preset_json_option(p):
    p.add_argument('-j', '--output-preset-json', type=str, help="Write pipeline/task preset.json of options.")
    return p


def add_run_show_preset_xml_options(p):
    add_base_options(p, "ERROR")
    p = _add_output_preset_json_option(p)
    return p


def _pretty_registered_pipelines(pipelines, pipeline_type):
    n = len(pipelines)
    if n == 0:
        return ""
    title = "{n} Registered {t} Pipelines (name -> version, id, tags)".format(n=n, t=pipeline_type)
    header = len(title) * "*"

    outs = [header, title, header]
    pad_len = 3

    def get_max(attr_name):
        return max(len(getattr(pipeline, attr_name)) for pipeline in pipelines)

    name_pad = pad_len + get_max("display_name")
    id_pad = pad_len + get_max("idx")
    version_pad = pad_len + get_max("version")

    spipelines = sorted(pipelines, key=lambda x: x.display_name)
    for i, k in enumerate(spipelines):
        sx = [(str(i + 1) + ".").rjust(4),
              k.display_name.ljust(name_pad),
              k.version.ljust(version_pad),
              k.idx.ljust(id_pad),
              ",".join(k.tags)]
        outs.append(" ".join(sx))

    return "\n".join(outs)


def pretty_registered_pipelines(registered_new_pipelines_d, show_all=True):

    internal_tags = ("dev", "internal")

    def _is_internal_pipeline(pipeline):
        return not _is_not_internal_pipeline(pipeline)

    def _is_not_internal_pipeline(pipeline):
        return not any(t in pipeline.tags for t in internal_tags)

    def _pipeline_summary(filter_func, description):
        return _pretty_registered_pipelines([p_ for p_ in registered_new_pipelines_d.values() if filter_func(p_)], description)

    outs = [_pipeline_summary(_is_not_internal_pipeline, "User")]

    if show_all:
        outs.append(_pipeline_summary(_is_internal_pipeline, "Developer/Internal"))

    return "\n\n".join(outs)


def pretty_bindings(bindings):
    entry_points = {i for i, o in bindings if i.startswith(ENTRY_PREFIX)}

    outs = []
    outs.append("**** Entry points ({n}) ****".format(n=len(entry_points)))

    for i, entry_point in enumerate(entry_points):
        outs.append(entry_point)

    max_length = max(len(i) for i, o in bindings)
    pad = 4

    outs.append("")
    outs.append("**** Bindings ({n}) ****".format(n=len(bindings)))

    for i, o in bindings:
        outs.append(" -> ".join([i.rjust(max_length + pad), o]))

    return "\n".join(outs)


def run_show_templates(json_output_dir=None, show_all=False):
    import pbsmrtpipe.loader as L
    from pbsmrtpipe.pb_io import write_pipeline_templates_to_json

    rtasks_d, _, _, pts = L.load_all()

    print(pretty_registered_pipelines(pts, show_all=show_all))
    if not show_all:
        print("Run with --show-all to display (unsupported) developer/internal pipelines")

    if json_output_dir is not None:
        write_pipeline_templates_to_json(pts.values(), rtasks_d, json_output_dir)

    return 0


def add_run_show_templates_options(p):

    add_base_options(p, default_level="ERROR")

    def _to_h(m):
        return "Resolve, Validate and Output Registered pipeline templates to {m} files to output-dir".format(m=m)

    p.add_argument('--output-templates-json', type=str, help=_to_h("JSON"))
    p.add_argument('--show-all', action="store_true", help="Display developer/internal pipelines")

    return p


def _args_run_show_templates(args):
    return run_show_templates(json_output_dir=args.output_templates_json,
                              show_all=args.show_all)


def run_show_template_details(pipeline_id, output_preset_json):

    rtasks, rfiles, operators, pipelines_d = __dynamically_load_all()

    from pbsmrtpipe.pb_io import binding_str_to_task_id_and_instance_id

    # Simple task opt key-value of id->value
    pb_options = {}

    if pipeline_id in pipelines_d:
        pipeline = pipelines_d[pipeline_id]
        print("**** Pipeline Summary ****")
        print("id            : {i}".format(i=pipeline.idx))
        print("version       : {i}".format(i=pipeline.version))
        print("name          : {x}".format(x=pipeline.display_name))
        # print("Schema version: {}".format(pipeline.schema_version))
        if pipeline.tags:
            print("Tags       : {t} ".format(t=",".join(pipeline.tags)))
        print("Description: \n {x}".format(x=pipeline.description.strip()))

        print("")
        print(pretty_bindings(pipeline.all_bindings))

        for b_out, b_in, in pipeline.all_bindings:
                for x in (b_out, b_in):
                    if not binding_str_is_entry_id(x):
                        task_id, _, _ = binding_str_to_task_id_and_instance_id(x)
                        task = rtasks.get(task_id, None)
                        if task is None:
                            log.warn("Unable to load task {x}".format(x=task_id))
                        else:
                            for pb_opt in task.option_schemas:
                                if pb_opt.option_id in pb_options:
                                    continue
                                elif pb_opt.option_id in pipeline.task_options:
                                    x = copy.deepcopy(pb_opt)
                                    value = pipeline.task_options[pb_opt.option_id]
                                    x._default = value # XXX hacky
                                    pb_options[pb_opt.option_id] = x
                                else:
                                    pb_options[pb_opt.option_id] = pb_opt

        warn_msg = "Pipeline {i} has no options.".format(i=pipeline.idx)
        task_options_d = OrderedDict(
            [(k, pb_options[k].default) for k in sorted(pb_options.keys())])

        if pb_options:
            _print_pacbio_options([pb_options[k] for k in sorted(pb_options.keys())])
        else:
            print(warn_msg)

        if isinstance(output_preset_json, str):
            preset_id = "{}preset".format(pipeline_id)
            comment = "pbsmrtpipe show-template-details {i}' -j {o}".format(i=pipeline_id, o=output_preset_json)
            IO.write_simple_preset_json(comment, pipeline.idx, preset_id, task_options_d, {}, output_preset_json)

    else:
        msg = "Unable to find template id '{t}' in registered pipelines. Use the show-templates option to get a list of workflow options.".format(t=pipeline_id)
        log.error(msg)
        print(msg)

    return 0


def _args_run_show_template_details(args):
    return run_show_template_details(args.template_id, args.output_preset_json)


def __dynamically_load_all():
    """ Load the registered tasks and operators

    """
    import pbsmrtpipe.loader as L

    def _f(x):
        """length or None"""
        return "None" if x is None else len(x)

    rtasks, rfile_types, roperators, rpipelines = L.load_all()
    _d = dict(n=_f(rtasks), f=_f(rfile_types), o=_f(roperators), p=_f(rpipelines))
    print("Registry Loaded. Number of ToolContracts:{n} FileTypes:{f} ChunkOperators:{o} Pipelines:{p}".format(**_d))
    return rtasks, rfile_types, roperators, rpipelines


def run_show_tasks():

    r_tasks, _, _, _ = __dynamically_load_all()

    sorted_tasks = sorted(r_tasks.values(), key=lambda x: x.task_id)
    max_id = max(len(t.task_id) for t in sorted_tasks)
    pad = 4
    offset = max_id + pad
    print("Registered ToolContracts ({n})".format(n=len(sorted_tasks)))
    print("")

    def _to_a(klass):
        d = {MetaTask: "", MetaScatterTask: "(scatter)", MetaGatherTask: "(gather)"}
        return d.get(klass, "")

    for i, t in enumerate(sorted_tasks):
        print(" ".join([(str(i + 1) + ".").rjust(4), t.task_id.ljust(offset), _to_a(t) + t.display_name]))

    return 0


def _args_run_show_tasks(args):
    return run_show_tasks()


def _print_option_schemas(option_schemas_d):
    """This is the legacy JSONSchema-ish supported model

    .. note: This will be deprecated.
    """

    def _get_v(oid, s, name):
        return s['properties'][oid][name]

    print("Number of Options {n}".format(n=len(option_schemas_d)))
    if option_schemas_d:
        n = 0
        for opt_id, schema in option_schemas_d.iteritems():
            print("Option #{n} Id: {i}".format(n=n, i=opt_id))
            print("\tDefault     : ", _get_v(opt_id, schema, 'default'))
            print("\tType        : ", _get_v(opt_id, schema, 'type'))
            print("\tDescription : ", _get_v(opt_id, schema, 'description'))
            n += 1
            print("")


def _print_pacbio_options(pacbio_options):
    """
    :type pacbio_options: list[pbcommand.models.BasePacBioOption]
    """
    pad = 15

    def to_s(name, value):
        return name.rjust(pad) + " : {}".format(value)

    def printer(name, value):
        print(to_s(name, value))

    print("Number of Options {n}".format(n=len(pacbio_options)))
    if pacbio_options:
        for i, pb_option in enumerate(pacbio_options):
            n = i + 1
            printer("Option #{n} Id".format(n=n), pb_option.option_id)
            printer("Type Id", pb_option.OPTION_TYPE_ID)
            printer("Default", pb_option.default)
            if isinstance(pb_option, BaseChoiceType):
                printer("Choices", pb_option.choices)
            printer("Display Name", pb_option.name)
            printer("Description", pb_option.description)
            print("")


def run_show_task_details(task_id, output_file=None):

    r_tasks, _, _, _ = __dynamically_load_all()

    meta_task = r_tasks.get(task_id, None)

    sep = "*" * 20

    if meta_task is None:
        raise KeyError("Unable to find Task id '{i}' Use 'show-tasks' option to list available task ids.".format(i=task_id))
    else:
        print(sep)
        print(meta_task.summary())
        print("Option type for MetaTask", type(meta_task.option_schemas))
        _print_pacbio_options(meta_task.option_schemas)

    if output_file is not None:

        # FIXME. I don't understand why this is so sloppy.
        if isinstance(meta_task.option_schemas, dict):
            opts = {str(o.option_id): o.default for o in meta_task.option_schemas}
        elif isinstance(meta_task.option_schemas, list):
            opts = {str(o.option_id): o.default for o in meta_task.option_schemas}
        else:
            raise TypeError("Malformed task {}. schema options expected dict got {}".format(meta_task, type(meta_task.option_schemas)))

        msg = "WARNING. Task {i} has no task options. NO preset was wrote to a preset.xml file.".format(i=task_id)
        if opts:
            pipeline_id = task_id
            preset_id = "{}preset".format(task_id)
            comment = ""
            IO.write_simple_preset_json(comment, pipeline_id, preset_id, opts, {}, output_file)
        else:
            print(msg)

    return 0


def _args_run_show_task_details(args):
    print(args)
    return run_show_task_details(args.task_id, output_file=args.output_preset_json)


def _cli_entry_point_args_to_dict(args_entry_points):
    """Translates the entry points from argparse style to the a dict of

    {e_0:/path/to/f1.txt, e_1:/path/to/f2.txt}

    argparse is kinda stupid, or I don't know how to use the API
    entry_points=[[('entry_idX', 'docs/index.rst')], [('entry_2', 'docs/wf_example.py')]]
    """
    ep_d = {}
    for elist in args_entry_points:
        for k, v in elist:
            if k in ep_d:
                log.info(pprint.pformat(args_entry_points))
                raise ValueError("entry point id '{i}' was given multiple times ".format(i=k))
            ep_d[k] = v
    return ep_d


def _args_run_pipeline(args):

    if args.debug:
        slog.debug(args)

    ep_d = _cli_entry_point_args_to_dict(args.entry_points)

    registered_tasks_d, registered_files_d, chunk_operators, pipelines_d = __dynamically_load_all()

    force_distribute, force_chunk = resolve_dist_chunk_overrides(args)

    # Validate all preset files exist
    preset_xmls = [os.path.abspath(os.path.expandvars(p)) for p in args.preset_xml]
    preset_jsons = [os.path.abspath(os.path.expandvars(p)) for p in args.preset_json]
    if len(preset_xmls) > 0 and len(preset_jsons) > 0:
        warnings.warn("WARNING: mixing JSON and XML presets is not recommended and may lead to undefined behavior; for best results use only JSON presets")
    return D.run_pipeline(pipelines_d, registered_files_d, registered_tasks_d, chunk_operators,
                          args.pipeline_template_xml,
                          ep_d, args.output_dir, preset_jsons, preset_xmls, args.preset_json_rc, args.service_uri,
                          force_distribute=force_distribute, force_chunk_mode=force_chunk, debug_mode=args.debug)


def _validate_entry_id(e):
    m = RX_ENTRY.match(e)
    if m is None:
        msg = "Entry point '{e}' should match pattern {p}".format(e=e, p=RX_ENTRY.pattern)
        raise MalformedEntryStrError(msg)
    else:
        return m.groups()[0]


def _validate_entry(e):
    """
    Validate that entry has the CLI form "entry_id:/path/to/file.txt"

    :raises ValueError, IOError
    """
    if ":" in e:
        x = e.split(":")
        if len(x) == 2:
            entry_id, path = x[0].strip(), x[1].strip()
            px = os.path.abspath(os.path.expanduser(path))
            if os.path.isfile(px):
                return entry_id, px
            else:
                raise IOError("Unable to find path '{p}' for entry id '{i}'".format(p=px, i=entry_id))

    raise ValueError("Invalid entry id '{e}' format. Expected (' -e 'entry_idX:/path/to/file.txt')".format(e=e))


def __validate_json_file(path):
    with open(path) as f:
        _ = json.loads(f.read())
    return path


def _validate_uri(value):
    # little bit of sanity testing
    u = urlparse.urlparse(value)
    msg = "Invalid or unsupported service URI '{i}".format(i=value)
    if u.scheme not in ("http", "https"):
        raise ValueError(msg)
    return value


def _add_webservice_config(p):
    p.add_argument('--service-uri', type=_validate_uri, default=None,
                   help="Remote Webservice URI to send status updates to.")
    return p


def __add_pipeline_parser_options(p):
    """Common options for all running pipelines or tasks"""
    funcs = [TU.add_override_chunked_mode,
             TU.add_override_distribute_option,
             _add_webservice_config,
             _add_rc_preset_xml_option,
             _add_preset_json_option,
             _add_preset_xml_option,
             _add_output_dir_option,
             _add_entry_point_option,
             add_log_debug_option]

    f = compose(*funcs)
    return f(p)


def add_pipline_parser_options(p):
    p.add_argument('pipeline_template_xml', type=validate_file,
                   help="Path to pipeline template XML file.")
    p = __add_pipeline_parser_options(p)
    return p


def add_pipeline_id_parser_options(p):
    p.add_argument('pipeline_id', type=str,
                   help="Registered pipeline id (run show-templates) to show a list of the registered pipelines.")
    p = __add_pipeline_parser_options(p)
    return p


def add_show_template_details_parser_options(p):
    p = _add_template_id_option(p)
    p = _add_output_preset_json_option(p)
    add_base_options(p, "ERROR")
    return p


def _add_entry_point_single_task_option(p):
    m = "Entry Points should be defined as e_{x}:/path/to/file for " \
        "each positional index of the Task. Example, for a task with 2 inputs, -e e_0:/path/to/f1.txt -e e_1:/path/to/f2.txt"
    return _add_entry_point_option(p, help_msg=m)


def add_task_parser_options(p):

    funcs = [
        TU.add_override_chunked_mode,
        TU.add_override_distribute_option,
        _add_webservice_config,
        _add_rc_preset_xml_option,
        _add_preset_xml_option,
        _add_preset_json_option,
        _add_output_dir_option,
        _add_entry_point_single_task_option,
        _add_task_id_run_option,
        add_log_debug_option]

    f = compose(*funcs)
    return f(p)


def resolve_dist_chunk_overrides(args):
    # Assumes add_override_chunk_mode and add_override_distribute_mode options
    # were added
    force_distribute = None
    if args.force_distributed is True:
        force_distribute = True
    if args.local_only is True:
        force_distribute = False

    force_chunk = None
    if args.force_chunk_mode is True:
        force_chunk = True
    if args.disable_chunk_mode is True:
        force_chunk = False

    return force_distribute, force_chunk


def _args_task_runner(args):
    registered_tasks, registered_file_types, chunk_operators, pipelines = __dynamically_load_all()

    # This will return a dict of {e_ix:/path/to/f1.txt, e_iy:/path/to/f1.txt}
    ep_d = _cli_entry_point_args_to_dict(args.entry_points)

    force_distribute, force_chunk = resolve_dist_chunk_overrides(args)
    preset_xmls = [os.path.abspath(os.path.expandvars(p)) for p in args.preset_xml]
    preset_jsons = [os.path.abspath(os.path.expandvars(p)) for p in args.preset_json]
    return D.run_single_task(registered_file_types, registered_tasks, chunk_operators,
                             ep_d, args.task_id, args.output_dir, preset_jsons,
                             preset_xmls, args.preset_json_rc, args.service_uri,
                             force_distribute=force_distribute,
                             force_chunk_mode=force_chunk, debug_mode=args.debug)


def run_show_workflow_level_options(output_json_preset=None, output_xml_preset=None):
    from pbsmrtpipe.pb_io import REGISTERED_WORKFLOW_OPTIONS

    workflow_opts = copy.deepcopy(REGISTERED_WORKFLOW_OPTIONS)

    rc_preset = IO.load_preset_from_env(validate=True)

    if rc_preset is not None:
        log.info("Loaded RC preset from ENV '{}' {}".format(GlobalConstants.ENV_PRESET, rc_preset))
        wopts_d = rc_preset.to_workflow_level_opt().to_dict()
        for k, v in wopts_d.iteritems():
            opt_d = workflow_opts[k]
            log.debug("Overwriting {} = {}".format(k, v))
            opt_d['properties'][k]['default'] = v

    _print_option_schemas(workflow_opts)

    pipeline_id = "default"
    preset_id = "default"

    if output_json_preset is not None:
        IO.write_workflow_presets_json(pipeline_id, preset_id, workflow_opts, output_json_preset)
        log.info("wrote options to {x}".format(x=output_json_preset))

    if output_xml_preset is not None:
        sys.stderr.write("WARNING. XML options will be DEPRECATED. Migrate to JSON format.\n")
        IO.write_schema_workflow_options_to_xml(workflow_opts, output_xml_preset)
        log.info("wrote options to {x}".format(x=output_xml_preset))

    return 0


def _args_run_show_workflow_level_options(args):
    return run_show_workflow_level_options(output_json_preset=args.output_preset_json)


def add_show_task_options(p):
    add_base_options(p, "ERROR")
    p = _add_task_id_option(p)
    p = _add_output_preset_json_option(p)
    return p


def _args_run_pipeline_id(args):

    registered_tasks_d, registered_files_d, chunk_operators, pipelines = __dynamically_load_all()

    if args.pipeline_id not in pipelines:
        raise ValueError("Unable to find pipeline id '{i}'".format(i=args.pipeline_id))

    pipeline = pipelines[args.pipeline_id]

    ep_d = _cli_entry_point_args_to_dict(args.entry_points)

    force_distribute, force_chunk = resolve_dist_chunk_overrides(args)
    preset_xmls = [os.path.abspath(os.path.expandvars(p)) for p in args.preset_xml]
    preset_jsons = [os.path.abspath(os.path.expandvars(p)) for p in args.preset_json]
    if len(preset_xmls) > 0 and len(preset_jsons) > 0:
        warnings.warn("WARNING: mixing JSON and XML presets is not recommended and may lead to undefined behavior; for best results use only JSON presets")
    return D.run_pipeline(pipelines, registered_files_d,
                          registered_tasks_d,
                          chunk_operators,
                          pipeline,
                          ep_d, args.output_dir, preset_jsons, preset_xmls,
                          args.preset_json_rc, args.service_uri,
                          force_distribute=force_distribute,
                          force_chunk_mode=force_chunk)


def _args_run_diagnostics(args):
    f = run_diagnostics
    if args.simple:
        f = run_simple_diagnostics

    precord = IO.parse_pipeline_preset_xml(args.preset_xml)
    wopts = precord.to_workflow_level_opt()

    if wopts.cluster_manager_path is not None and wopts.distributed_mode is True:
        output_dir = os.path.abspath(args.output_dir)
        return f(args.preset_xml, output_dir)
    else:
        log.warning("Cluster mode not enabled. Skipping cluster submission tests")
        return 0


def _args_show_chunk_operator_summary(args):
    import pbsmrtpipe.loader as L
    chunk_operators = L.load_all_installed_chunk_operators()

    if chunk_operators:
        for i, xs in enumerate(chunk_operators.iteritems()):
            op_id, chunk_operator = xs
            print("{i}. Chunk Operator Id: {o}".format(o=op_id, i=i))
            print("  Scatter :")
            print("    Scatter Task: {i} -> Chunk Task: {t}".format(i=chunk_operator.scatter.task_id, t=chunk_operator.scatter.scatter_task_id))
            for si, c in enumerate(chunk_operator.scatter.chunks):
                print("    {i} Task Binding: {t} -> Chunk Key: {k}".format(t=c.task_input, k=c.chunk_key, i=si))
            print("  Gather :")
            for ci, c in enumerate(chunk_operator.gather.chunks):
                print("    {i} Gather Task: {t} ".format(t=c.gather_task_id, i=ci))
                print("      Task Binding: {i} -> Chunk Key: {k} ".format(i=c.task_input, k=c.chunk_key))
    else:
        print("WARNING. No Chunk operators loaded")

    return 0


def _add_required_preset_xml_option(p):
    p.add_argument('preset_xml', type=validate_file, help="Path to Preset XML file.")
    return p


def _add_simple_mode_option(p):
    # Run the Full diagnostics suite
    p.add_argument('--simple', action='store_true',
                   help="Perform full diagnostics tests (e.g., submit test job to cluster).")
    return p


def add_args_run_diagnstic(p):
    _add_required_preset_xml_option(p)
    add_log_debug_option(p)
    _add_output_dir_option(p)
    _add_simple_mode_option(p)
    return p


def custom_subparser_builder(subparser, subparser_id, description, options_func, runner_func, setup_func, shutdown_func, error_handler_func):
    """
    Util to add subparser options

    :param options_func: Function that will add args and options to Parser instance F(subparser) -> None
    :param runner_func: Function to run F(pargs) -> Int (exit-code)
    :param setup_func: Function to run before the exe (pargs, log-instance) -> Unit
    :param shutdown_func: Func to run post execution. F(exit_code, started_at, log-instance) -> Int (exit_code)
    :param error_handler_func: Func to handle unhandled exceptions in runner_func F(exit_code, started_at, log-instance, exception) -> Int (exit code)

    Note, `pargs` in the above func signatures will be the results of p.parse_args()
    """
    p = subparser.add_parser(subparser_id, help=description)
    options_func(p)

    # this is a bit of abusing dicts/python
    p.set_defaults(runner_func=runner_func)
    p.set_defaults(setup_func=setup_func)
    p.set_defaults(shutdown_func=shutdown_func)
    p.set_defaults(error_handler_func=error_handler_func)

    return p


def _extract_log_level_from_pargs(pargs, default_level):
    """
    This is a pile of slop to attempt to extract the
    expected log level from a parsed args.

    Ideally, the parser should consistently add/set
    options such that the determination of the level is the
    expected output.
    """
    # print("Raw pargs {}".format(pargs))
    if hasattr(pargs, 'quiet') and pargs.quiet:
        level = logging.ERROR
    elif hasattr(pargs, 'verbosity') and pargs.verbosity > 0:
        if pargs.verbosity >= 2:
            level = logging.DEBUG
        else:
            level = logging.INFO
    elif hasattr(pargs, 'debug') and pargs.debug:
        level = logging.DEBUG
    elif hasattr(pargs, 'log_level'):
        level = getattr(pargs, 'log_level')
    else:
        level = default_level

    return level


def _default_setup_func_with_level(log_level):
    def _f(pargs, alog):
        level = _extract_log_level_from_pargs(pargs, log_level)
        # print("Got default level {}".format(level))
        setup_logger(None, level=level)
        alog.info("Starting pbsmrtpipe {v}".format(v=pbsmrtpipe.get_version()))
        return None
    return _f


def default_setup_func(pargs, alog):
    return _default_setup_func_with_level(logging.INFO)(pargs, alog)


def default_analysis_setup_func(pargs, alog):
    """
    This is a essentially just a bootstrapping step before the job-dir/logs
    can be created and proper log files (pbsmrtpipe.log, master.log) will
    be setup for this to work with the new global dict setup model would have to
    extended to support adding a custom filter.
    """
    level = _extract_log_level_from_pargs(pargs, logging.INFO)
    # print("Got default level {}".format(level))
    str_formatter = '%(message)s'
    setup_logger(None, level=level, formatter=str_formatter)
    alog.info("Starting pbsmrtpipe {v}".format(v=pbsmrtpipe.get_version()))


def default_setup_func_quiet(pargs, alog):
    return _default_setup_func_with_level(logging.ERROR)(pargs, alog)


def _default_status_message(exit_code, started_at):
    run_time = time.time() - started_at
    _d = dict(r=exit_code, s=run_time, v=pbsmrtpipe.get_version())
    return "Exiting pbsmrtpipe {v} with return code {r} in {s:.2f} sec.".format(**_d)


def _default_exception_to_exit_code(ex, default_exit_code=GlobalConstants.DEFAULT_EXIT_CODE):
    return GlobalConstants.EXCEPTION_TO_EXIT_CODE.get(ex.__class__, default_exit_code)


def default_shutdown_func(exit_code, started_at, alog):
    msg = _default_status_message(exit_code, started_at)
    slog.info(msg)
    return exit_code


def default_error_handler_func(pargs, started_at, alog, ex):
    slog.error("{} {}".format(ex.__class__.__name__, ex))
    rcode = _default_exception_to_exit_code(ex)
    msg = _default_status_message(rcode, started_at)
    slog.info(msg)
    return rcode


def default_parser_handler_func(pargs, started_at, alog, ex):
    """
    The root exception handler for the argparse Parsing layer.

    This enables translating the Exception raised at the argparse
    level to specific Exit codes and returns the "standard" output
    message format.
    """
    rcode = _default_exception_to_exit_code(ex)
    msg = _default_status_message(rcode, started_at)
    _, _, ex_traceback = sys.exc_info()
    tb_lines = traceback.format_exception(ex.__class__, ex, ex_traceback)
    tb_text = ''.join(tb_lines)
    sys.stderr.write(tb_text + "\n")
    sys.stderr.write(msg + "\n")
    return rcode


def base_quiet_options(p):
    return add_base_options(p, "ERROR")


def get_parser():
    desc = "Pbsmrtpipe workflow engine"
    p = get_default_argparser(pbsmrtpipe.get_version(), desc)

    sp = p.add_subparsers(help='commands')

    # There are two main cases here, "analysis" running and simple utils that inspect resources registered
    # to pbsmrtpipe. Hence, defining two util funcs to build the subparsers
    # Note, the options_func and default_setup_func should be consistent to correctly configure the logging
    def builder(subparser_id, description, options_func, runner_func, setup_func=default_analysis_setup_func):
        custom_subparser_builder(sp, subparser_id, description, options_func, runner_func,
                                 setup_func,
                                 default_shutdown_func,
                                 default_error_handler_func)

    def builder_quiet(subparser_id, description, options_func, runner_func):
        custom_subparser_builder(sp, subparser_id, description, options_func, runner_func,
                                 default_setup_func_quiet,
                                 default_shutdown_func,
                                 default_error_handler_func)

    wf_desc = "Run a pipeline using a pipeline template or with explict Bindings and EntryPoints."
    builder('pipeline', wf_desc, add_pipline_parser_options, _args_run_pipeline)

    # Run a pipeline by id
    pipline_id_desc = "Run a registered pipeline by specifying the pipeline id."
    builder('pipeline-id', pipline_id_desc, add_pipeline_id_parser_options, _args_run_pipeline_id)

    builder('task', "Run Task (i.e., ToolContract) by id", add_task_parser_options, _args_task_runner)

    # Show Templates
    desc = "List all pipeline templates. A pipeline 'id' can be referenced in " \
           "your my_pipeline.xml file using '<import-template id=\"pbsmrtpipe.pipelines.my_pipeline_id\" />. This " \
           "can replace the explicit listing of EntryPoints and Bindings."

    builder_quiet('show-templates', desc, add_run_show_templates_options, _args_run_show_templates)

    # Show Template Details
    builder_quiet('show-template-details', "Show details about a specific Pipeline template.",
                  add_show_template_details_parser_options, _args_run_show_template_details)

    # Show Tasks
    show_tasks_desc = "Show completed list of Tasks by id. Use ENV {x} to define a " \
                      "custom directory of tool contracts. These TCs will override " \
                      "the installed TCs (e.g., {x}=/path/to/my-tc-dir/)".format(x=ENV_TC_DIR)

    builder_quiet('show-tasks', show_tasks_desc, base_quiet_options, _args_run_show_tasks)

    # Show Task id details
    desc_task_details = "Show Details of a particular task by id (e.g., 'pbsmrtpipe.tasks.filter_report'). " \
                        "Use 'show-tasks' to get a completed list of registered tasks."

    builder_quiet('show-task-details', desc_task_details, add_show_task_options, _args_run_show_task_details)

    wfo_desc = "Display all workflow level options that can be set in <options /> for preset.json"
    builder_quiet('show-workflow-options', wfo_desc, add_run_show_preset_xml_options, _args_run_show_workflow_level_options)

    diag_desc = "Diagnostic tests of preset.xml and cluster configuration"
    builder('run-diagnostic', diag_desc, add_args_run_diagnstic, _args_run_diagnostics)

    desc_chunk_op_show = "Show a list of loaded chunk operators for Scatter/Gather Tasks. Extend resource loading by " \
                         "exporting ENV var {i}. Example export {i}=/path/to/chunk-operators-xml-dir".format(i=ENV_CHK_OPT_DIR)

    builder_quiet('show-chunk-operators', desc_chunk_op_show, base_quiet_options, _args_show_chunk_operator_summary)

    return p


def main_runner(argv, parser, alog, parser_error_handler=default_parser_handler_func):
    """
    Fundamental interface to pbsmrtpipe CL application.
    """
    started_at = time.time()

    # This will catch the argparse level errors
    try:
        pargs = parser.parse_args(argv)
    except Exception as e:
        exit_code = parser_error_handler({}, started_at, slog, e)
        return exit_code

    # If a custom error handler isn't defined on the parser instance,
    # default to the error handler at the parser level
    try:
        error_handler_func = pargs.error_handler_func
    except AttributeError:
        error_handler_func = default_parser_handler_func

    try:
        setup_func = pargs.setup_func
        runner_func = pargs.runner_func
        shutdown_func = pargs.shutdown_func

        setup_func(pargs, alog)
        raw_exit_code = runner_func(pargs)
        # There's still a bit of friction here depending on
        # when logging gets setup in the runner_func
        exit_code = shutdown_func(raw_exit_code, started_at, alog)
    except Exception as ex:
        exit_code = error_handler_func(pargs, started_at, alog, ex)

    return exit_code


def main(argv=None):

    argv_ = sys.argv if argv is None else argv
    parser = get_parser()

    return main_runner(argv_[1:], parser, log)
