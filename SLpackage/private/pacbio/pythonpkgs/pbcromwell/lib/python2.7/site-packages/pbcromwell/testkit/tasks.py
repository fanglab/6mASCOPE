"""
Module for writing WDL task tests (that actually execute in Cromwell).
"""

from collections import namedtuple
import subprocess
import tempfile
import unittest
import logging
import json
import os.path as op
import os

from pbcromwell.cli import get_wdl_zip
from pbcromwell.client import get_server_status, get_job_metadata, WorkflowError, run_workflow

NOOP = int(os.environ.get('PBCROMWELL_NOOP', 0))
PB_CROMWELL_HOST = os.environ.get("PB_CROMWELL_HOST", None)
PB_CROMWELL_PORT = os.environ.get("PB_CROMWELL_PORT", None)
SERVER_MODE = not None in [PB_CROMWELL_HOST, PB_CROMWELL_PORT]
log = logging.getLogger(__name__)
RetryArgs = namedtuple(
    "RetryArgs", ["retries", "retry_delay", "retry_timeout"])


def _load_json(file_name):
    with open(file_name, "r") as json_in:
        return json.loads(json_in.read())


def absolutize_dataset(file_name,
                       ds_out=None,
                       tmp_out=None,
                       skip_counts=False):
    try:
        from pbcore.io import openDataSet
    except ImportError:
        log.error("Sorry, dataset XML support requires pbcore.")
        raise
    log.info("Opening {f} as a PacBio XML dataset".format(f=file_name))
    ds = openDataSet(file_name, skipCounts=skip_counts)
    if not skip_counts and not ds:
        msg = 'File "{}" does not appear to be a PacBio dataset'.format(
            file_name)
        raise Exception(msg)
    if ds_out is None:
        if tmp_out is None:
            tmp_out = os.getcwd()
        if not op.isdir(tmp_out):
            os.makedirs(tmp_out)
        ds_out = op.abspath(op.join(tmp_out, op.basename(file_name)))
    if op.exists(ds_out):
        log.warn("%s already exists, will be overwritten", ds_out)
        os.remove(ds_out)
    log.info("Saving input with absolute paths to %s", ds_out)
    ds.write(ds_out, relPaths=False)
    return ds_out, ds.datasetType


def _absolutize_if_dataset(fn, tmp_dir):
    if not isinstance(fn, basestring) or " " in fn:
        return fn
    if fn.endswith("set.xml"):
        ds_out = op.join(tmp_dir, op.basename(fn))
        return absolutize_dataset(fn, ds_out=ds_out, skip_counts=True)[0]
    return op.abspath(fn)


def make_workflow_wdl(task_id, task_path, task_inputs, task_outputs, nproc=1):
    def _indent(n, s): return "{p}{s}".format(p=" " * 4 * n, s=s)
    workflow_name = "test_{t}".format(t=task_id)
    workflow_file = workflow_name + ".wdl"
    workflow_inputs = []
    for input_id in task_inputs:
        fields = input_id.split()
        if len(fields) == 1:  # no type name supplied
            workflow_inputs.append(_indent(2, "File {f}".format(f=input_id)))
        else:
            workflow_inputs.append(_indent(2, input_id))
    def to_output(file_id):
        if file_id.startswith("?"):
            return "File? {i} = {t}.{i}".format(i=file_id[1:], t=task_id)
        else:
            return "File {i} = {t}.{i}".format(i=file_id, t=task_id)
    workflow_outputs = [_indent(2, to_output(file_id))
                        for file_id in task_outputs]
    call_inputs = [_indent(3, "{i} = {i}".format(
        i=inp_id.split()[-1])) for inp_id in task_inputs]
    return """
version 1.0

import "%(path)s" as task_lib

workflow %(workflow)s {
    input {
%(inputs)s
    }
    call task_lib.%(id)s {
        input:
%(call_inputs)s,
            nproc = %(nproc)d
    }
    output {
%(outputs)s
    }
}
""" % dict(path=task_path,
           id=task_id,
           workflow=workflow_name,
           inputs="\n".join(workflow_inputs),
           call_inputs=",\n".join(call_inputs),
           nproc=nproc,
           outputs="\n".join(workflow_outputs))


class WdlTaskTestBase(unittest.TestCase):
    TASK_ID = None
    TASK_PATH = None
    TASK_INPUTS_D = {}
    TASK_OUTPUTS = []
    NPROC = 1

    @classmethod
    def setUpClass(cls):
        cls._start_dir = os.getcwd()
        cls._tmp_dir = tempfile.mkdtemp('-pbcromwell')
        os.chdir(cls._tmp_dir)
        if SERVER_MODE:
            log.warn(
                "Running in server mode - note that call caching will not be used!")
            assert get_server_status(PB_CROMWELL_HOST, PB_CROMWELL_PORT,
                                     require_db=False)

    @classmethod
    def tearDownClass(cls):
        os.chdir(cls._start_dir)

    def test_end_to_end(self):
        workflow_id = "test_{t}".format(t=self.TASK_ID)
        workflow_src = make_workflow_wdl(
            self.TASK_ID, self.TASK_PATH, self.TASK_INPUTS_D.keys(), self.TASK_OUTPUTS, nproc=self.NPROC)
        inputs_d = {"{w}.{i}".format(w=workflow_id, i=input_id.split()[-1]): fn
                    for (input_id, fn) in self.TASK_INPUTS_D.iteritems()}
        inputs_d = {k: _absolutize_if_dataset(
            v, self._tmp_dir) for k, v in inputs_d.iteritems()}
        if SERVER_MODE:
            metadata = self.run_server_job(workflow_id, workflow_src, inputs_d)
        else:
            metadata = self.run_cli_job(workflow_id, workflow_src, inputs_d)
        if NOOP:
            log.warning(
                'Skipping "run_after()": PBCROMWELL_NOOP={}'.format(NOOP))
            return
        self.run_after(workflow_id, metadata)

    def run_server_job(self, workflow_id, workflow_src, inputs_d):
        try:
            retry_args = RetryArgs(0, 0, 10)
            return run_workflow(
                host=PB_CROMWELL_HOST,
                port=PB_CROMWELL_PORT,
                workflow_src=workflow_src,
                inputs_d=inputs_d,
                options_d={},
                dependencies_zip=get_wdl_zip(),
                retry_args=retry_args)
        except WorkflowError as e:
            self.fail(str(e))

    def run_cli_job(self, workflow_id, workflow_src, inputs_d):
        wdl_file = workflow_id + ".wdl"
        with open(wdl_file, "w") as wdl_out:
            wdl_out.write(workflow_src)
        log.info("Wrote workflow to %s", op.abspath(wdl_file))
        with open("inputs.json", "w") as settings_out:
            settings_out.write(json.dumps(inputs_d, sort_keys=True))
        log.info("Wrote inputs to %s", op.abspath("inputs.json"))
        args = [
            "cromwell_cli.sh",
            wdl_file,
            "-i", "inputs.json",
            "-m", "metadata.json",
            "--imports", get_wdl_zip()
        ]
        stdout_file = op.join(self._tmp_dir, "stdout")
        stderr_file = op.join(self._tmp_dir, "stderr")
        with open(stdout_file, "w") as stdout:
            with open(stderr_file, "w") as stderr:
                try:
                    log.info('args={}\nstd* in {}/'.format(args, self._tmp_dir))
                    if NOOP:
                        open('metadata.json', "w").write("{}")
                        log.warning(
                            'Skipping service call: PBCROMWELL_NOOP={}'.format(NOOP))
                    else:
                        subprocess.check_call(
                            args, stdout=stdout, stderr=stderr)
                except Exception as e:
                    log.error("Task failed, check %s and %s for details",
                              stdout_file, stderr_file)
                    raise
        self.assertTrue(op.isfile("metadata.json"))
        log.info("Cromwell metadata are in %s", op.abspath("metadata.json"))
        return _load_json("metadata.json")

    def run_after(self, workflow_id, metadata):
        pass
