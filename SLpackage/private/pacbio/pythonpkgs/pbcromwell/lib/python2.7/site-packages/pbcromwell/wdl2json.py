"""
Compatibility layer for generating an old pbsmrtpipe pipeline JSON file from
a Cromwell workflow, for UI applications in SMRT Link.
"""

import logging
import json
import re

from pbcromwell import __version__
from pbcromwell.wdl import parse_workflow
from pbcromwell.constants import *

log = logging.getLogger(__name__)


def wdl_to_pipeline_json(wdl, workflow_defs, option_defs):
    PRIVATE_OPTIONS = {"log_level", "nproc", "max_nchunks", "target_size",
                       "tmp_dir"}
    entry_points = []
    task_options = []
    engine_options = []
    for inp in wdl.inputs:
        if inp.type_name == "File":
            entry_points.append({
                "entryId": inp.name,
                "fileTypeId": ENTRY_POINT_TYPES[inp.name],
                "name": "Entry {i}".format(i=inp.name),
                "optional": not inp.required
            })
        else:
            if inp.name.startswith("internal_"):
                log.warn("Skipping internal parameter %s", inp.name)
                continue
            default_value = inp.value
            if inp.value is None:
                if not inp.name in PRIVATE_OPTIONS:
                    log.warn("Parameter %s has a null default value", inp.name)
                if inp.option_type_id == "string":
                    default_value = ""
                elif inp.option_type_id in ["integer", "float"]:
                    default_value = 0
            option_metadata = option_name = option_description = None
            if option_defs is not None:
                option_metadata = option_defs.get(inp.name)
                if option_metadata is not None:
                    option_name = option_metadata["name"]
                    option_description = option_metadata["description"]
            if option_name is None:
                if not inp.name in PRIVATE_OPTIONS and not inp.name.startswith("internal_"):
                    log.warn("No metadata found for option %s", inp.name)
                option_name = re.sub("_", " ", inp.name.capitalize())
                option_description = "Cromwell workflow option %s" % inp.name
            assert isinstance(option_name, basestring)
            assert isinstance(option_description, basestring)
            opt = {
                "name": option_name,
                "description": option_description,
                "optionTypeId": inp.option_type_id,
                "id": inp.name,
                "default": default_value
            }
            if not inp.name in PRIVATE_OPTIONS:
                task_options.append(opt)
            else:
                engine_options.append(opt)
            if option_metadata is not None and "choices" in option_metadata:
                opt["choices"] = option_metadata["choices"]
                opt["optionTypeId"] = "choice_{t}".format(t=inp.option_type_id)
    base_tags = []
    workflow_name = wdl.name
    if workflow_defs is not None:
        base_tags = workflow_defs[wdl.name]["tags"]
        workflow_name = workflow_defs[wdl.name]["title"]
    return {
        "id": "cromwell.workflows.{w}".format(w=wdl.name),
        "name": workflow_name,
        "description": "Cromwell workflow {n}".format(n=wdl.name),
        "entryPoints": entry_points,
        "options": [],  # engine_options,
        "taskOptions": task_options,
        "tags": base_tags + ["cromwell"],
        "schemaVersion": "2.0.0",
        "_comment": "Automatically generated by pbcromwell.wdl2json",
        "version": __version__
    }


def parse_workflow_defs(json_file):
    with open(json_file, "r") as json_in:
        defs = json.loads(json_in.read())
        return {w["workflowId"]: w for w in defs}


def parse_option_defs(json_file):
    with open(json_file, "r") as json_in:
        defs = json.loads(json_in.read())
        return {o["optionId"]: o for o in defs}


def convert(wdl_sin, workflow_defs=None, option_defs=None):
    """Given WDL, return JSON for pbsmrtpipe.
    """
    workflow = parse_workflow(wdl_sin)
    pipeline = wdl_to_pipeline_json(workflow, workflow_defs, option_defs)
    return json.dumps(pipeline, indent=2, separators=(',', ': '), sort_keys=True)
