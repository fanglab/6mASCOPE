"""
Simple library of Cromwell REST API calls for running and querying workflows
"""

from collections import namedtuple
import functools
import requests
import logging
import uuid
import json
import time
import re
import os.path as op
import os
import sys

log = logging.getLogger(__name__)


class JobStates(object):
    SUBMITTED = "Submitted"
    RUNNING = "Running"
    ABORTED = "Aborted"
    ABORTING = "Aborting"
    FAILED = "Failed"
    SUCCEEDED = "Succeeded"
    ON_HOLD = "On Hold"

    FAILING_JOB_STATES = {"Aborted", "Aborting", "Failed"}
    TERMINAL_JOB_STATES = {"Aborted", "Failed", "Succeeded"}


class WorkflowError(RuntimeError):
    pass


def _get_base_workflow_url(host, port):
    return "http://{h}:{p}/api/workflows/v1".format(h=host, p=port)


def _get_base_engine_url(host, port):
    return "http://{h}:{p}/engine/v1".format(h=host, p=port)


def _get_job_endpoint_url(host, port, job_id, endpoint):
    return "{b}/{u}/{e}".format(b=_get_base_workflow_url(host, port), u=job_id, e=endpoint)


def __unmarshal(api_class, response):
    try:
        return api_class(**(json.loads(response.text)))
    except Exception:
        log.error("Can't unmarshal {r} as {c} object".format(
            r=response.text, c=api_class.__name__))
        raise


JobStatus = namedtuple("JobStatus", ["id", "status"])
_to_job_status = functools.partial(__unmarshal, JobStatus)


def get_server_status(host, port, require_db=True):
    url = "{u}/status".format(u=_get_base_engine_url(host, port))
    log.debug("GET {u}".format(u=url))
    response = requests.get(url)
    log.info(response.text)
    d = json.loads(response.text)
    if require_db:
        return d.get("Engine Database", {}).get("ok", False)
    else:
        return isinstance(d, dict)


RetryArgs = namedtuple(
    "RetryArgs", ["retries", "retry_timeout", "retry_delay"])


def get_engine_options(options_d=None,
                       cache=None,
                       backend=None,
                       queue=None,
                       max_retries=None,
                       log_dir=None):
    if options_d is None:
        options_d = {}
    if isinstance(options_d, str) and op.isfile(options_d):
        with open(options_d) as json_in:
            options_d = json.loads(json_in.read())
    # FIXME this is not ideal
    if backend is not None:
        options_d["backend"] = backend
    if cache is not None:
        options_d.update({
            "write_to_cache": cache,
            "read_from_cache": cache
        })
    if log_dir is not None:
        options_d.update({
            "final_workflow_log_dir": log_dir,
            "final_call_logs_dir": log_dir
        })
    runtime_d = options_d.get("default_runtime_attributes", {})
    # FIXME this simply doesn't work, probable Cromwell bug
    if backend is not None:
        runtime_d["backend"] = backend
    if queue is not None:
        runtime_d["queue_name"] = queue
    if max_retries is not None:
        runtime_d["maxRetries"] = max_retries
    options_d["default_runtime_attributes"] = runtime_d
    return options_d


def submit_workflow(host,
                    port,
                    workflow_src,
                    inputs_d,
                    options_d,
                    dependencies_zip,
                    retry_args):
    url = _get_base_workflow_url(host, port)
    log.debug("Final inputs: {d}".format(d=inputs_d))
    log.debug("Final options: {d}".format(d=options_d))
    files = {
        "workflowSource": ("workflow.wdl",
                           workflow_src,
                           "application/octet-stream"),
        "workflowInputs": ("inputs.json",
                           json.dumps(inputs_d, sort_keys=True),
                           "application/json"),
        "workflowOptions": ("options.json",
                            json.dumps(options_d, sort_keys=True),
                            "application/json"),
    }
    if dependencies_zip is not None:
        files.update({
            "workflowDependencies": ("imports.zip",
                                     open(dependencies_zip, "rb"),
                                     "application/octet-stream")
        })
    tries = 1 + retry_args.retries
    sleep_time = retry_args.retry_delay
    timeout = retry_args.retry_timeout
    r = None
    while tries > 0:
        tries -= 1
        try:
            call = "POST {u}".format(u=url)
            log.debug(call)
            r = requests.request('POST', url, files=files, timeout=timeout)
        except requests.ConnectTimeout:
            log.exception("Timed out calling '{}' after {} seconds. Retries remaining = {}.".format(
                call, timeout, tries))
            if tries > 0:
                log.info("Sleeping {} seconds before retry.".format(sleep_time))
                time.sleep(sleep_time)
                sleep_time *= 2
            else:
                raise
    status = _to_job_status(r)
    log.info("Job {u} now in state {s}".format(u=status.id, s=status.status))
    log.info("Job metadata URL: {u}/{i}/metadata".format(u=url, i=status.id))
    return status


def get_job_status(host, port, job_id):
    url = _get_job_endpoint_url(host, port, job_id, "status")
    log.debug("GET {u}".format(u=url))
    return _to_job_status(requests.get(url))


def abort_job(host, port, job_id):
    url = _get_job_endpoint_url(host, port, job_id, "abort")
    log.debug("POST {u}".format(u=url))
    r = requests.post(url)
    return _to_job_status(r)


def get_job_metadata(host, port, job_id):
    url = _get_job_endpoint_url(host, port, job_id, "metadata?expandSubWorkflows=true")
    log.debug("GET {u}".format(u=url))
    r = requests.get(url)
    return json.loads(r.text)


def get_running_jobs(host, port):
    base_url = _get_base_workflow_url(host, port)
    url = "{b}/query?status=Succeeded".format(b=base_url)
    log.debug("GET {u}".format(u=url))
    return json.loads(requests.get(url).text)

# --- end of single API calls


def log_status_or_fail(s):
    msg = "Job {i} status: {s}".format(i=s.id, s=s.status)
    if s.status in JobStates.FAILING_JOB_STATES:
        log.error(msg)
        return 1
    else:
        log.info(msg)
        return 0


OutputFile = namedtuple("OutputFile", ["file_id", "path"])


def get_output_files(metadata):
    files = []
    for key, value in metadata["outputs"].iteritems():
        if isinstance(value, basestring) and op.isfile(value):
            files.append(OutputFile(key, value))
    return files


def log_output_files(metadata):
    for f in get_output_files(metadata):
        log.info("Output file {i}: {p}".format(i=f.file_id, p=f.path))
    return metadata


def log_job_info(metadata):
    log.info("Job ID: {j}".format(j=metadata["id"]))
    log.info("Output directory: {o}".format(
        o=metadata.get("workflowRoot", "unknown")))
    log.info("Submitted at: {t}".format(t=metadata["submission"]))
    if "start" in metadata:
        log.info("Started at: {t}".format(t=metadata["start"]))
    if "end" in metadata:
        log.info("Ended at: {t}".format(t=metadata["end"]))
    log.info("Current state: {s}".format(s=metadata["status"]))
    return metadata


def log_failures(metadata):
    for failure in metadata.get("failures", []):
        for caused_by in failure.get("causedBy", []):
            log.error(caused_by["message"])
        log.error(failure["message"])
    return metadata


def poll_until_job_complete(host,
                            port,
                            job_id,
                            max_time=sys.maxint,
                            polling_interval=10,
                            abort_on_interrupt=True,
                            log_interval=300,
                            t_wait_initial=10):  # FIXME possible race condition
    """
    Block and poll for a job to reach a terminal state.

    :param host: Hostname of Cromwell server
    :param port: Cromwell server port
    :param job_id: Workflow run UUID
    :param max_time: Maximum wait time in seconds
    :param polling_interval: Wait time in seconds between status checks
    :param abort_on_interrupt: Abort the job if a keyboard interrupt is received
    :param log_interval: Time in seconds between log INFO checkpoints
    :return: Final JobStatus object
    """
    log.info("Polling for job {j} status for up to {m} seconds at {s}-second intervals".format(
        j=job_id, m=max_time, s=polling_interval))
    s = None
    last_status = JobStates.SUBMITTED
    t_start = time.time()
    t_elapsed = 0
    t_log = 0
    try:
        time.sleep(t_wait_initial)  # FIXME
        while t_elapsed < max_time:
            s = get_job_status(host, port, job_id)
            if s.status != last_status:
                log.info("Job state changed to {s}".format(s=s.status))
            last_status = s.status
            if s.status in JobStates.TERMINAL_JOB_STATES:
                log.info("Job {j} finished with status {s} after {t} seconds".format(
                    j=job_id, s=s.status, t=t_elapsed))
                break
            t_elapsed = time.time() - t_start
            if t_elapsed - t_log > log_interval:
                log.info("Polling job {j} (current state: {s})".format(
                         j=job_id, s=s.status))
                t_log = t_elapsed
            time.sleep(polling_interval)
    except KeyboardInterrupt:
        if abort_on_interrupt:
            log.warn("Interrupted, aborting job {j}".format(j=job_id))
            abort_job(host, port, job_id)
        raise
    return s


def poll_until_job_succeeds(host, port, job_id, *args, **kwds):
    """
    Block and poll until a job completes successfully, raising an exception
    if not.  Returns the job metadata.
    """
    result = poll_until_job_complete(host, port, job_id, *args, **kwds)
    md = get_job_metadata(host, port, job_id)
    log_job_info(md)
    log_output_files(md)
    if result.status != JobStates.SUCCEEDED:
        log_failures(md)
        raise WorkflowError("Job {j} failed with state {s}".format(
                            j=result.id, s=result.status))
    return md


def run_workflow(host,
                 port,
                 workflow_src,
                 inputs_d,
                 options_d,
                 dependencies_zip,  # XXX lame...
                 retry_args,
                 max_time=43200,  # 12 hours
                 abort_on_interrupt=True,
                 no_block=False):
    """
    Submit a Cromwell workflow to the server, block until it finishes
    successfully, and return the metadata object
    """
    status = submit_workflow(
        host=host,
        port=port,
        workflow_src=workflow_src,
        inputs_d=inputs_d,
        options_d=options_d,
        dependencies_zip=dependencies_zip,
        retry_args=retry_args)
    if no_block:
        log.info("Exiting without waiting for job to complete")
        return None
    return poll_until_job_succeeds(host, port, status.id,
                                   max_time=max_time,
                                   abort_on_interrupt=abort_on_interrupt)


def wait_for_server(host, port, max_tries=10, sleep_time=2):
    k = 1
    while k <= max_tries:
        try:
            status = get_server_status(host, port)
        except requests.exceptions.ConnectionError:
            log.debug("Attempt {k} failed".format(k=k))
            time.sleep(sleep_time)
            k += 1
        else:
            log.info("Successfully connected")
            return status
    raise requests.exceptions.ConnectionError(
        "Can't connect to server after {t} seconds".format(t=max_tries * sleep_time))
