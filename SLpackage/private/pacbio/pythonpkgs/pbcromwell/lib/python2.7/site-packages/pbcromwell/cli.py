#! python

"""
PacBio command-line utility for running Cromwell workflows, plus advanced
utilities for interacting directly with a Cromwell server.

This tool is primarily designed for running workflows distributed and supported
by PacBio, but it is written to handle any valid WDL source (version 1.0), and
is very flexible in how it takes input.  PacBio workflows are expected to be
found in the directory defined by the SMRT_PIPELINE_BUNDLE_DIR environment
variable, which is automatically defined by the SMRT Link distribution.

Note that this tool does not interact with SMRT Link services; to run
a Cromwell workflow as a SMRT Link job, please use 'pbservice'.  Interaction
with the Cromwell server is primarily intended for developers and power users.

Examples of use:

  Show available PacBio-developed workflows:

    $ pbcromwell --quiet show-workflows

  Show details for a PacBio workflow:

    $ pbcromwell --quiet show-workflow-details pb_ccs

  Generate cromwell.conf with HPC settings:

    $ pbcromwell configure --backend SGE

  Launch a PacBio workflow on the command line:

    $ pbcromwell run pb_ccs -e /path/to/movie.subreadset.xml --nproc 8 --config cromwell.conf
"""

from __future__ import print_function
from collections import OrderedDict
from zipfile import ZipFile
import subprocess
import argparse
import textwrap
import logging
import shutil
import uuid
import json
import re
import os.path as op
import os
import sys
import StringIO

from pbcommand.models.common import FileTypes, MimeTypes, DataStore
from pbcommand.models.legacy import Pipeline
from pbcommand.utils import setup_log as pbcommand_setup_log, Constants as LogConstants, get_dataset_metadata
from pbcommand.cli import get_default_argparser_with_base_opts, pacbio_args_runner

from pbcromwell import __version__
from pbcromwell.wdl import parse_workflow
from pbcromwell.client import (get_job_status,
                               submit_workflow,
                               RetryArgs,
                               abort_job,
                               get_job_metadata,
                               get_running_jobs,
                               JobStates,
                               poll_until_job_succeeds,
                               wait_for_server,
                               get_engine_options)
from pbcromwell.constants import *
# FIXME backwards compatibility temporary workaround, please delete
from pbcromwell.wdl2json import parse_workflow_defs, parse_option_defs, convert

log = logging.getLogger(__name__)

CALL_CACHING_DEFAULT = None
RESOURCES = op.join(op.dirname(op.abspath(__file__)), "resources")
TEMPLATE_CONF = op.join(RESOURCES, "template.conf")
DEFAULT_HOST = os.environ.get("PB_CROMWELL_HOST", "localhost")
DEFAULT_PORT = int(os.environ.get("PB_CROMWELL_PORT", 8000))

class Constants(object):
    BUNDLE_ENV = "SMRT_PIPELINE_BUNDLE_DIR"
    PIPELINES = "resolved-pipeline-templates"
    CROMWELL_PREFIX = "cromwell.workflows."
    MISSING_BUNDLE_ERROR = "Can't find the pipeline resources - make sure the SMRT_PIPELINE_BUNDLE_DIR environment variable is set"
    MISSING_ZIP_ERROR = "Can't find the ZIP file containing workflow dependencies.  Please make sure that wdl.zip has been built and is present in the directory pointed to by the SMRT_PIPELINE_BUNDLE_DIR environment variable"

# FIXME this probably belongs in pbcommand
class UserError(Exception):
    pass


def _get_bundle_dir():
    bundle_dir = os.environ.get(Constants.BUNDLE_ENV, None)
    log.info('{}="{}"'.format(Constants.BUNDLE_ENV, bundle_dir))
    # These exceptions will be converted to warnings by the cli runner, but
    # later steps may fail entirely
    if bundle_dir is None:
        raise UserError(Constants.MISSING_BUNDLE_ERROR)
    return bundle_dir


def get_wdl_zip():
    bundle_dir = _get_bundle_dir()
    wdl_zip = op.join(bundle_dir, "wdl.zip")
    if not op.isfile(wdl_zip):
        raise UserError(Constants.MISSING_ZIP_ERROR)
    return wdl_zip


def _get_wdl_zip_or_none(quiet=False):
    try:
        return get_wdl_zip()
    except UserError as e:
        if not quiet:
            log.error(e)
            log.warn(
                "Continuing without dependencies zip file, but PacBio-provided workflows will probably fail")
        return None


def read_workflow(zip_file, workflow_id):
    fn = "{i}.wdl".format(i=workflow_id)
    try:
        with ZipFile(zip_file, "r") as wdl_zip:
            return wdl_zip.open(fn, "r").read()
    except Exception as exc:
        msg = 'Could not open "{}" in ZipFile "{}"'.format(fn, zip_file)
        log.error(msg)
        raise


def get_workflow_src(zip_file, workflow):
    if op.isfile(workflow):
        log.info('workflow="{}" (ignoring zip_file="{}")'.format(op.abspath(workflow), zip_file))
        return open(workflow).read()
    else:
        if zip_file is None:
            raise IOError(Constants.MISSING_ZIP_ERROR)  # this really is fatal
        log.info('workflow="{}" in zip_file "{}"'.format(
            workflow, op.abspath(zip_file)))
        return read_workflow(zip_file, workflow.split(".")[-1])


def _get_entry_points_dict(entry_points):
    d = {}
    for ep in entry_points:
        fields = ep.split(":")
        file_name = op.abspath(fields[-1])
        entry_id = None
        if len(fields) == 2:
            entry_id = fields[0]
        if file_name.endswith(".xml"):
            dataset_type = get_dataset_metadata(file_name).metatype
            if entry_id is None:
                entry_id = ENTRY_POINT_IDS[dataset_type]
        if entry_id is None:
            raise ValueError(
                "Can't automatically determine entry point ID for file '{f}'".format(f=file_name))
        log.info("Final entry point {i}: {f}".format(i=entry_id, f=file_name))
        d[entry_id] = file_name
    return d


def _get_workflow_name(workflow):
    if not os.path.isfile(workflow):
        log.info("{w} is not a file, so it will be interpeted as a workflow ID to extract from the PacBio-bundled applications")
        return workflow
    with open(workflow) as wdl_in:
        for line in wdl_in.readlines():
            if line.strip().startswith("workflow "):
                try:
                    line = re.sub("{.*", "", line.strip())
                    return line.split()[1]
                except Exception:
                    log.error('In {}, failed to parse line:\n"{}"'.format(
                        workflow, line))
                    raise


def _load_all_pipeline_json():
    bundle_dir = _get_bundle_dir()
    pipeline_dir = op.join(bundle_dir, Constants.PIPELINES)
    pipelines = []
    for file_name in os.listdir(pipeline_dir):
        if file_name.startswith(Constants.CROMWELL_PREFIX) and file_name.endswith(".json"):
            json_file = op.join(pipeline_dir, file_name)
            pipelines.append(Pipeline.load_from_json(json_file))
    return pipelines


def _load_pipeline_by_id(pipeline_id):
    """
    Load an old pbsmrtpipe JSON used by SMRT Link, for display purposes only.
    """
    if pipeline_id.startswith("pbsmrtpipe"):
        raise UserError("pbsmrtpipe pipelines not supported")
    if not pipeline_id.startswith(Constants.CROMWELL_PREFIX):
        pipeline_id = Constants.CROMWELL_PREFIX + pipeline_id
    bundle_dir = _get_bundle_dir()
    pipeline_dir = op.join(bundle_dir, Constants.PIPELINES)
    file_name = op.join(pipeline_dir, pipeline_id + ".json")
    if not op.isfile(file_name):
        raise UserError("Can't find a pipeline JSON at {p}; make sure you have run 'make wdl' in the pbpipeline-resources repo.".format(p=file_name))
    return Pipeline.load_from_json(file_name)


# FIXME this is pretty hacky, we can do better
def _to_value(v):
    if v.isdigit():
        return int(v)
    try:
        return float(v)
    except ValueError:
        if v in ["true", "True"]:
            return True
        elif v in ["false", "False"]:
            return False
        return v


def _get_inputs_dict(args):
    zip_file = _get_wdl_zip_or_none(quiet=True)
    workflow = parse_workflow(StringIO.StringIO(get_workflow_src(zip_file, args.workflow)))
    valid_keys = workflow.valid_input_keys()

    def _to_key(key): return "{n}.{k}".format(n=workflow.name, k=key)
    d = {}
    if args.inputs is not None:
        with open(args.inputs) as inputs:
            d.update(json.loads(inputs.read()))
    eps = _get_entry_points_dict(args.entry_points)
    for eid, path in eps.iteritems():
        d[_to_key(eid)] = path
    if args.max_nchunks is not None and "max_nchunks" in valid_keys:
        d[_to_key("max_nchunks")] = args.max_nchunks
    elif args.target_size is not None and "target_size" in valid_keys:
        d[_to_key("target_size")] = args.target_size
    if "nproc" in valid_keys:
        d[_to_key("nproc")] = args.nproc
    if args.tmp_dir is not None and "tmp_dir" in valid_keys:
        d[_to_key("tmp_dir")] = args.tmp_dir
    for option in args.task_options:
        key, value = option.split("=")
        if not key in valid_keys:
            log.warn("Can't find key '{k}' in workflow source.  This is very likely to cause a fatal error in Cromwell")
        d[_to_key(key)] = _to_value(value)
    return d


def _get_options_dict(args, log_dir=None):
    return get_engine_options(
        options_d=args.options,
        cache=getattr(args, "cache", None),
        backend=args.backend,
        queue=args.queue,
        max_retries=args.max_retries,
        log_dir=log_dir)


def write_java_config_file(output_file,
                           port=8000,
                           max_workflows=1,
                           local_job_limit=10,
                           jms_job_limit=500,
                           default_backend="Local",
                           pg_port=0,
                           db_prefix="#",
                           call_caching=False,
                           db_user="",
                           db_password="",
                           remove_logs=False,
                           timeout=600):
    """
    Write a configuration file using the template resources/template.conf,
    which is derived from the Cromwell example.  The argument defaults are
    suitable for local jobs with the HSQLDB backend.
    """
    with open(TEMPLATE_CONF, "r") as conf_in:
        with open(output_file, "w") as conf_out:
            conf_out.write(conf_in.read() % dict(
                CROMWELL_PORT=port,
                MAX_WORKFLOWS=max_workflows,
                LOCAL_JOB_LIMIT=local_job_limit,
                JMS_JOB_LIMIT=jms_job_limit,
                DEFAULT_BACKEND=default_backend,
                PGPORT=pg_port,
                DBPREFIX=db_prefix,
                CALL_CACHING=json.dumps(call_caching),
                DBUSER=db_user,
                DBPASSWORD=db_password,
                REMOVE_LOGS=json.dumps(remove_logs),
                TIMEOUT=timeout))
            log.info("Wrote config file to %s", output_file)


def _to_smrttools_cli_cmd(wdl_file,
                          inputs,
                          options,
                          conf_file=None,
                          metadata_json="metadata.json"):
    conf_args = ""
    # FIXME
    #if conf_file is not None:
    #    conf_args = "-Dconfig.file={p}".format(p=op.abspath(conf_file))
    try:
        zip_file = get_wdl_zip()
    except UserError as e:
        log.warn(e)
        zip_args = ""
    else:
        zip_args = "--imports {z}".format(z=zip_file)
    return "cromwell {c} run {w} -i {i} -o {o} -m {m} {z}".format(
        c=conf_args, w=op.abspath(wdl_file), i=inputs,
        o=options, m=metadata_json, z=zip_args)


def validate_cli_job(inputs, workflow):
    missing = []
    try:
        import pbcromwell.wdl as wdl
        missing = wdl.get_missing_inputs(open(workflow), inputs)
        if missing:
            msg = 'Missing inputs (maybe entry points): {!r} not in {!r}'.format(
                missing, inputs)
            raise Exception(msg)
        # Get the inputs (key, path) that are File type.
        file_inputs = {i: inputs[i]
                       for i in wdl.get_file_inputs(open(workflow), inputs)}
        log.debug('file_inputs: {!r}'.format(file_inputs))
        not_found = [(key, path) for key, path in file_inputs.items()
                     if not os.path.exists(path)]
        if not_found:
            msg = 'Some inputs do not exist: {!r}'.format(not_found)
            raise Exception(msg)
    except ImportError:
        msg = 'Cannot check for missing inputs because a module is not installed.'
        # If we do not need the stack-trace, this could be a warning.
        log.exception(msg)
    except SyntaxError:
        msg = 'Cannot parse "{}" to check for missing inputs.'.format(workflow)
        # If we do not need the stack-trace, this could be a warning.
        log.exception(msg)


def _is_file(s):
    # need to use exists(), not isfile(), because cromwell use symlinks too
    return isinstance(s, basestring) and op.exists(s)


# XXX ideally this would link report resources (i.e. plot images) too
def _link_file_if_possible(fn):
    link_path = op.join("outputs", op.basename(fn))
    orig_link_path = link_path
    if op.exists(link_path):
        log.warn("Output {o} already exists".format(o=link_path))
        k = 1
        while True:
            link_path = orig_link_path + "-" + str(k)
            if not op.exists(link_path):
                break
            else:
                k += 1
    log.info("Linking {f} to {o}".format(f=fn, o=op.abspath(link_path)))
    os.symlink(fn, link_path)
    if fn.endswith(".datastore.json"):
        datastore = DataStore.load_from_json(fn)
        for ds_file in datastore.files.values():
            _link_file_if_possible(ds_file.path)


def _process_job(metadata_json):
    if not op.isfile(metadata_json):
        raise IOError("File {m} does not exist".format(m=metadata_json))
    with open(metadata_json) as json_md:
        md = json.loads(json_md.read())
        os.mkdir("outputs")
        for k, v in md["outputs"].iteritems():
            if _is_file(v):
                _link_file_if_possible(v)
            elif isinstance(v, list) and all([_is_file(s) for s in v]):
                for fn in v:
                    _link_file_if_possible(fn)


def execute_cli_job(
        inputs_d,
        options_d,
        workflow_file,
        conf_file=None,
        execute_job=True):
    inputs_file = op.join(os.getcwd(), "inputs.json")
    with open(inputs_file, "wb") as inps_out:
        inps_out.write(json.dumps(inputs_d, indent=2,
                                  separators=(',', ': '), sort_keys=True))
        log.info("Wrote workflow inputs to %s", inputs_file)
    options_file = op.join(os.getcwd(), "options.json")
    with open(options_file, "wb") as opts_out:
        opts_out.write(json.dumps(options_d, indent=2,
                                  separators=(',', ': '), sort_keys=True))
        log.info("Wrote engine options to %s", options_file)
    if conf_file is None:
        log.info("Will generate standard Java config file")
        conf_file = op.join(os.getcwd(), "cromwell.conf")
        write_java_config_file(conf_file, remove_logs=True)
    metadata_json = "metadata.json"
    cmd = _to_smrttools_cli_cmd(workflow_file, inputs_file, options_file,
                                conf_file=conf_file,
                                metadata_json=metadata_json)
    script_file = "run.sh"
    with open(script_file, "wb") as sh_out:
        sh_out.write("""\
#!/bin/bash
set -e
export CROMWELL_PATH=$PATH
export CROMWELL_CONFIG_FILE={f}
{c}
""".format(f=conf_file, c=cmd))
        log.info("Wrote Bash script to %s", script_file)
        os.chmod(script_file, 0777)
    if execute_job:
        log.info("Executing Cromwell CLI app")
        subprocess.check_call(["bash", script_file])
        _process_job(metadata_json)
        shutil.rmtree("cromwell-workflow-logs")
    return 0


def execute_cli_job_in_dir(
        output_dir,
        inputs_d,
        options_d,
        workflow=None,
        conf_file=None,
        execute_job=True,
        workflow_src=None,
        overwrite=False):
    if op.exists(output_dir):
        if overwrite:
            log.warn("%s exists, removing", output_dir)
            shutil.rmtree(output_dir)
        else:
            raise UserError("The output directory {d} already exists.  Please move or delete it to continue, add --overwrite to remove it automatically, or specify another path to use with --output-dir".format(d=output_dir))
    cwd = os.getcwd()
    os.makedirs(output_dir)
    if workflow is not None and op.isfile(workflow):
        workflow = op.abspath(workflow)
    try:
        os.chdir(output_dir)
        os.mkdir("logs")
        workflow_file = workflow
        if workflow_file is None or not op.isfile(workflow):
            # if it's not a wdl file, it's assumed to be a workflow ID, to be
            # pulled out of the zip file
            wdl_zip = _get_wdl_zip_or_none()
            if workflow_src is None:
                workflow_src = get_workflow_src(wdl_zip, workflow)
            workflow_file = re.sub(" ", "_", workflow) + ".wdl"
            log.info("Writing workflow source for '{w}' to {f}".format(
                     w=workflow, f=workflow_file))
            with open(workflow_file, "w") as wdl_out:
                wdl_out.write(workflow_src)
        return execute_cli_job(inputs_d, options_d, workflow_file, conf_file, execute_job)
    finally:
        os.chdir(cwd)


def run_cli(args):
    """
    Run a Cromwell workflow in standalone mode, i.e. as a command-line program.
    Multiple input modes are supported, including a pbsmrtpipe-like set of
    arguments (for PacBio workflows only), and JSON files already in the native
    Cromwell format.

    All PacBio workflows have similar requirements to the old pbsmrtpipe
    pipelines in previous versions of SMRT Link:
      1. One or more PacBio dataset XML entry points, usually a SubreadSet
         or ConsensusReadSet (--entry-point <FILE>)
      2. Any number of workflow-specific task options (--task-option <OPTION>)
      3. Engine options independent of the workflow, such as number of
         processors per task (--nproc), or compute backend (--backend)

    Output will be directed to a new directory (--output-dir, defaults to
    cromwell_out).  This will include final inputs for the Cromwell CLI, and
    subdirectories for logs (workflow and task outputs), links to output files,
    and the Cromwell execution itself, which has a complex nested directory
    structure.  Detailed information about the workflow execution can be
    found in the file "metadata.json", in the native Cromwell format.

    Note that output file links do not include the individual resource files of
    datasets and reports (BAM files, index files, plot PNGs, etc.).  Follow
    the symbolic links to their real path (for example using `readlink -f`) to
    find report plots.

    For further information about Cromwell, please consult the official
    documentation at https://cromwell.readthedocs.io.

    Examples:

      Run CCS workflow:

        $ pbcromwell run pb_ccs -e <SUBREADS> --nproc 8 --config cromwell.conf

      Run Iso-Seq workflow, including mapping to reference, and execute on SGE:

        $ pbcromwell run pb_isoseq3 -e <SUBREADS> -e <PRIMERS> -e <REFERENCE> --nproc 8 --config cromwell.conf --backend SGE

      Run a user-defined workflow:

        $ pbcromwell run my_workflow.wdl -i inputs.json -o options.json --config cromwell.conf

      Set up input files but exit before starting Cromwell:

        $ pbcromwell run pb_ccs -e <SUBREADS> --nproc 8 --dry-run
    """
    inputs_d = _get_inputs_dict(args)
    log.info(inputs_d)
    if op.isfile(args.workflow):
        validate_cli_job(inputs_d, args.workflow)
    options_d = _get_options_dict(args, log_dir=op.abspath(op.join(args.output_dir, "logs")))
    return execute_cli_job_in_dir(args.output_dir, inputs_d, options_d, args.workflow, args.config, args.execute, overwrite=args.overwrite)


def run_submit(args):
    """
    WARNING: DEVELOPER UTILITY, NOT OFFICIALLY SUPPORTED

    Submit a workflow to a Cromwell server.  This can work with either
    officially supported PacBio workflows (specified by ID), or external WDL
    files.  Multiple input modes are supported, including a pbsmrtpipe-like
    set of arguments (for PacBio workflows only), and JSON files already in the
    native Cromwell format.

    To see a list of installed workflows, run `pbcromwell show-workflows`.

    Examples:

      Run CCS workflow:

        $ pbcromwell submit pb_ccs -e <SUBREADS> --nproc 8 --backend SGE

      Run an external WDL file and pre-defined JSON inputs:

        $ pbcromwell submit <WDL_FILE> -i inputs.json -o options.json
    """
    wdl_zip = _get_wdl_zip_or_none()
    options_d = _get_options_dict(args)
    inputs = _get_inputs_dict(args)
    log.info(inputs)
    workflow_src = get_workflow_src(wdl_zip, args.workflow)
    retry_args = RetryArgs(0, 1, 0)
    status = submit_workflow(
        host=args.host,
        port=args.port,
        workflow_src=workflow_src,
        inputs_d=inputs,
        options_d=options_d,
        dependencies_zip=wdl_zip,
        retry_args=retry_args)
    if args.block:
        poll_until_job_succeeds(args.host, args.port,
                                status.id, max_time=args.max_time)
    return 0


def run_job_status(args):
    """
    WARNING: DEVELOPER UTILITY, NOT OFFICIALLY SUPPORTED

    Display the status of a Cromwell workflow on the server.
    """
    s = get_job_status(args.host, args.port, args.job_id)
    msg = "Job {i} status: {s}".format(i=s.id, s=s.status)
    if s.status in JobStates.FAILING_JOB_STATES:
        log.error(msg)
        return 1
    else:
        log.info(msg)
        return 0


def run_abort(args):
    """
    WARNING: DEVELOPER UTILITY, NOT OFFICIALLY SUPPORTED

    Abort a currently running Cromwell workflow on the server.
    """
    print(abort_job(args.host, args.port, args.job_id).status)
    return 0


def run_metadata(args):
    """
    WARNING: DEVELOPER UTILITY, NOT OFFICIALLY SUPPORTED

    Retrieve metadata JSON for a Cromwell workflow run on the server.
    """
    d = get_job_metadata(args.host, args.port, args.job_id)
    print(json.dumps(d, indent=2, separators=(',', ': '), sort_keys=True))
    return 0


def run_show_running(args):
    """
    WARNING: DEVELOPER UTILITY, NOT OFFICIALLY SUPPORTED

    Print a list of workflows currently running on the Cromwell server.
    """
    d = get_running_jobs(args.host, args.port)
    print(json.dumps(d, indent=2, separators=(',', ': '), sort_keys=True))
    return 0


def run_wait(args):
    """
    WARNING: DEVELOPER UTILITY, NOT OFFICIALLY SUPPORTED

    Wait for the server to become available for workflow submissions.
    """
    wait_for_server(args.host, args.port, args.max_tries, args.sleep_time)
    return 0


def run_configure(args):
    """
    Generate the Java config file used by Cromwell to define backends and
    other important engine options that can't be set at runtime.  You can
    pass this to ``pbcromwell run`` with the --config argument.
    """
    db_prefix = ""
    db_port = args.db_port
    if db_port is None:
        log.warn("No database port specified - will run with in-memory DB")
        db_prefix = "#"
        db_port = 0
    enable_call_caching = True
    if args.cache == False:
        enable_call_caching = False
    write_java_config_file(
        args.output_file,
        args.port,
        args.max_workflows,
        args.local_job_limit,
        args.jms_job_limit,
        args.default_backend,
        db_port,
        db_prefix,
        enable_call_caching,
        args.db_user,
        args.db_password,
        timeout = args.timeout,
        remove_logs = False)
    return 0


def run_show_workflows(args):
    """
    Print a list of built-in PacBio workflows.
    """

    def _is_visible(p):
        return not ("internal" in p.tags or "dev" in p.tags or "alpha" in p.tags or "obsolete" in p.tags)

    pipelines = _load_all_pipeline_json()
    pipelines.sort(lambda a, b: cmp(a.display_name, b.display_name))
    print("\n")
    for pipeline in pipelines:
        if _is_visible(pipeline):
            print("{i}: {n}".format(n=pipeline.display_name, i=pipeline.idx))
    print("")
    print("Run 'pbcromwell show-workflow-details <ID>' to display further")
    print("information about a workflow.  Note that the cromwell.workflows.")
    print("prefix is optional.")
    return 0


def run_show_details(args):
    """
    Print details about the named PacBio workflow, including input files and
    task options.  Note that the prefix "cromwell.workflows." is optional.

    Example:

      $ pbcromwell show-details pb_ccs
      $ pbcromwell show-details cromwell.workflows.pb_ccs
    """

    def _pipeline_to_inputs(p):
        inputs = []
        def _to_key(key): return "{n}.{k}".format(n=p.idx.split(".")[-1], k=key)
        for eid, _ in p.entry_bindings:
            inputs.append((_to_key(eid), ""))
        inputs.append((_to_key("nproc"), 1))
        for option_id, value in p.iter_options():
            inputs.append((_to_key(option_id), value))
        return OrderedDict(inputs)

    pipeline = _load_pipeline_by_id(args.workflow_id)
    print("\n")
    print(pipeline.summary())
    print("")
    if args.inputs_json is not None:
        with open(args.inputs_json, "w") as json_out:
            json_out.write(json.dumps(_pipeline_to_inputs(pipeline), indent=2, separators=(',', ': ')))
        log.info("Wrote template inputs to {f}; you will need to fill in the entry point paths".format(f=args.inputs_json))
    return 0


def add_server_args(parser):
    g = parser.add_argument_group("server", "Cromwell server settings")
    g.add_argument("--host", action="store", default=DEFAULT_HOST,
                   help="Hostname of Cromwell server, $PB_CROMWELL_HOST by default")
    g.add_argument("--port", action="store", type=int,
                   default=DEFAULT_PORT, help="Cromwell server port, $PB_CROMWELL_PORT by default")
    return g


def add_call_caching_args(parser, default=CALL_CACHING_DEFAULT):
    parser.add_argument("--cache", action="store_true", dest="cache",
                        default=default,
                        help="Enable call caching")
    parser.add_argument("--no-cache", action="store_false", dest="cache",
                        default=default,
                        help="Disable call caching")
    return parser


def _get_parser():

    def _add_arg_job_id(parser):
        parser.add_argument("job_id", action="store",
                            type=uuid.UUID, help="Cromwell job ID - this must be a valid UUID")

    def _add_job_settings_args(parser):
        parser.add_argument("workflow", help="WDL source")
        parser.add_argument("-i", "--inputs", action="store", default=None,
                            help="Cromwell inputs and settings as JSON")
        parser.add_argument("-e", "--entry", dest="entry_points",
                            action="append",
                            default=[],
                            help="Entry point dataset")
        parser.add_argument("-n", "--nproc", action="store", type=int,
                            default=1, help="Number of processors per task")
        parser.add_argument("-c", "--max-nchunks", action="store", type=int,
                            default=None,
                            help="Maximum number of chunks per task")
        parser.add_argument("--target-size", action="store", type=int,
                            default=None,
                            help="Target chunk size")
        parser.add_argument("--queue", action="store", default=None,
                            help="Cluster queue to use")
        parser.add_argument("-o", "--options", dest="options", action="store",
                            default=None,
                            help="Additional Cromwell engine options, as JSON file")
        parser.add_argument("-t", "--task-option", dest="task_options", action="append", default=[], help="Workflow- or task-level option as key=value string, specific to the application.  May be specified multiple times for multiple options.")
        parser.add_argument("-b", "--backend", dest="backend", default=None,
                            help="Backend to use for running tasks [EXPERIMENTAL AND NOT WORKING]")
        parser.add_argument("-r", "--maxRetries", dest="max_retries", default=1, help="Maxmimum number of times to retry a failing task")
        parser.add_argument("--tmp-dir", dest="tmp_dir", default=None,
                            help="Optional temporary directory for Cromwell tasks (must exist on all compute hosts)")

    def _add_job_polling_args(parser):
        parser.add_argument("--block", action="store_true",
                            help="Block and poll until job completes successfully")
        parser.add_argument("--max-time", action="store", type=int,
                            default=43200,  # i.e. 12 hours
                            help="Maximum time in seconds to poll job")

    def _add_config_args(parser):
        parser.add_argument("--port", type=int, action="store",
                            default=DEFAULT_PORT,
                            help="Port that cromwell should listen on")
        parser.add_argument("--local-job-limit", type=int, action="store",
                            default=10, help="Maximum number of local jobs/tasks that can be run at once")
        parser.add_argument("--jms-job-limit", type=int, action="store",
                            default=500, help="Maximum number of jobs/tasks that can be submitted to the queueing system at a time")
        parser.add_argument("--db-port", type=int, action="store",
                            default=None,
                            help="Database port for Cromwell to use; if undefined, database configuration will be omitted")
        parser.add_argument("--db-user", action="store", default="smrtlink_user", help="Name of user to connect to Postgres as")
        parser.add_argument("--db-password", action="store", default="",
                            help="Password to connect to Postgres as")
        parser.add_argument("--default-backend", action="store",
                            default="Local", help="Default job execution backend")
        parser.add_argument("--max-workflows", action="store", type=int,
                            default=100, help="Maximum number of workflows that cromwell can run at once")
        parser.add_argument("--output-file", action="store",
                            default="cromwell.conf",
                            help="Name of output config file")
        parser.add_argument("--timeout", action="store", default=600,
                            help="Timeout to wait for task completion before checking cluster job status")
        return add_call_caching_args(parser, default=True)

    class Formatter(argparse.ArgumentDefaultsHelpFormatter,
                    argparse.RawDescriptionHelpFormatter):
        pass

    p = get_default_argparser_with_base_opts(
        description=__doc__,
        version=__version__,
        default_level="INFO")
    p.formatter_class = Formatter
    sp = p.add_subparsers(
        title="subcommands",
        help="Type pbcromwell {command} -h for a command's options")

    def add_parser(name, runner):
        p_sub = sp.add_parser(name, formatter_class=Formatter)
        p_sub.description = textwrap.dedent(runner.__doc__)
        p_sub.set_defaults(runner=runner)
        return p_sub

    def args_run():
        # run CLI job
        p_run = add_parser("run", run_cli)
        p_run.add_argument("--output-dir", action="store", default="cromwell_out",
                           help="Output directory to run Cromwell in")
        p_run.add_argument("--overwrite", action="store_true", default=False,
                           help="Overwrite output directory if it exists")
        _add_job_settings_args(p_run)
        p_run.add_argument("--config", action="store", default=None,
                           help="Java config file for running Cromwell")
        p_run.add_argument("--dry-run", action="store_false", dest="execute",
                           default=True,
                           help="Don't execute Cromwell, just write final inputs and exit")

    def args_show_workflows():
        p_show = add_parser("show-workflows", run_show_workflows)

    def args_show_details():
        p_details = add_parser("show-workflow-details", run_show_details)
        p_details.add_argument("workflow_id",
                               help="Workflow ID, e.g. 'pb_ccs'")
        p_details.add_argument("--inputs-json", action="store", default=None,
                               help="Write inputs JSON to this file")

    def args_config():
        p_config = add_parser("configure", run_configure)
        _add_config_args(p_config)

    def args_submit():
        # submit job
        p_submit = add_parser("submit", run_submit)
        g_server = add_server_args(p_submit)
        add_call_caching_args(g_server)
        _add_job_settings_args(p_submit)
        _add_job_polling_args(p_submit)

    def args_status():
        # status
        p_job_status = add_parser("get-job", run_job_status)
        add_server_args(p_job_status)
        _add_arg_job_id(p_job_status)

    def args_abort():
        # abort
        p_abort = add_parser("abort", run_abort)
        add_server_args(p_abort)
        _add_arg_job_id(p_abort)

    def args_metadata():
        # metadata
        p_md = add_parser("metadata", run_metadata)
        add_server_args(p_md)
        _add_arg_job_id(p_md)

    def args_running():
        # show running jobs
        p_running = add_parser("show-running", run_show_running)
        add_server_args(p_running)

    def args_wait():
        # wait for server to start
        p_wait = add_parser("wait", run_wait)
        add_server_args(p_wait)
        p_wait.add_argument("--max-tries", action="store", type=int, default=10,
                            help="Maximum number of connection attempts")
        p_wait.add_argument("--sleep-time", action="store", type=int, default=2,
                            help="Wait time (in seconds) in between connection attempts")

    args_run()
    args_show_workflows()
    args_show_details()
    args_config()
    # advanced tools
    args_submit()
    args_status()
    args_abort()
    args_metadata()
    args_running()
    args_wait()
    return p


def run_args(args):
    return args.runner(args)


def setup_log(alog, **log_options):
    log_options['str_formatter'] = LogConstants.LOG_FMT_STD
    return pbcommand_setup_log(alog, **log_options)


def main(argv=sys.argv):
    return pacbio_args_runner(
        argv=argv[1:],
        parser=_get_parser(),
        args_runner_func=run_args,
        alog=log,
        setup_log_func=setup_log,
        dump_alarm_on_error=False)


if __name__ == "__main__":
    sys.exit(main(sys.argv))
