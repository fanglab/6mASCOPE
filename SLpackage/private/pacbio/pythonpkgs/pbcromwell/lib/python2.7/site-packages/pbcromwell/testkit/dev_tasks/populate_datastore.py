# ported from pbsmrtpipe.pb_tasks.dev.run_dev_txt_to_datastore

"""
Write a datastore containing an arbitrary number of mock demultiplexed
SubreadSet XMLs.
"""

import logging
import random
import time
import os.path as op
import sys

from pbcore.io import SubreadSet
from pbcommand.models.common import DataStoreFile, DataStore, FileTypes
from pbcommand.cli import (pacbio_args_runner,
                           get_default_argparser_with_base_opts)
from pbcommand.utils import setup_log

__version__ = "0.1"
log = logging.getLogger(__name__)


def run_populate_datastore(subreads,
                           output_file,
                           num_subreadsets=25,
                           sleep_multiplier=0):

    p = op.dirname(op.abspath(output_file))
    t_sleep = sleep_multiplier * random.random()
    log.info("Sleeping for %.1f seconds", t_sleep)
    time.sleep(t_sleep)
    add_parent = True
    if len(subreads.metadata.provenance) > 0:
        log.warn("Not adding provenance since input already has a parent")
        add_parent = False

    def to_f(x):
        source_id = "out-1"
        sset_out = subreads.copy()
        sset_out.newUuid(random=True)
        if add_parent:
            sset_out.metadata.addParentDataSet(subreads.uuid,
                                               subreads.datasetType,
                                               createdBy="AnalysisJob",
                                               timeStampedName="")
        file_name = "file-{x:03d}.subreadset.xml".format(x=x)
        out_path = op.join(p, file_name)
        sset_out.write(out_path)
        sset_uuid = sset_out.uniqueId
        name = "subreadset-{}".format(x)
        dsf = DataStoreFile(sset_uuid, source_id,
                            FileTypes.DS_SUBREADS.file_type_id,
                            out_path,
                            name=name,
                            description="{} Example Description".format(name))
        return dsf

    ds_files = [to_f(i + 1) for i in xrange(num_subreadsets)]
    ds = DataStore(ds_files)
    ds.write_json(output_file)
    return 0


def run_args(args):
    return run_populate_datastore(args.subreads_in, args.output_file,
                                  args.num_subreadsets, args.sleep_multiplier)


def _get_parser():
    p = get_default_argparser_with_base_opts(
        version=__version__,
        description=__doc__,
        default_level="INFO")
    p.add_argument("subreads_in", type=SubreadSet,
                   help="SubreadSet to use as input")
    p.add_argument("output_file", help="Output datastore")
    p.add_argument("--num-subreadsets", type=int, default=25,
                   help="Number of SubreadSets in output datastore")
    p.add_argument("--sleep-multiplier", type=int, default=0,
                   help="Optional sleep time")
    return p


def main(argv=sys.argv):
    return pacbio_args_runner(
        argv=argv[1:],
        parser=_get_parser(),
        args_runner_func=run_args,
        alog=log,
        setup_log_func=setup_log)


if __name__ == "__main__":
    sys.exit(main(sys.argv))
