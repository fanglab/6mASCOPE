# TODO better failure modes and debugging output
# TODO XML output
# TODO SMRT Link jobs API support

"""
pbcromwell-test-runner: Cromwell job test driver

This tool fills a similar niche to pbtestkit-runner and
pbtestkit-service-runner in pbsmrtpipe, but with a simplified data
model.  It consists of a blocking job runner followed by simple report-driven
tests (similar to those in pysiv2).

A test job is defined by at least two input files:

    1. a test configuration JSON (PacBio format)
    2. workflow inputs JSON (Cromwell's format)
    3. the workflow source (in WDL format)

By default (2) is assumed to be "inputs.json" in the same directory as (1),
unless explicitly specified.  The workflow source is read from the
common "imports.zip" defined by the environment variable
SMRT_WORKFLOW_DEPENDENCIES_ZIP, using the workflowId in (1).  Alternately a
custom workflow may be specified as a filesystem path with the
-w/--workflow-src flag.

This tool can run workflows in one of four different modes:

    1. By submitting to a Cromwell server, defaulting to http://localhost:8000
    2. By running Cromwell as a command-line tool (--cli) (NOT RECOMMENDED)
    3. By submitting to SMRT Link with a pbsmrtpipe-like job options model
       (--smrtlink)
    4. By submitting to a SMRT Link server using an API similar to the native
       Cromwell REST API (--smrtlink-cromwell); this only works from localhost

In all modes, if the workflow fails the program will exit early without
running any further tests.  The default behavior is to abort the job on
keyboard interrupt, unless --no-abort is specified.  Whether call caching is
used is left up to the server by default, but you may specify --no-cache to
force it off.

The configuration file is simpler than the JSON configs used for pbsmrtpipe:

{
    "jobId": "resequencing_tiny_lambda_iguana",
    "workflowId": "pb_resequencing",
    "description": "Tiny resequencing job with lambda data, Iguana chemistry",
    "reports": {
        "mapping_stats": {
            "mapped_reads_n": 2,
            "mapped_subreads_n": 4,
            "mapped_subread_concordance_mean__ge": 0.85,
            "mapped_subread_bases_n__ge": 39950,
            "mapped_subread_bases_n__le": 40050
        }
    }
}

The optional 'reports' field specifies report IDs and corresponding tests run
on report attribute values - append suffices __lt, __gt, __le, or __ge to the
attribute ID for approximate comparisions as shown above.  Failure of any
one of these comparisons will result in a non-zero exit code, as will failure
to find a specified report in the workflow outputs.

The inputs.json accompanying the above configuration would look like this:

{
    "pb_resequencing.eid_subread": "/path/to/subreadset.xml",
    "pb_resequencing.eid_ref_dataset": "/path/to/referenceset.xml"
}

With call caching repeated workflow executions should be relatively quick,
however to disable this for debugging, add the argument '--no-cache'.
To resume testing against an already running or completed workflow (in server
mode only), use this command:

    pbcromwell-test-runner <workflow.wdl> <config.json> --resume <ID>

where ID is the Cromwell workflow UUID.  If you are using the SMRT Link jobs
interface, it can be either the job integer ID or UUID.

Parallelization behavior is controlled flexibly either through the inputs.json,
command-line arguments, or both/none.  If no settings are defined the default
will be nproc=1, max_nchunks=None, target_size=None.  Values supplied as
command-line args take precedence over inputs.json.
"""

from StringIO import StringIO
import operator as OP
import tempfile
import argparse
import logging
import json
import re
import os.path as op
import os
import sys

from pbcommand.cli import get_default_argparser_with_base_opts, pacbio_args_runner
from pbcommand.pb_io import load_report_from_json
from pbcommand.utils import get_dataset_metadata

from pbcromwell import __version__
from pbcromwell.cli import (DEFAULT_HOST,
                            DEFAULT_PORT,
                            ENTRY_POINT_IDS,
                            UserError,
                            get_wdl_zip,
                            get_workflow_src,
                            read_workflow,
                            add_server_args,
                            add_call_caching_args,
                            execute_cli_job_in_dir,
                            setup_log)
from pbcromwell.client import (run_workflow,
                               RetryArgs,
                               abort_job,
                               get_job_metadata,
                               poll_until_job_succeeds,
                               JobStates,
                               get_engine_options,
                               WorkflowError)
from pbcromwell.testkit.smrtlink import (run_smrtlink_workflow,
                                         run_smrtlink_analysis,
                                         resume_smrtlink_job)
from pbcromwell.testkit.testcases import make_report_tests, run_tests

log = logging.getLogger(__name__)


class TestkitCfg(object):
    """
    Container for test metadata and expected report values
    """

    def __init__(self,
                 job_id,
                 workflow_id,
                 description,
                 xrays,
                 reports={}):
        self.job_id = job_id
        self.workflow_id = workflow_id
        self.description = description
        self.reports = reports
        self.xrays = xrays

    @staticmethod
    def from_json(f):
        with open(f, "r") as json_in:
            d = json.loads(json_in.read())
            xrays = d.get('xrays', [])
            if not isinstance(xrays, list):
                msg = 'In "{}", the optional "xrays" field must be a list, not {!r}. Ignored.'.format(f, xrays)
                log.error(msg)
                xrays = []
            return TestkitCfg(
                job_id=d['jobId'],
                workflow_id=d["workflowId"],
                description=d['description'],
                xrays=xrays,
                reports=d.get("reports", {}))


def _make_pbvalidate_test(file_name):
    def _pbvalidate(self):
        # pbcoretools may not be installed when pylint runs
        from pbcoretools.pbvalidate.main import get_parser, run_validator # pylint: disable=import-error
        argv = [
            "pbvalidate",
            "--quick",
            op.realpath(file_name)
        ]
        args = get_parser().parse_args(argv[1:])
        out = StringIO()
        v = run_validator(args, out=out)
        self.assertEqual(v.n_errors, 0, out.getvalue())
        log.info("Passed: {a}".format(a=" ".join(argv)))
    return _pbvalidate


def _run_tests(outputs_d, testkit_cfg, output_xml, pbvalidate=False):
    reports = {}
    report_sources = {}
    other_tests_d = {}
    for key, value in outputs_d.iteritems():
        if value is None:
            continue
        if value.endswith(".report.json"):
            report = load_report_from_json(value)
            log.info("Read report {i} from {f}".format(i=report.id, f=value))
            reports[report.id] = report
            report_sources[report.id] = value
        elif value.endswith("set.xml") or value.endswith(".fasta"):
            if pbvalidate:
                test_id = "test_pbvalidate_{k}".format(k=key)
                other_tests_d[test_id] = _make_pbvalidate_test(value)
    tests = []
    if len(testkit_cfg.reports) == 0:
        log.warn(
            "No report tests defined - the job was successful but accurate results are not guaranteed")
    test_classes = []
    job_id = "{w}_{j}".format(w=testkit_cfg.workflow_id,
                              j=re.sub("-", "_", testkit_cfg.job_id))
    for report_id, test_values in testkit_cfg.reports.iteritems():
        if not report_id in reports:
            raise KeyError(
                "Can't find report with ID '{i}' in workflow outputs".format(i=report_id))
        log.info("Comparing attribute values in {r}".format(r=report_id))
        report = reports[report_id]
        report_src = report_sources[report_id]
        test_classes.append(make_report_tests(
            job_id, report, test_values, report_src, other_tests_d))
    result = run_tests(test_classes, output_xml=output_xml, stream=sys.stdout)
    return 0 if result.wasSuccessful() else 1


def _check_status_and_run_tests(md, testkit_cfg, output_xml, pbvalidate=False):
    if md["status"] in JobStates.FAILING_JOB_STATES:
        raise RuntimeError(
            "Workflow failed with status {s}".format(s=md["status"]))
    elif md["status"] != "Succeeded":
        raise RuntimeError("Workflow has not completed yet, aborting")
    return _run_tests(md["outputs"], testkit_cfg, output_xml,
                      pbvalidate=pbvalidate)


def run_cli_test_job(inputs_d, options_d, workflow, workflow_src):
    """
    Run a Cromwell workflow on the command line and return the metadata object
    """
    log_dir = op.abspath(op.join("cromwell_out", "logs"))
    options_d.update({
        "final_workflow_log_dir": log_dir,
        "final_call_logs_dir": log_dir
    })
    execute_cli_job_in_dir(
        "cromwell_out",
        inputs_d,
        options_d,
        workflow=workflow,
        overwrite=True,
        workflow_src=workflow_src)
    with open("cromwell_out/metadata.json") as md_json:
        return json.loads(md_json.read())


def _resolve_inputs(inputs_json, nproc, max_nchunks, target_size,
                    tmp_out):
    with open(inputs_json) as json_in:
        d = json.loads(json_in.read())
        prefixes = sorted(list({k.split(".")[0] for k in d.keys()}))
        if len(prefixes) > 1:
            raise ValueError("The input JSON file {f} contains more than one dictionary prefix: {p}.  Each key in the dictionary must be prefixed with the workflow_id, e.g. \"my_workflow.nproc\".".format(
                f=inputs_json, p=", ".join(prefixes)))
        have_args = {k.split(".")[1] for k in d.keys()}

        def _to_key(k): return "{p}.{k}".format(p=prefixes[0], k=k)
        if not "nproc" in have_args:
            d[_to_key("nproc")] = nproc if nproc is not None else 1
        elif nproc is not None:
            d[_to_key("nproc")] = nproc
        if target_size is not None:
            d[_to_key("target_size")] = target_size
        if max_nchunks is not None:
            d[_to_key("max_nchunks")] = max_nchunks
        # TODO log_level
        return d


def add_smrtlink_server_args(p):
    ARG = p.add_argument
    ARG("--user", dest="user", action="store",
        default=os.environ.get("PB_SERVICE_AUTH_USER", None),
        help="SMRT Link user to authenticate with (if using HTTPS)")
    ARG("--password", dest="password", action="store",
        default=os.environ.get("PB_SERVICE_AUTH_PASSWORD", None),
        help="SMRT Link password to authenticate with (if using HTTPS)")
    ARG("--smrtlink", action="store_true", default=False,
        help="Submit workflow as SMRT Link Analysis job")
    ARG("--pbsmrtpipe", action="store_true", dest="smrtlink",
        help="Submit workflow as SMRT Link Analysis job (deprecated)")
    ARG("--smrtlink-cromwell", action="store_true", default=False,
        help="Submit Cromwell workflow directly via SMRT Link web services")
    return p


def add_control_args(p):
    ARG = p.add_argument
    ARG("-n", "--nproc", action="store", type=int,
        default=None, help="Number of processors per task (1 if not otherwise specified)")
    ARG("-c", "--max-nchunks", action="store", type=int,
        default=None,
        help="Maximum number of chunks per task")
    ARG("--target-size", action="store", type=int,
        default=None,
        help="Target chunk size")
    ARG("--backend", action="store", default=None, help="Backend to use")
    ARG("--queue", action="store", default=None,
        help="SGE queue to use")
    ARG("-o", "--option", dest="options", action="append",
        default=[],
        help="Additional workflow options")
    ARG("--cli", dest="cli_only", action="store_true",
        help="Run Cromwell as a CLI tool instead of submitting to a server")
    ARG("--max-time", action="store", type=int, default=43200,
        help="Maximum time in seconds to wait for job to complete")
    ARG("--retries", action="store", type=int, default=0,
        help="Retry submission to service, after failure.")
    ARG("--retry-timeout", action="store", type=int, default=1,
        help="Time (seconds) to wait before considering the initial job-submission call a failure.")
    ARG("--retry-delay", action="store", type=int, default=1,
        help="Initial delay (seconds) before retry. Doubles after each retry.")
    ARG("--no-abort", action="store_false", dest="abort_on_interrupt",
        default=True,
        help="Don't abort workflow if this program is interrupted")
    add_server_args(p)
    add_call_caching_args(p)
    add_smrtlink_server_args(p)
    return p


def _get_parser():
    class Formatter(argparse.ArgumentDefaultsHelpFormatter,
                    argparse.RawDescriptionHelpFormatter):
        pass

    p = get_default_argparser_with_base_opts(
        description=__doc__,
        version=__version__)
    p.formatter_class = Formatter
    ARG = p.add_argument
    ARG("testkit_cfg", help="Test job configuration JSON")
    ARG("-w", "--workflow", default=None,
        help="WDL source file or workflow ID (optional if workflow is present in imports.zip)")
    ARG("-i", "--inputs", action="store", default=None,
        help="Cromwell inputs and settings as JSON; if not specified this will be inputs.json in the same directory as testkit_cfg")
    add_control_args(p)
    ARG("--resume", dest="test_job_id", default=None,
        help="Resume an existing test job; if already completed this will just run the report tests and exit.  Not valid when combined with --cli")
    ARG("--tmp-datasets-path", action="store", default=".wdl",
        help="Directory for temporary input datasets")
    ARG("--xunit-out", action="store", default="xunit_results.xml",
        help="XUnit test results file")
    ARG("--pbvalidate", action="store_true",
        help="Run pbvalidate on all relevant output files")
    ARG("--no-block", action="store_true",
        help="Start job and exit without running tests")
    return p


def _get_workflow_inputs(args, testkit_cfg):
    inputs_json = args.inputs
    if inputs_json is None:
        inputs_json = op.join(op.dirname(args.testkit_cfg), "inputs.json")
    log.info("Reading inputs JSON: {f}".format(f=inputs_json))
    inputs_d = _resolve_inputs(
        inputs_json, args.nproc, args.max_nchunks, args.target_size, args.tmp_datasets_path)
    options_d = get_engine_options(
        queue=args.queue,
        cache=args.cache,
        backend=args.backend)
    log.info("Inputs: {d}".format(d=inputs_d))
    try:
        wdl_zip = get_wdl_zip()
    except UserError as e:
        log.warn(e)
        wdl_zip = None
    if args.workflow is not None:
        workflow_src = get_workflow_src(wdl_zip, args.workflow)
    else:
        workflow_src = read_workflow(wdl_zip, testkit_cfg.workflow_id)
    return (workflow_src,
            inputs_d,
            options_d,
            wdl_zip)


def _run_smrtlink_job(args, testkit_cfg):
    log.info("Running as SMRT Link job")
    (workflow_src, inputs_d, options_d,
     wdl_zip) = _get_workflow_inputs(args, testkit_cfg)
    if args.smrtlink:
        return run_smrtlink_analysis(
            args.host,
            args.port,
            testkit_cfg.job_id,
            testkit_cfg.workflow_id,
            inputs_d,
            options_d,
            user=args.user,
            password=args.password,
            max_time=args.max_time,
            abort_on_interrupt=args.abort_on_interrupt)
    else:
        if args.host != "localhost":
            raise ValueError("Raw Cromwell workflow submission via SMRT Link services only works on localhost")
        return run_smrtlink_workflow(
            args.host,
            args.port,
            workflow_src,
            inputs_d,
            options_d,
            dependencies_zip=wdl_zip,
            name=testkit_cfg.job_id,
            user=args.user,
            password=args.password,
            max_time=args.max_time,
            abort_on_interrupt=args.abort_on_interrupt)


def _run_cromwell_job(args, testkit_cfg):
    (workflow_src, inputs_d, options_d,
     wdl_zip) = _get_workflow_inputs(args, testkit_cfg)
    if args.cli_only:
        return run_cli_test_job(
            inputs_d,
            options_d,
            testkit_cfg.workflow_id,
            workflow_src)
    else:
        retry_args = RetryArgs(
            args.retries, args.retry_timeout, args.retry_delay)
        return run_workflow(args.host,
                            args.port,
                            workflow_src,
                            inputs_d,
                            options_d,
                            dependencies_zip=wdl_zip,
                            max_time=args.max_time,
                            abort_on_interrupt=args.abort_on_interrupt,
                            retry_args=retry_args,
                            no_block=args.no_block)


def _run_job(args, testkit_cfg):
    if args.smrtlink or args.smrtlink_cromwell:
        assert not args.cli_only
        return _run_smrtlink_job(args, testkit_cfg)
    else:
        return _run_cromwell_job(args, testkit_cfg)


def _get_job_metadata_cromwell(args):
    md = get_job_metadata(args.host, args.port, args.test_job_id)
    if not md["status"] in JobStates.TERMINAL_JOB_STATES:
        md = poll_until_job_succeeds(args.host, args.port, args.test_job_id,
                                     max_time=args.max_time,
                                     abort_on_interrupt=False)
    return md


def _resume_job(args):
    if args.smrtlink:
        return resume_smrtlink_job(args.host,
                                   args.port,
                                   args.test_job_id,
                                   args.user,
                                   args.password,
                                   max_time=args.max_time,
                                   abort_on_interrupt=args.abort_on_interrupt)
    else:
        return _get_job_metadata_cromwell(args)


def _run_args(args):
    log.info("Reading testkit config file {f}".format(f=args.testkit_cfg))
    testkit_cfg = TestkitCfg.from_json(args.testkit_cfg)
    try:
        if args.test_job_id:
            md = _resume_job(args)
        else:
            md = _run_job(args, testkit_cfg)
        if args.no_block:
            return 0
        return _check_status_and_run_tests(md, testkit_cfg, args.xunit_out,
            args.pbvalidate)
    except WorkflowError as e:
        log.error(e)
        return 1


def main(argv=sys.argv):
    return pacbio_args_runner(
        argv=argv[1:],
        parser=_get_parser(),
        args_runner_func=_run_args,
        alog=log,
        setup_log_func=setup_log,
        dump_alarm_on_error=False)


if __name__ == "__main__":
    sys.exit(main(sys.argv))
