#! python
"""
Utils for outputing read status of FL reads, and making
read mapping abundance file.
"""

from collections import defaultdict
from ..io import GroupReader, MapStatus, ReadStatRecord, \
    ReadStatReader, ReadStatWriter, AbundanceRecord, AbundanceWriter


__author__ = 'etseng@pacificbiosciences.com'


def read_group_file(group_filename, sample_prefixes=None):
    """
    Read Group file, map transcript (e.g., transcript/0) to collapsed cluster id (ex: PB.1.1).
    Return: dict of transcript_id --> collapsed cluster ID
    """
    cid_info = {}  # ex: i1 --> c123 --> PB.1.1, or None --> c123 --> PB.1.1
    if sample_prefixes is not None:
        for sample_prefix in sample_prefixes:
            cid_info[sample_prefix] = {}
    else:
        cid_info[None] = {}

    with GroupReader(group_filename) as reader:
        for group in reader:
            pbid, members = group.name, group.members
            for cid in members:  # ex: cid is 'unknown_sample_HQ_transcript/0'
                cid = cid[cid.rfind('transcript'):]  # ex: cid is 'transcript/0'
                if sample_prefixes is None:
                    cid_info[None][cid] = pbid
                else:
                    if any(cid.startswith(sample_prefix + '|') for sample_prefix in sample_prefixes):
                        sample_prefix, cid = cid.split('|', 1)
                    cid_info[sample_prefix][cid] = pbid
    return cid_info


def output_read_count_FL(cid_info, output_filename, sample_prefix,
                         transcript_to_reads_dict, read_to_length_dict,
                         output_mode='w', restricted_movies=None):
    """
    Given cid_info, output read status of FL CCS reads in restricted_movies.

    If restricted_movies is None, all FL reads are output.
    Otherwise (esp. in case where I binned by size then pooled in the end),
    give the list of movies associated with a particular list of cell runs
    (ex: brain_2to3k_phusion FL only)

    Because may have multiple pickles, can ONLY determine which FL reads
    are unmapped at the VERY END.

    Parameters:
        cid_info -- a dict read from group file, seq_or_ice_cluster --> collapsed cluster ID
        output_filename -- a tab delimited file reporting FL reads status
        restricted_movies -- if not None, only output status of reads in these movies.
    """
    unmapped_holder = set()  # will hold anything that was unmapped in one of the pickles
    mapped_holder = set()  # will hold anything that was mapped in (must be exactly) one of
    # the pickles then to get the true unmapped just to {unmapped} - {mapped}
    writer = ReadStatWriter(output_filename, mode=output_mode)

    for cid, members in transcript_to_reads_dict.iteritems():
        assert cid.startswith('transcript')

        def read_id_yielder():  # yield read_id
            for read_id in members:
                movie = read_id.split('/')[0]
                if restricted_movies is None or movie in restricted_movies:
                    yield read_id

        if cid in cid_info[sample_prefix]:
            # can immediately add all (movie-restricted) members to mapped
            for read_id in read_id_yielder():
                mapped_holder.add(read_id)
                record = ReadStatRecord(name=read_id, length=read_to_length_dict[read_id],
                                        is_fl=True, stat=MapStatus.UNIQUELY_MAPPED,
                                        pbid=cid_info[sample_prefix][cid])
                writer.writeRecord(record)
        else:
            # is only potentially unmapped, add all (movie-restricted) members to
            # unmapped holder
            for read_id in read_id_yielder():
                unmapped_holder.add(read_id)

    # now with all the pickles processed we can determine which of all (movie-restricted) FL reads
    # are not mapped in any of the pickles
    unmapped_holder = unmapped_holder.difference(mapped_holder)
    for read_id in unmapped_holder:
        record = ReadStatRecord(name=read_id, length=read_to_length_dict[read_id],
                                is_fl=True, stat=MapStatus.UNMAPPED, pbid=None)
        writer.writeRecord(record)

    writer.close()


def make_abundance_file(read_stat_filename, output_filename, given_total=None,
                        restricted_movies=None, write_header_comments=True):
    """
    Make read mapping abundance file.
    If given_total is not None, use it instead of the total count based on <read_stat_filename>
    given_total should be dict of {fl}
    Parameters:
      read_stat_filename - path to a read status file each line of which is a ReadStatRecord
      output_filename - path to output abundance file.

    total_ids['fl'] --- number of FL, mapped or unmapped
    """
    total_ids = {'fl': set()}

    # pbid, could be NA or None --> # of FL reads mapped to it
    tally = defaultdict(lambda: {'fl': 0})

    reader = ReadStatReader(read_stat_filename)
    for r in reader:
        movie = r.name.split('/')[0]
        if restricted_movies is None or movie in restricted_movies:
            if r.pbid is not None:
                if r.is_fl:  # FL, must be uniquely mapped
                    assert r.is_uniquely_mapped
                    tally[r.pbid]['fl'] += 1
                    total_ids['fl'].add(r.name)
                # ignore non-FL
            else:  # even if it is unmapped it still counts in the abundance total!
                if r.is_fl:
                    total_ids['fl'].add(r.name)

    if given_total is not None:
        use_total_fl = given_total['fl']
    else:
        use_total_fl = len(total_ids['fl'])

    comments = None
    if write_header_comments:
        comments = AbundanceWriter.make_comments(total_fl=use_total_fl)
    writer = AbundanceWriter(output_filename, comments=comments)

    # ("pbid\tcount_fl\tnorm_fl\n")
    keys = tally.keys()
    keys.sort(key=lambda x: map(int, x.split('.')[1:]))  # sort by PB.1, PB.2....
    for pbid in keys:
        v = tally[pbid]
        count_fl = v['fl']
        norm_fl = count_fl*1./use_total_fl
        record = AbundanceRecord(pbid=pbid, count_fl=count_fl, norm_fl=norm_fl)
        writer.writeRecord(record)
    writer.close()
