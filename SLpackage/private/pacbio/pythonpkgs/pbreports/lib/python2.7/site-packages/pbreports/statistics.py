
"""
Basic calculations of sequence statistics.
"""

# XXX keeping this free of PB dependencies
from collections import namedtuple, defaultdict
import functools
import logging
import time
import copy
import math
import sys

import numpy as np

from pbcore.util.statistics import accuracy_as_phred_qv

log = logging.getLogger(__name__)


class Constants(object):
    MAX_QV = 60
    NBINS_CONCORDANCE = 200
    NBINS_READLENGTH = 100
    READ_LENGTH_BIN_SIZES = [100, 200, 500, 1000, 2000]


def mean_int(total, n_items):
    """
    Convenience wrapper for computing an average to the nearest integer
    """
    if n_items == 0:
        return 0
    return int(np.round(total / float(n_items)))


def get_read_length_histogram_bin_width(length_max):
    """
    Heuristic for binning read or subread length distributions in clear and
    intuitive increments.
    """
    for bin_width in Constants.READ_LENGTH_BIN_SIZES:
        if (length_max / float(bin_width)) < Constants.NBINS_READLENGTH:
            return bin_width
    return Constants.READ_LENGTH_BIN_SIZES[-1]


# FIXME need to double-check n95, this may be backwards
def compute_n50_and_n95(readlengths):
    """
    :param contig_lengths: list of contig lengths
    see get_fasta_readlengths
    """
    # this might not be the best expected behavior.
    # this could be a np.array
    if len(readlengths) == 0:
        return (0, 0)
    return _compute_n50_and_n95_np(readlengths)


def _compute_n50_and_n95_np(readlengths):
    """
    Numpy implementation of N50/N95 calculation.
    """
    if isinstance(readlengths, list):
        readlengths = np.array(readlengths)
    readlengths[::-1].sort()  # = np.sort(readlengths)
    total_length = np.sum(readlengths)
    csum = np.cumsum(readlengths)
    half_length = total_length // 2
    csum_n50 = csum[csum > half_length].min()
    i_n50 = np.where(csum == csum_n50)[0]
    n50 = readlengths[i_n50[0]]
    n95_length = int(total_length * 0.05)
    csum_n95 = csum[csum > n95_length].min()
    i_n95 = np.where(csum == csum_n95)[0]
    n95 = readlengths[i_n95[0]]
    return (n50, n95)


def compute_n50(readlengths):
    return compute_n50_and_n95(readlengths)[0]


def compute_n95(readlengths):
    return compute_n50_and_n95(readlengths)[1]


def compute_n50_and_n95_binned(length_counts):
    """
    Alternate form of N50/N95 computation using a numpy.bincount array
    as input (for lower memory overhead).
    """
    lengths = np.arange(len(length_counts))
    lengths_combined = length_counts * lengths
    total_length = np.sum(lengths_combined)
    half_length = total_length // 2
    csum = np.cumsum(lengths_combined)
    n50 = lengths[csum > half_length].min()
    n95_length = int(total_length * 0.95)
    n95 = lengths[csum > n95_length].min()
    return (n50, n95)


def compute_stddev_binned(length_counts):
    n_reads = np.sum(length_counts)
    if n_reads == 0:
        return 0.0
    lengths = np.arange(len(length_counts))
    lengths_combined = length_counts * lengths
    mean = np.sum(lengths_combined) / float(n_reads)
    return math.sqrt(np.sum(length_counts * np.power(lengths - mean, 2)) / n_reads)


class ReadLengthMetrics(namedtuple("ReadLengthMetrics",
                                   ["min", "max", "median", "mean", "n50", "n95", "q95", "nbases", "size", "stddev"])):
    """
    Container for various statistics for read length distributions.
    """

    @staticmethod
    def from_array(readlengths, is_sorted=False):
        """
        Compute metrics given an array of read lengths (assumed to be
        unsorted unless is_sorted=True).
        """
        if len(readlengths) == 0:
            return ReadLengthMetrics(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
        if not is_sorted:
            readlengths.sort()
        median = readlengths[len(readlengths) // 2]
        rl_min, rl_max = readlengths[0], readlengths[-1]
        q95_idx = min(int(math.floor(0.95 * len(readlengths))),
                      len(readlengths) - 1)
        q95 = readlengths[q95_idx]
        # this re-sorts in descending order!
        n50, n95 = _compute_n50_and_n95_np(readlengths)
        return ReadLengthMetrics(
            min=rl_min,
            max=rl_max,
            median=median,
            mean=int(round(readlengths.mean())),
            n50=n50,
            n95=n95,
            q95=q95,
            nbases=np.sum(readlengths),
            size=int(readlengths.size),
            stddev=np.std(readlengths))

    def with_size(self, n):
        """
        Return a copy that reports a different size than the length of the
        input array.  This allows us to report the total number of *unique*
        subreads, which due to the presence of supplemental alignments may
        not be the same as the number of mapped segments.
        """
        return ReadLengthMetrics(self.min, self.max, self.median, self.mean, self.n50, self.n95, self.q95, self.nbases, n, self.stddev)

    @staticmethod
    def from_bincounts(length_counts):
        """
        A more memory-efficient factory method that uses a numpy.bincount
        array instead of individual read lengths.
        """
        if len(length_counts) == 0 or all(length_counts == 0):
            return ReadLengthMetrics(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
        lengths = np.arange(len(length_counts))
        nonzero_sel = length_counts != 0
        have_lengths = lengths[nonzero_sel]
        counts_csum = np.cumsum(length_counts)
        n_reads = np.sum(length_counts)
        sel_median = counts_csum > n_reads // 2
        median = lengths[sel_median][0]
        rl_min = have_lengths.min()
        rl_max = have_lengths.max()
        q95_sel = counts_csum > int(math.floor(0.95 * n_reads))
        q95 = lengths[q95_sel][0]
        n_bases = np.sum(have_lengths * length_counts[nonzero_sel])
        n50, n95 = compute_n50_and_n95_binned(length_counts)
        stddev = compute_stddev_binned(length_counts)
        return ReadLengthMetrics(
            min=rl_min,
            max=rl_max,
            median=median,
            mean=int(round(n_bases / float(n_reads))),
            n50=n50,
            n95=n95,
            q95=q95,
            nbases=n_bases,
            size=n_reads,
            stddev=stddev)


class Subreads(namedtuple("Subreads",
                          ["holeNumber", "qStart", "qEnd", "contextFlag", "qLen"])):
    """
    Container for sub-arrays defining a collection of subreads, usually
    corresponding to a single ZMW.

    Note that this class is also be used to mock the PacBioBamIndex in unit
    tests.

    :param holeNumber: either a single ZMW or an int array
    :param qStart: numpy array of subread start points in zmw read
    :param qEnd: numpy array of subread end points in zmw read
    :param contextFlag: numpy array of subread adapter context
    """

    def __len__(self):
        return len(self.qStart)

    @property
    def hq_read_length(self):
        """
        Approximate length of the high-quality region of the read (minus
        adapter sequences), defined as the end of the last subread minus the
        start of the first subread.
        """
        return self.qEnd.max() - self.qStart.min()

    @property
    def mean_subread_length(self):
        """
        Mean length of subreads.
        """
        return int(round(self.qLen.mean()))

    @property
    def insert_length(self):
        """
        Approximate insert length, defined as:
        1) If there are no adapters, return the hq-region length.
        2) If there is 1 adapter found, return the longer of the two partial
           passes (full pass is a subread anchored by 2 adapters, here there
           is only 1 adapter).
        3) Else, return the max subread length (we have at least 1 full pass,
           subread anchored by 2 adapters).
        """
        return int(round(self.qLen.max()))


def _iter_subreads_by_zmw(pb_index):
    """
    Memory-efficient loop over subreads per ZMW.

    :return: yield a tuple of (zmw, qStart, qEnd) [int, array, array]
    """
    k = 0
    hole_numbers = pb_index.holeNumber
    qStart = pb_index.qStart
    qEnd = pb_index.qEnd
    contextFlag = pb_index.contextFlag
    # this takes advantage of the sort order to use a single loop over the
    # index
    total_length = len(pb_index)
    while k < total_length:
        last_zmw = hole_numbers[k]
        records = []
        while k < total_length and hole_numbers[k] == last_zmw:
            records.append(k)
            k += 1
        qs = qStart[records]
        qe = qEnd[records]
        yield Subreads(last_zmw, qs, qe, contextFlag[records], qe - qs)


zmw_reads = namedtuple("ZmwReads", ["read", "insert", "subread"])


class ZmwReadDistributions(zmw_reads):
    """
    Container for multiple data arrays of sequence lengths (ZMW read, insert,
    or individual subreads).  Note that while the first two arrays have the
    same length (i.e. a 1-to-1 relationship to ZMWs), the subread lengths are
    treated separately rather than take the mean for each ZMW read.
    """

    @staticmethod
    def empty():
        return ZmwReadDistributions(np.array([], dtype=int),
                                    np.array([], dtype=int),
                                    np.array([], dtype=int))

    # FIXME obsolete, but convenient for unit testing with mocked inputs
    @staticmethod
    def from_pb_index(pb_index):
        def _iter_reads():
            for read in _iter_subreads_by_zmw(pb_index):
                yield read.hq_read_length
                yield read.insert_length
        zmw_data = np.fromiter(_iter_reads(), int)
        srl = pb_index.qEnd - pb_index.qStart
        return ZmwReadDistributions(zmw_data[0::2], zmw_data[1::2], srl)

    def to_metrics(self):
        """
        Convert distributions (in raw data arrays) to an equivalent set of
        ReadLengthMetrics objects
        """
        return zmw_reads(
            ReadLengthMetrics.from_array(self.read),
            ReadLengthMetrics.from_array(self.insert),
            ReadLengthMetrics.from_array(self.subread))

    def to_metrics_binned_subreads(self):
        """
        Convert distributions to an equivalent set of ReadLengthMetrics
        objects, with the subreads represented by a bincount array to save
        memory.
        """
        return zmw_reads(
            # note that we convert the read/insert arrays to bincount first
            # to avoid sorting them in place
            ReadLengthMetrics.from_bincounts(np.bincount(self.read)),
            ReadLengthMetrics.from_bincounts(np.bincount(self.insert)),
            ReadLengthMetrics.from_bincounts(self.subread))

    def combine_binned(self, other):
        """
        Combine with another distribution, assuming that both have actual
        data points for read and insert lengths, and binned subread length
        counts.
        """
        return ZmwReadDistributions(
            read=np.concatenate([self.read, other.read]),
            insert=np.concatenate([self.insert, other.insert]),
            subread=combine_bincounts(self.subread, other.subread))


def combine_bincounts(*bins):
    combined = np.zeros(max([len(b) for b in bins]), dtype=int)
    for b in bins:
        if len(b) == 0:
            continue
        combined[0:len(b)] += b
    return combined


def expand_bincounts(counts):
    if len(counts) == 0:
        return np.array([], dtype=int)

    def _iter_counts():
        for value, count in enumerate(counts[1:], start=1):
            for k in range(count):
                yield value
    return np.fromiter(_iter_counts(), int)


def rebin_histogram(bincounts, bin_size):
    """
    Take an array from numpy.bincount and compress into a histogram with the
    specified bin size.
    """
    bins = []
    k = 0
    while k * bin_size < len(bincounts):
        bins.append(np.sum(bincounts[k * bin_size:(k + 1) * bin_size]))
        k += 1
    return bins


class Histogram2D(namedtuple("Histogram2D", ["data", "xedges", "yedges"])):
    """
    Container for storing pre-calculated 2D histograms (i.e. from numpy's
    histogram2d() function) with any combination of scales and binning.
    """

    def __add__(self, other):
        assert self.xedges.size == other.xedges.size
        assert (self.xedges == other.xedges).all()
        assert self.yedges.size == other.yedges.size
        assert (self.yedges == other.yedges).all()
        return Histogram2D(self.data + other.data, self.xedges, self.yedges)

    def __len__(self):
        return self.data.size

    def crop(self, x_limits=None, y_limits=None):
        """
        Trim the histogram (and associated edges) to only span the range of
        populated values.
        """
        def _get_bin_range(f_get_data, edges):
            idx_start = 0
            idx_end = len(edges)
            for i_bin in xrange(0, len(edges) - 1):
                bin_data = f_get_data(i_bin)
                if not (bin_data == 0.0).all():
                    idx_start = i_bin
                    break
            for i_bin in xrange(idx_end - 2, 0, -1):
                bin_data = f_get_data(i_bin)
                if not (bin_data == 0.0).all():
                    idx_end = i_bin + 1
                    break
            return (idx_start, idx_end)

        def _get_bin_edge_range(edges, limits):
            idx_start = 0
            idx_end = len(edges)
            idx_range = np.where(
                (edges >= limits[0]) & (edges <= limits[1]))[0]
            if len(idx_range) != 0:
                idx_start, idx_end = idx_range[0], idx_range[-1] + 1
            return idx_start, idx_end
        if x_limits is not None:
            x_idx_start, x_idx_end = _get_bin_edge_range(self.xedges, x_limits)
        else:
            x_idx_start, x_idx_end = _get_bin_range(
                lambda i: self.data[i], self.xedges)
        if y_limits is not None:
            y_idx_start, y_idx_end = _get_bin_edge_range(self.yedges, y_limits)
        else:
            y_idx_start, y_idx_end = _get_bin_range(
                lambda i: self.data[:, i], self.yedges)
        x_edges = self.xedges[x_idx_start:x_idx_end + 1]
        y_edges = self.yedges[y_idx_start:y_idx_end + 1]
        data_trunc1 = self.data[x_idx_start:x_idx_end]
        data_trunc2 = []
        for y_bin in data_trunc1:
            data_trunc2.append(y_bin[y_idx_start:y_idx_end])
        return Histogram2D(np.array(data_trunc2), x_edges, y_edges)


def to_readlength_accuracy_histogram2d(read_lengths,
                                       accuracy,
                                       length_max=100000,
                                       bin_sizes=(0.005, 100)):
    """
    Compute a 2D histogram of read length and "accuracy" (concordance, etc.)
    on known scales.  By default this histogram will be 100000 elements, and
    histograms from different chunks may be added, so the total memory usage
    is constant.
    """
    x_bins = int(length_max / bin_sizes[1])
    y_bins = int(0.5 / bin_sizes[0])
    H, xedges, yedges = np.histogram2d(read_lengths, accuracy,
                                       bins=(x_bins, y_bins), # must be ints
                                       range=[[0, length_max], [0.5, 1.0]])
    return Histogram2D(H, xedges, yedges)


def combine_histogram2d(histograms):
    if len(histograms) == 0:
        return Histogram2D(np.array([]), np.array([]), np.array([]))
    total = histograms[0]
    for other in histograms[1:]:
        total = total + other
    return total


def _get_zmw_metrics(pb_index, zmw_index):
    qStart = pb_index.qStart
    qEnd = pb_index.qEnd
    n_zmws = len(zmw_index)
    n_subreads = len(pb_index)
    subread_lengths = qEnd - qStart
    subread_length_counts = np.bincount(subread_lengths)

    def _iter_read_insert_lengths():
        for i_zmw, idx in enumerate(zmw_index):
            range_max = n_subreads
            if i_zmw < n_zmws - 1:
                range_max = zmw_index[i_zmw + 1]
            zmw_len = qEnd[range_max - 1] - qStart[idx]
            zmw_qlen = subread_lengths[idx:range_max]
            hq_start = qStart[idx].min()
            insert_len = zmw_qlen.max()
            insert_median = sorted(zmw_qlen)[min(len(zmw_qlen) / 2, n_subreads - 1)]
            yield zmw_len
            yield insert_len
            yield insert_median
            yield hq_start
    read_insert_lengths = np.fromiter(_iter_read_insert_lengths(), dtype=int)
    unique_molecular_yield = np.sum(read_insert_lengths[2::4])
    hq_start = read_insert_lengths[3::4]
    return ZmwReadDistributions(
        read=read_insert_lengths[0::4],
        insert=read_insert_lengths[1::4],
        subread=subread_length_counts), unique_molecular_yield, hq_start


def collect_streamed_pbi_read_distributions(pbi_stream):
    unique_molecular_yield = 0
    combined = ZmwReadDistributions.empty()
    for i_chunk, (pb_index, zmw_index) in enumerate(pbi_stream.iter_with_zmw_index()):
        log.debug("Collecting read metrics from chunk %d", i_chunk)
        metrics, chunk_yield, hq_start = _get_zmw_metrics(pb_index, zmw_index)
        unique_molecular_yield += chunk_yield
        combined = combined.combine_binned(metrics)
        log.debug("Done with chunk")
        log.debug("chunk metrics: %s", metrics)
        log.debug("chunk unique_molecular_yield: %s", chunk_yield)
    return combined, unique_molecular_yield
