"""Summarize the Long Amplicon Analysis using the ZMW results"""

from collections import defaultdict, namedtuple
from pprint import pformat
import logging
import csv
import os.path as op
import os
import sys

from pbcore.io import SubreadSet, BarcodeSet
from pbcommand.models.report import Report, Table, Column, PlotGroup
from pbcommand.models import FileTypes, get_pbparser
from pbcommand.cli import pbparser_runner
from pbcommand.utils import setup_log

from pbreports.plot.plotly_helper import to_simple_layout, to_plotly_plot, to_heatmap
from pbreports.io.specs import load_spec

log = logging.getLogger(__name__)

__version__ = '0.2.1'


class Constants(object):
    TOOL_ID = "pbreports.tasks.amplicon_analysis_input"
    DRIVER_EXE = "python -m pbreports.report.amplicon_analysis_input --resolved-tool-contract "
    R_ID = "amplicon_analysis_input"
    DATA_GOOD = "good"
    DATA_CHIMERA = "chimera"
    DATA_NOISE = "noise"
    T_R = "result_table"
    C_BC = "barcode_col"
    C_GOOD = "good"
    C_GOOD_PCT = "good_pct"
    C_CHIM = "chimera"
    C_CHIM_PCT = "chimera_pct"
    C_NOISE = "noise"
    C_NOISE_PCT = "noise_pct"

    # optional plot (LAAgc only)
    PG_MAPPED_ID = "laagc_mapping"
    P_MAPPED_ID = "mapped_sureads"


spec = load_spec(Constants.R_ID)


def create_table(summary_csv):
    """Long Amplicon Analysis results table"""

    columns = []
    columns.append(Column(Constants.C_BC))
    columns.append(Column(Constants.C_GOOD))
    columns.append(Column(Constants.C_GOOD_PCT))
    columns.append(Column(Constants.C_CHIM))
    columns.append(Column(Constants.C_CHIM_PCT))
    columns.append(Column(Constants.C_NOISE))
    columns.append(Column(Constants.C_NOISE_PCT))

    t = Table(Constants.T_R, columns=columns)

    COL_IDS = [Constants.C_GOOD, Constants.C_GOOD_PCT, Constants.C_CHIM,
               Constants.C_CHIM_PCT, Constants.C_NOISE, Constants.C_NOISE_PCT]

    def add_column(barcode_id, n_good, n_chimera, n_noise):
        pct_good = pct_chimera = pct_noise = 0
        total = n_good + n_chimera + n_noise
        if total > 0:
            pct_good = n_good / float(total)
            pct_chimera = n_chimera / float(total)
            pct_noise = n_noise / float(total)
        values = [n_good, pct_good, n_chimera, pct_chimera, n_noise, pct_noise]
        t.add_data_by_column_id(Constants.C_BC, bc_id)
        for column_id, value in zip(COL_IDS, values):
            t.add_data_by_column_id(column_id, value)

    with open(summary_csv) as csv_in:
        reader = csv.reader(csv_in, delimiter=',')
        reader.next()
        for rec in reader:
            assert len(rec) == 7, rec
            bc_id = rec[0]
            if bc_id == "All":
                continue
            add_column(bc_id, int(rec[1]), int(rec[3]), int(rec[5]))
    n_good = sum(t.get_column_by_id(Constants.C_GOOD).values)
    n_chimera = sum(t.get_column_by_id(Constants.C_CHIM).values)
    n_noise = sum(t.get_column_by_id(Constants.C_NOISE).values)
    add_column("All", n_good, n_chimera, n_noise)
    return t


DataPoint = namedtuple("DataPoint", ["x", "y", "z"])


def _sort_by_barcode_order(data, barcode_set):
    rec_idx = {rec.id: k for k, rec in enumerate(barcode_set)}

    def __cmp(a, b):
        bc_a = a.y.split("--")
        bc_b = b.y.split("--")
        if len(bc_a) != 2 or len(bc_b) != 2:
            return cmp(0, 0)
        idx_a = (rec_idx.get(bc_a[0], 0), rec_idx.get(bc_a[1], 0))
        idx_b = (rec_idx.get(bc_b[0], 0), rec_idx.get(bc_b[1], 0))
        return cmp(idx_b, idx_a)

    data.sort(__cmp)


def plot_mapped_loci_heatmap(locus_csv, file_name, barcode_set=None):
    """
    Generate a Plotly heatmap of number of mapped subreads per locus (X axis)
    per barcode (Y axis).
    """
    by_barcode = defaultdict(dict)
    loci = set()
    with open(locus_csv, "r") as csv_file:
        for rec in csv.DictReader(csv_file):
            if rec['Locus'] == "All":
                continue
            loci.add(rec['Locus'])
            by_barcode[rec['Barcode']][rec['Locus']] = int(rec['Subreads'])
    barcodes = sorted(by_barcode.keys())
    loci = sorted(list(loci))
    data = []
    for barcode in barcodes:
        for locus in loci:
            z_value = by_barcode[barcode].get(locus, 0)
            data.append(DataPoint(locus, barcode, z_value))
    if barcode_set is not None:
        _sort_by_barcode_order(data, barcode_set)
    else:
        log.warn("Original BarcodeSet not available, samples will be unsorted")
    y = [p.y for p in data]
    label_width = max(100, 10 * max([len(bc_label) for bc_label in y]))
    heatmap = to_heatmap(
        x=[p.x for p in data],
        y=y,
        z=[p.z for p in data],
        colorscale="Viridis")
    layout = to_simple_layout("Locus", "Barcode", "Mapped Subreads",
                              margin=dict(l=label_width))
    plot = to_plotly_plot(Constants.P_MAPPED_ID, file_name, heatmap, layout)
    return PlotGroup(Constants.PG_MAPPED_ID, plots=[plot])


def run_to_report(summary_csv, mapped_locus_csv=None, base_dir=None,
                  barcoded_subreads=None):
    log.info("Generating PCR report v{v} from summary '{s}'".format(
             v=__version__,
             s=summary_csv))
    table = create_table(summary_csv)
    plotgroups = []
    dataset_uuids = []
    if mapped_locus_csv is not None:
        assert barcoded_subreads is not None
        barcode_set = None
        with SubreadSet(barcoded_subreads) as ds_in:
            dataset_uuids.append(ds_in.uuid)
            for ext_res in ds_in.externalResources:
                if ext_res.barcodes is not None:
                    try:
                        barcode_set = BarcodeSet(ext_res.barcodes)
                        dataset_uuids.append(barcode_set.uuid)
                    except IOError as e:
                        log.error(e)
                        log.warn("Samples will be in unsorted order")
                    finally:
                        break
        json_file = op.join(base_dir, "mapped_loci.json.gz")
        plotgroups.append(plot_mapped_loci_heatmap(mapped_locus_csv, json_file, barcode_set))
    r = Report(Constants.R_ID, tables=[table], plotgroups=plotgroups,
               dataset_uuids=dataset_uuids)
    return spec.apply_view(r)


def make_report(summary_csv, report_json, mapped_locus_csv=None,
                barcoded_subreads=None):
    log.info("Running {f} v{v}.".format(f=op.basename(__file__), v=__version__))
    base_dir = op.dirname(report_json)
    report = run_to_report(summary_csv, mapped_locus_csv, base_dir, barcoded_subreads)
    log.info(pformat(report.to_dict()))
    report.write_json(report_json)
    return 0


def _args_runner(args):
    return make_report(args.report_csv, args.report_json)


def _resolved_tool_contract_runner(rtc):
    return make_report(rtc.task.input_files[0], rtc.task.output_files[0])


def _add_options_to_parser(p):
    p.add_input_file_type(
        FileTypes.CSV,
        file_id="report_csv",
        name="Consensus Summary CSV",
        description="Consensus summary CSV by barcode")
    p.add_output_file_type(
        FileTypes.REPORT,
        file_id="report_json",
        name="LAA Input Report",
        description="Summary of input amplicon quality",
        default_name="amplicon_input_report")
    return p


def _get_parser(tool_id=Constants.TOOL_ID, driver_exe=Constants.DRIVER_EXE):
    p = get_pbparser(
        tool_id,
        __version__,
        "Amplicon Analysis Input",
        __doc__,
        driver_exe)
    return _add_options_to_parser(p)


def main(argv=sys.argv):
    return pbparser_runner(argv[1:],
                           _get_parser(),
                           _args_runner,
                           _resolved_tool_contract_runner,
                           log,
                           setup_log)


# for 'python -m pbreports.report.amplicon_analysis_input ...'
if __name__ == "__main__":
    sys.exit(main())
