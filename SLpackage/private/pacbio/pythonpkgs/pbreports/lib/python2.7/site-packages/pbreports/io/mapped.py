"""
Collection of statistics for mapped reads.  Despite its location, this module
does very little IO, but it does mostly use PacBioBamIndex objects as input.
"""

from collections import namedtuple, defaultdict, Counter
import functools
import logging
import time
import copy
import math
import os.path as op
import sys

import numpy as np

from pbcommand.models.common import PacBioAlarm

from pbreports.statistics import ReadLengthMetrics, combine_bincounts, expand_bincounts, Constants
from pbreports.util import reraise

log = logging.getLogger(__name__)


def _to_binned_identities(identity):
    # XXX this uses np.ceil for consistency with the old mapping stats report,
    # should we be using np.floor instead?
    identity_pct = np.ceil(identity * Constants.NBINS_CONCORDANCE).astype(int)
    return np.bincount(identity_pct)


def _to_concordance_histogram(bins):
    nbins_all = Constants.NBINS_CONCORDANCE
    plot_x = np.array(range(nbins_all + 1), dtype=float) / nbins_all
    plot_y = np.array([0] * (nbins_all + 1))
    nbins = len(bins)
    assert nbins <= (nbins_all + 1), nbins
    plot_y[0:nbins] = bins
    nonzero_x = plot_x[plot_y > 0]
    if len(nonzero_x) > 0:
        min_x, max_x = nonzero_x[0], nonzero_x[-1]
        sel_range = (plot_x >= min_x) & (plot_x <= max_x)
        plot_x, plot_y = plot_x[sel_range], plot_y[sel_range]
    return plot_x, plot_y


class MovieAlignmentInfo(namedtuple("MovieInfo",
                                    ["movie_name", "read_bins", "subread_bins", "identity_sum", "n_reads", "n_subreads", "identity_bins"])):

    @staticmethod
    def from_raw_arrays(movie_name, read_lengths, subread_lengths, identities, n_subreads):
        identity_bins = _to_binned_identities(identities)
        return MovieAlignmentInfo(movie_name,
                                  np.bincount(read_lengths),
                                  np.bincount(subread_lengths),
                                  np.sum(identities),
                                  len(read_lengths),
                                  n_subreads,
                                  identity_bins)

    def read_lengths(self):
        return expand_bincounts(self.read_bins)

    def read_metrics(self):
        return ReadLengthMetrics.from_bincounts(self.read_bins)

    def subread_lengths(self):
        return expand_bincounts(self.subread_bins)

    def subread_metrics(self):
        return ReadLengthMetrics.from_bincounts(self.subread_bins).with_size(self.n_subreads)

    @property
    def n_alignments(self):
        return np.sum(self.subread_bins)

    @property
    def mean_identity(self):
        n_alignments = self.n_alignments
        if n_alignments == 0:
            return 0.0
        return self.identity_sum / n_alignments

    def combine_with(self, *others):
        """
        Combine these metrics with a sequence of other metrics objects.
        These must be from the same movie unless self.movie_name is None.
        """
        if self.movie_name is not None:
            assert all([self.movie_name == o.movie_name for o in others]
                       ), "Expected movie_name={n}".format(n=self.movie_name)
        return MovieAlignmentInfo(
            self.movie_name,
            combine_bincounts(self.read_bins,
                              *[o.read_bins for o in others]),
            combine_bincounts(self.subread_bins,
                              *[o.subread_bins for o in others]),
            sum([self.identity_sum] + [o.identity_sum for o in others]),
            sum([self.n_reads] + [o.n_reads for o in others]),
            sum([self.n_subreads] + [o.n_subreads for o in others]),
            combine_bincounts(self.identity_bins,
                              *[o.identity_bins for o in others]))

    def get_subread_concordance_histogram(self):
        """
        Convert the binned rounded identities to a series of X and Y values
        on the appropriate scale.
        """
        return _to_concordance_histogram(self.identity_bins)


MappingMetrics = namedtuple(
    "MappingMetrics", ["movie_name", "info", "reads", "subreads"])


def combine_chunked_alignment_metrics(chunk_metrics):
    """
    Take a list of dicts output by collect_pbi_alignment_metrics,
    and combine information from different movies as well as computing
    overall metrics.
    """
    total = MovieAlignmentInfo(None, [], [], 0, 0, 0, [])
    by_movie = {}
    for chunk in chunk_metrics:
        for movie_id, stats in chunk.iteritems():
            if not movie_id in by_movie:
                by_movie[movie_id] = stats
            else:
                by_movie[movie_id] = by_movie[movie_id].combine_with(stats)
    # we pre-calculate the read/subread statistics now to share among
    # various report functions
    m_by_movie = {movie_name: MappingMetrics(movie_name, info, info.read_metrics(
    ), info.subread_metrics()) for movie_name, info in by_movie.iteritems()}
    if len(by_movie.keys()) == 1:  # hacky optimization
        m_movie = list(m_by_movie.values())[0]  # py3 compatibility
        m_combined = MappingMetrics(
            None, m_movie.info, m_movie.reads, m_movie.subreads)
    else:
        combined = total.combine_with(*(by_movie.values()))
        m_combined = MappingMetrics(
            "All Movies", combined, combined.read_metrics(), combined.subread_metrics())
    return m_combined, m_by_movie


def collect_pbi_alignment_metrics(pb_index, movie_names, ignore_negative=False):
    """
    Collect alignment data from a single PacBioBamIndex object, and return
    a dictionary of MovieAlignmentInfo objects keyed by movie name.  This
    assumes that all mapped subreads for a single ZMW are present in the
    same file, which is the case for chunked pbsmrtpipe jobs.
    """
    if len(pb_index) == 0:
        return {}
    qId = pb_index.qId
    holeNumber = pb_index.holeNumber
    aStart = pb_index.aStart
    aEnd = pb_index.aEnd
    qStart = pb_index.qStart
    subread_lengths = np.int32(aEnd) - aStart
    identity = pb_index.identity
    bad_sel = (subread_lengths <= 0) | (identity < 0)
    good_sel = np.invert(bad_sel)
    if np.any(bad_sel):
        n_bad = Counter(bad_sel)[True]
        msg = "{n} alignments found with negative lengths or computed sequence identity - this indicates that the .pbi index is corrupt".format(n=n_bad)
        bad_qId = qId[bad_sel]
        bad_zmw = holeNumber[bad_sel]
        bad_qstart = qStart[bad_sel]
        bad_astart = aStart[bad_sel]
        bad_aend = aEnd[bad_sel]
        for rg, zmw, qs, a, e in zip(bad_qId, bad_zmw, bad_qstart, bad_astart, bad_aend):
            log.error("BAD ALIGNMENT: qId={q} ZMW={z} qStart={s} aStart={a} aEnd={e}".format(
                      q=rg, z=zmw, s=qs, a=a, e=e))
        if not op.exists("alarms.json"):
            alarm = PacBioAlarm(
                exception=None,
                info=None,
                message=msg,
                name="Bad Alignments Found",
                severity=logging.ERROR,
                owner="pbreports.report.mapping_stats")
            alarm.to_json("alarms.json")
        if ignore_negative:
            log.error(msg)
            log.warn("Will continue with bad alignments discarded, but this may impact the accuracy of the reported metrics")
            qId = qId[good_sel]
            holeNumber = holeNumber[good_sel]
            aStart = aStart[good_sel]
            aEnd = aEnd[good_sel]
            qStart = qStart[good_sel]
            subread_lengths = subread_lengths[good_sel]
            identity = identity[good_sel]
        else:
            raise ValueError(msg)
    by_movie = {}
    for rg_id in movie_names.keys():
        try:
            sel = qId == rg_id
            rg_identity = identity[sel]
            rg_srl = subread_lengths[sel]
            rg_zmws = holeNumber[sel]
            rg_aStart = aStart[sel]
            rg_aEnd = aEnd[sel]
            rg_qStart = qStart[sel]
            have_subreads = set()
            zmw_start = defaultdict(lambda: sys.maxint)
            zmw_end = defaultdict(lambda: 0)
            subread_start = defaultdict(lambda: sys.maxint)
            subread_end = defaultdict(lambda: 0)
            for i_subread, (zmw, qs) in enumerate(zip(rg_zmws, rg_qStart)):
                zmw_start[zmw] = min(rg_aStart[i_subread], zmw_start[zmw])
                zmw_end[zmw] = max(rg_aEnd[i_subread], zmw_end[zmw])
                if zmw_start[zmw] >= zmw_end[zmw]:
                    msg = 'For zmw {} (subread {}), start >= end ({} >= {})'.format(
                        zmw, i_subread, zmw_start[zmw], zmw_end[zmw])
                    raise Exception(msg)
                if not (zmw, qs) in have_subreads:
                    have_subreads.add((zmw, qs))
            read_lengths = [zmw_end[z] - zmw_start[z] for z in zmw_start.keys()]
            movie_stats = MovieAlignmentInfo.from_raw_arrays(
                movie_names[rg_id], read_lengths, subread_lengths[sel], rg_identity, len(have_subreads))
            by_movie[movie_names[rg_id]] = movie_stats
        except Exception as exc:
            extra_msg = ': Error collecting metrics for movie "{}" (@RG ID:{:08x})'.format(
                movie_names[rg_id], (rg_id & ((1 << 32) - 1)))
            reraise(type(exc), type(exc)(str(exc) + extra_msg), sys.exc_info()[2])
    return by_movie


def collect_ccs_pbi_alignment_metrics(pb_index, movie_names):
    """
    Equivalent to collect_pbi_alignment_metrics but operates on the index of a
    CCS BAM file, which results in a much simpler (and faster) function.
    """
    if len(pb_index) == 0:
        return {}
    identity = pb_index.identity
    read_lengths = pb_index.aEnd - pb_index.aStart
    by_movie = {}
    for rg_id in movie_names.keys():
        sel = pb_index.qId == rg_id
        zmws = pb_index.holeNumber[sel]
        movie_identity = identity[sel]
        movie_read_lengths = read_lengths[sel]
        movie_stats = MovieAlignmentInfo.from_raw_arrays(
            movie_names[rg_id], movie_read_lengths, movie_read_lengths, movie_identity, len(set(zmws)))
        by_movie[movie_names[rg_id]] = movie_stats
    return by_movie


def get_best_length_binning(totalLength, numRecords):
    """
    Estimate appropriate histogram bin size based on the total length and
    number of records extracted from dataset XML.
    """
    if totalLength > 0 and numRecords > 0:
        mean_subread_length = totalLength / numRecords
        if mean_subread_length > 5000:
            return 100
        elif mean_subread_length > 2500:
            return 50
    return 25
