
"""
Tool for subsetting a BAM or PacBio DataSet file based on either a whitelist of
hole numbers or a percentage of reads to be randomly selected.
"""

from __future__ import division
from __future__ import print_function
from collections import defaultdict, OrderedDict
import subprocess
import warnings
import logging
import random
import shutil
import os.path as op
import re
import sys

try:
    from pysam.calignmentfile import AlignmentFile  # pylint: disable=no-name-in-module, import-error, fixme, line-too-long
except ImportError:
    from pysam.libcalignmentfile import AlignmentFile  # pylint: disable=no-name-in-module, import-error, fixme, line-too-long

from pbcommand.common_options import (add_log_quiet_option,
                                      add_log_verbose_option)
from pbcommand.cli import (pacbio_args_runner,
                           get_default_argparser_with_base_opts)
from pbcommand.utils import setup_log
from pbcore.io import openDataFile, openDataSet, BamReader, IndexedBamReader, ReadSet

VERSION = "0.2.0"

log = logging.getLogger(__name__)


def _process_zmw_list(zmw_list):
    zmws = set()
    if zmw_list is None:
        return zmws
    elif isinstance(zmw_list, set):
        return zmw_list
    elif isinstance(zmw_list, (list, tuple)):
        return set(zmw_list)
    elif op.isfile(zmw_list):
        base, ext = op.splitext(zmw_list)
        if ext in [".bam", ".xml"]:
            with openDataFile(zmw_list) as ds_zmw:
                for f in ds_zmw.resourceReaders():
                    zmws.update(set(list(f.holeNumber)))
        else:
            with open(zmw_list) as f:
                lines = f.read().splitlines()
                zmws.update(set([int(x) for x in lines]))
    else:
        zmws.update(set([int(x) for x in zmw_list.split(",")]))
    return zmws


def _make_qname(movie, zmw, start, stop):
    return '{}/{}/{}_{}'.format(movie, zmw, start, stop)


def _make_qname_from_index_row(qid2mov, bam, i_rec):
    qid = bam.qId[i_rec]
    movie_name = qid2mov[qid]
    zmw = bam.holeNumber[i_rec]
    start = bam.qStart[i_rec]
    stop = bam.qEnd[i_rec]
    return _make_qname(movie_name, zmw, start, stop)


def _process_subread_list(subread_list):
    def _get_subreads_from_dataset(subread_list):
        with openDataFile(subread_list) as ds_in:
            if ds_in.isIndexed:
                qid = ds_in.index.qId
                mname = [ds_in.qid2mov[q] for q in qid]
                zmws = ds_in.index.holeNumber
                start = ds_in.index.qStart
                stop = ds_in.index.qEnd
                return set([_make_qname(*x) for x in zip(mname, zmws, start, stop)])
            else:
                subreads = set()
                for record in ds_in:
                    subreads.add(record.qName)
                return subreads

    subreads = set()
    if subread_list is None:
        return subreads
    elif isinstance(subread_list, set):
        return subread_list
    elif isinstance(subread_list, (list, tuple)):
        return set(subread_list)
    elif op.isfile(subread_list):
        base, ext = op.splitext(subread_list)
        if ext in ['.bam', '.xml']:
            subreads = _get_subreads_from_dataset(subread_list)
        else:
            with open(subread_list) as f:
                lines = f.read().splitlines()
                subreads.update(set(lines))
    else:
        subreads.update(set(subread_list.split(',')))
    return subreads


def _anonymize_sequence(rec):
    rseq_ = [random.randint(0, 3) for i in range(len(rec.query_sequence))]
    rseq = "".join(["ACTG"[i] for i in rseq_])
    rec.query_sequence = rseq
    return rec


def _create_whitelist(bam_readers, percentage=None, count=None):
    zmws = set()
    movies = set()
    for i_file, bam in enumerate(bam_readers):
        movies.update(set([rg["MovieName"] for rg in bam.readGroupTable]))
        zmws.update(set(bam.pbi.holeNumber))
    if len(movies) > 1:
        warnings.warn("The input BAM/dataset contains multiple movies, " +
                      "which may have overlapping ZMWs.")
    if percentage is not None:
        count = int(len(zmws) * percentage / 100.0)
    zmws = list(zmws)
    if count >= len(zmws):
        warnings.warn("Count exceeds total number of ZMWs (%d > %d); will output all records" % (
            count, len(zmws)))
        return set(zmws)
    have_zmws = set()
    whitelist = set()
    k = 0
    while k < count:
        i_zmw = random.randint(0, len(zmws) - 1)
        if not zmws[i_zmw] in have_zmws:
            whitelist.add(zmws[i_zmw])
            have_zmws.add(zmws[i_zmw])
            k += 1
    return whitelist


def _process_bam_whitelist(bam_in, bam_out, whitelist, blacklist,
                           use_barcodes=False, anonymize=False,
                           use_subreads=False, qid2mov=None):

    def _is_whitelisted(x):
        if ((len(whitelist) > 0 and x in whitelist) or
                (len(blacklist) > 0 and not x in blacklist)):
            return True

    def _add_read(i_rec):
        rec = bam_in[i_rec]
        if anonymize:
            _anonymize_sequence(rec.peer)
        bam_out.write(rec.peer)
        have_zmws.add(rec.holeNumber)
        have_records.append(i_rec)

    have_zmws = set()
    have_records = []
    if use_barcodes:
        for i_rec in range(len(bam_in.holeNumber)):
            bc_fwd = bam_in.bcForward[i_rec]
            bc_rev = bam_in.bcReverse[i_rec]
            if _is_whitelisted(bc_fwd) or _is_whitelisted(bc_rev):
                _add_read(i_rec)
    elif use_subreads:
        for i_rec in range(len(bam_in.holeNumber)):
            qname = _make_qname_from_index_row(qid2mov, bam_in, i_rec)
            if _is_whitelisted(qname):
                _add_read(i_rec)
    else:
        for i_rec, zmw in enumerate(bam_in.holeNumber):
            if _is_whitelisted(zmw):
                _add_read(i_rec)
    return len(have_records), have_zmws


def filter_reads(input_bam,
                 output_bam,
                 whitelist=None,
                 blacklist=None,
                 percentage=None,
                 count=None,
                 seed=None,
                 ignore_metadata=False,
                 relative=None,
                 anonymize=False,
                 use_barcodes=False,
                 sample_scraps=False,
                 keep_original_uuid=False,
                 use_subreads=False):
    if output_bam is None:
        log.error("Must specify output file")
        return 1
    output_bam = op.abspath(output_bam)
    if not op.isdir(op.dirname(output_bam)):
        log.error("Output path '{d}' does not exist.".format(
                  d=op.dirname(output_bam)))
        return 1
    n_specified = 4 - [whitelist, blacklist, percentage, count].count(None)
    if n_specified != 1:
        log.error("You must choose one and only one of the following " +
                  "options: --whitelist, --blacklist, --count, --percentage")
        return 1
    if seed is not None:
        random.seed(seed)
    if whitelist is None and blacklist is None:
        if not 0 < percentage < 100 and not count > 0:
            log.error("No reads selected for output.")
            return 1
    output_ds = base_name = None
    if output_bam.endswith(".xml"):
        if not input_bam.endswith(".xml"):
            log.warning("DataSet output only supported for DataSet inputs.")
            return 1
        ds_type = output_bam.split(".")[-2]
        ext2 = OrderedDict([
            ("subreadset", "subreads"),
            ("alignmentset", "subreads"),
            ("consensusreadset", "ccs"),
            ("consensusalignmentset", "ccs"),
            ("transcriptset", "transcripts"),
            ("transcriptalignmentset", "transcripts")
        ])
        if not ds_type in ext2:
            raise ValueError("Invalid output file extension '{t}.xml'; valid extensions are:\n{e}".format(
                t=ds_type, e="\n".join(["  %s.xml" % e for e in ext2.keys()])))
        output_ds = output_bam
        base_name = ".".join(output_ds.split(".")[:-2])
        output_bam = base_name + "." + ".".join([ext2[ds_type], "bam"])
    if output_bam == input_bam:
        log.error("Input and output files must not be the same path")
        return 1
    elif not output_bam.endswith(".bam"):
        log.error("Output file name must end in either '.bam' or '.xml'")
        return 1
    n_file_reads = 0
    have_zmws = set()
    scraps_bam = barcode_set = sts_xml = None
    with openDataFile(input_bam) as ds_in:
        if not isinstance(ds_in, ReadSet):
            raise TypeError("{t} is not an allowed dataset type".format(
                            t=type(ds_in).__name__))
        # TODO(nechols)(2016-03-11): refactor this to enable propagation of
        # filtered scraps
        if not ds_in.isIndexed:
            log.error("Input BAM must have accompanying .pbi index")
            return 1
        for ext_res in ds_in.externalResources:
            if ext_res.barcodes is not None:
                assert barcode_set is None or barcode_set == ext_res.barcodes
                barcode_set = barcode_set
            if ext_res.sts is not None:
                if sts_xml is None:
                    sts_xml = ext_res.sts
                else:
                    log.warn("Multiple sts.xml files, will not propagate")
        f1 = ds_in.resourceReaders()[0]
        if percentage is not None or count is not None:
            bam_readers = list(ds_in.resourceReaders())
            if sample_scraps:
                for ext_res in ds_in.externalResources:
                    if ext_res.scraps is not None:
                        scraps_in = IndexedBamReader(ext_res.scraps)
                        bam_readers.append(scraps_in)
            whitelist = _create_whitelist(bam_readers, percentage, count)
        # convert these to Python sets
        if use_subreads:
            _whitelist = _process_subread_list(whitelist)
            _blacklist = _process_subread_list(blacklist)
        else:
            _whitelist = _process_zmw_list(whitelist)
            _blacklist = _process_zmw_list(blacklist)
        scraps_in = None
        if output_ds is not None and output_ds.endswith(".subreadset.xml"):
            for ext_res in ds_in.externalResources:
                if ext_res.scraps is not None:
                    if use_barcodes:
                        log.warn("Scraps BAM is present but lacks " +
                                 "barcodes - will not be propagated " +
                                 "to output SubreadSet")
                    else:
                        scraps_in = IndexedBamReader(ext_res.scraps)
                    break
        with AlignmentFile(output_bam, 'wb',
                           template=f1.peer) as bam_out:
            for bam_in in ds_in.resourceReaders():
                n_records, have_zmws_ = _process_bam_whitelist(
                    bam_in, bam_out,
                    whitelist=_whitelist,
                    blacklist=_blacklist,
                    use_barcodes=use_barcodes,
                    anonymize=anonymize,
                    use_subreads=use_subreads,
                    qid2mov=ds_in.qid2mov)
                n_file_reads += n_records
                have_zmws.update(have_zmws_)
        if scraps_in is not None:
            scraps_bam = re.sub("subreads.bam$", "scraps.bam", output_bam)
            with AlignmentFile(scraps_bam, 'wb',
                               template=scraps_in.peer) as scraps_out:
                for ext_res in ds_in.externalResources:
                    if ext_res.scraps is not None:
                        scraps_in_ = IndexedBamReader(ext_res.scraps)
                        n_records, have_zmws_ = _process_bam_whitelist(
                            scraps_in_, scraps_out, _whitelist, _blacklist,
                            use_barcodes=use_barcodes,
                            anonymize=anonymize,
                            use_subreads=use_subreads)
                        have_zmws.update(have_zmws_)
    if n_file_reads == 0:
        log.error("No reads written")
        return 1
    log.info("{n} records from {z} ZMWs written".format(
        n=n_file_reads, z=len(have_zmws)))

    def _run_pbindex(bam_file):
        try:
            rc = subprocess.call(["pbindex", bam_file])
        except OSError as e:
            if e.errno == 2:
                log.warn("pbindex not present, will not create .pbi file")
            else:
                raise
    _run_pbindex(output_bam)
    if output_ds is not None:
        with openDataSet(input_bam) as ds_in:
            ds_out = ds_in.__class__(output_bam)
            if scraps_bam is not None:
                _run_pbindex(scraps_bam)
                ds_out.externalResources[0].scraps = scraps_bam
                # XXX it doesn't pick up the .pbi file - sort of annoying
                # but since the pbcore API doesn't provide a read for the
                # scraps automatically anyway, the impact is minimal
            if barcode_set is not None:
                ds_out.externalResources[0].barcodes = barcode_set
            if sts_xml is not None:
                sts_xml_out = base_name + ".sts.xml"
                log.info("Copying {s} to {d}".format(s=sts_xml, d=sts_xml_out))
                shutil.copyfile(sts_xml, sts_xml_out)
                ds_out.externalResources[0].sts = sts_xml_out
            if not ignore_metadata:
                ds_out.metadata = ds_in.metadata
                ds_out.updateCounts()
            if relative:
                ds_out.makePathsRelative(op.dirname(output_ds))
            if keep_original_uuid:
                log.warn("Keeping input UUID {u}".format(u=ds_in.uuid))
                ds_out.objMetadata["UniqueId"] = ds_in.uuid
            ds_out.write(output_ds)
            log.info("wrote {t} XML to {x}".format(
                     t=ds_out.__class__.__name__, x=output_ds))
    return 0


def _iter_bam_files(input_file):
    def __read_bam(fn):
        if op.exists(fn + ".pbi"):
            with IndexedBamReader(fn) as bam_in:
                return bam_in
        else:
            with BamReader(fn) as bam_in:
                return bam_in
    if input_file.endswith(".xml"):
        with openDataFile(input_file) as ds_in:
            if not ds_in.isIndexed:
                log.warning("Unindexed file(s), this may be very slow")
            for er in ds_in.externalResources:
                for bam in [er.bam, er.scraps]:
                    if bam is not None:
                        yield __read_bam(bam)
    else:
        yield __read_bam(input_file)


def show_zmws(input_file):
    zmws = []
    for rr in _iter_bam_files(input_file):
        if isinstance(rr, IndexedBamReader):
            zmws.extend(list([int(x) for x in rr.holeNumber]))
        else:
            zmws.extend([int(rec.HoleNumber) for rec in rr])
    print("\n".join([str(x) for x in sorted(list(set(zmws)))]))


def run(args):
    if args.show_zmws:
        if [args.whitelist, args.blacklist, args.percentage].count(None) != 3:
            log.warning("Ignoring unused filtering arguments")
        show_zmws(args.input_bam)
        return 0
    return filter_reads(
        input_bam=args.input_bam,
        output_bam=args.output_bam,
        whitelist=args.whitelist,
        blacklist=args.blacklist,
        percentage=args.percentage,
        count=args.count,
        seed=args.seed,
        ignore_metadata=args.ignore_metadata,
        relative=args.relative,
        anonymize=args.anonymize,
        use_barcodes=args.barcodes,
        sample_scraps=args.sample_scraps,
        keep_original_uuid=args.keep_uuid,
        use_subreads=args.subreads)


def get_parser():
    p = get_default_argparser_with_base_opts(
        version=VERSION,
        description=__doc__,
        default_level="WARN")
    p.add_argument("input_bam",
                   help="Input BAM or DataSet from which reads will be read")
    p.add_argument("output_bam", nargs='?', default=None,
                   help="Output BAM or DataSet to which filtered reads will "
                        "be written")
    p.add_argument("--show-zmws", action="store_true", default=False,
                   help="Print a list of ZMWs and exit")
    p.add_argument("--whitelist", action="store", default=None,
                   help="Comma-separated list of ZMWs, or file containing " +
                        "whitelist of one hole number per line, or " +
                        "BAM/DataSet file from which to extract ZMWs")
    p.add_argument("--blacklist", action="store", default=None,
                   help="Opposite of --whitelist, specifies ZMWs to discard")
    p.add_argument("--subreads", action="store_true",
                   help="If set, the whitelist or blacklist will be assumed to contain " +
                        "one subread name per line, or " +
                        "a BAM/DataSet file from which to extract subreads")
    p.add_argument("--percentage", action="store", type=float, default=None,
                   help="If you prefer to recover a percentage of a SMRTcell "
                        "rather than a specific list of reads specify that "
                        "percentage (range 0-100) here")
    p.add_argument("-n", "--count", action="store", type=int, default=None,
                   help="Recover a specific number of ZMWs picked at random")
    p.add_argument("-s", "--seed", action="store", type=int, default=None,
                   help="Random seed for selecting a percentage of reads")
    p.add_argument("--ignore-metadata", action="store_true",
                   help="Discard input DataSet metadata")
    p.add_argument("--relative", action="store_true",
                   help="Make external resource paths relative")
    p.add_argument("--anonymize", action="store_true",
                   help="Randomize sequences for privacy")
    p.add_argument("--barcodes", action="store_true",
                   help="Indicates that the whitelist or blacklist contains " +
                        "barcode indices instead of ZMW numbers")
    p.add_argument("--sample-scraps", action="store_true",
                   help="If enabled, --percentage and --count will include " +
                        "hole numbers from scraps BAM files when picking a " +
                        "random sample (default is to sample only ZMWs " +
                        "present in subreads BAM).")
    p.add_argument("--keep-uuid", action="store_true",
                   help="If enabled, the UUID from the input dataset will " +
                        "be used for the output as well.")
    return p


def main(argv=sys.argv):
    return pacbio_args_runner(
        argv=argv[1:],
        parser=get_parser(),
        args_runner_func=run,
        alog=log,
        setup_log_func=setup_log)


if __name__ == "__main__":
    sys.exit(main(sys.argv))
