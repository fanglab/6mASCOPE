from __future__ import absolute_import

import io
import os.path as op
import gzip
import bz2
import sys
import copy
from StringIO import StringIO

from pbcore.io import SubreadSet, ContigSet, FastaRecord, BamAlignment, Filters
from ..libs import AlignmentFile, AlignedSegment, AlignmentHeader
from ..independent.utils import autofmt, merge_sam_headers, is_subreadset, is_bam, is_sam, read_mode_of_file, is_json, is_fofn, bai_of_bam, has_bai_of_bam, generate_random_sample_str
from ..independent.common import consolidate_ref_regions
from ..utils import modify_alignedseg_sample_tag, get_moviename_from_alignedseg, get_readgroup_from_alignedseg
import json
import heapq
import hashlib
from collections import defaultdict



def cmp_alignedseg_by_coordinate(lhs, rhs):
    """Compare two AlignedSegment by reference coordinates, aka (reference_id, reference_start).
    The same as `samtools sort`.
    """
    return cmp((lhs.reference_id, lhs.reference_start), (rhs.reference_id, rhs.reference_start))

class IdObj(object):
    """A wrapper class which associates an object with an id.
    ...doctest:
        >>> a = IdObj('b', 1)
        >>> b = IdObj('a', 2)
        >>> a < b, a > b, a == b, a <= b, a >= b, a != b, a == a
        (True, False, False, True, False, True, True)
    """
    def __init__(self, id, obj):
        self.id = id # identifier of obj
        self.obj = obj

    def __repr__(self):
        return "(%r, %r)" % (self.id, self.obj)

    def __cmp__(self, other):
        if isinstance(self.obj, AlignedSegment) and isinstance(other.obj, AlignedSegment):
            # AlignedSegment.compare() does not work
            return cmp_alignedseg_by_coordinate(self.obj, other.obj)
        return cmp(self.obj, other.obj)

    def __lt__(self, other): # heapq only uses __Lt__ for obj comparison
        return self.__cmp__(other) < 0


class TransformerIterator(object):
    """Given an iterator, iterate over all objects and yield transform_function(object).
    ...doctest:
        >>> def f(i): return i + 1
        >>> [r for r in TransformerIterator(iter([1,2,3]), f)]
        [2, 3, 4]
    """
    def __init__(self, iterator, transformer):
        super(TransformerIterator, self).__init__()
        self.iterator = iterator
        self.transformer = transformer

    def __iter__(self):
        for obj in self.iterator:
            yield self.transformer(obj)

    def close(self):
        self.iterator.close()

    def __enter__(self):
        return self

    def __exit__(self, exception_type, exception_value, traceback):
        return self.close()


def to_alignedseg(in_obj):
    """Input in_obj can either be BamAlignment or AlignedSegment, return AlignedSegment as output.
    """
    if isinstance(in_obj, BamAlignment):
        return in_obj.peer
    else:
        if not isinstance(in_obj, AlignedSegment):
            raise ValueError("Object must either be BamAlignment or AlignedSegment while type of %r is %r" % (in_obj, type(in_obj)))
        return in_obj


class SequentialStreamMergerClass(object):
    """Sequentially iterate records from multiple iterators, yield IdObj(id, read).
    This class will iterate from the first record to the last record of the first stream,
    then move to remaining iterators.
    The term `iterator` can be used interchangeably with `stream`.
    ...doctest:
        >>> s = SequentialStreamMergerClass(['a', 'b'], [iter([1,2,3]), iter([4,5,6])], [{}, {}])
        >>> s.ids
        ['a', 'b']
        >>> [r for r in s]
        [1, 2, 3, 4, 5, 6]
    """
    def __init__(self, ids, iterators, headers):
        assert len(ids) == len(iterators)
        assert len(set(ids)) == len(ids) # ids must be unique
        self.ids = ids # ids of iterators
        self.iterators = iterators
        self.headers = headers
        self.id2iterator = dict(zip(ids, iterators))
        self.id2header = dict(zip(ids, headers))

    @property
    def header(self):
        return merge_sam_headers(self.headers)

    def __iter__(self):
        """Iterators over records in multiple iterators."""
        for id, iterator in zip(self.ids, self.iterators):
            for r in iterator:
                #yield IdObj(id, r)
                yield r

    def close(self):
        """Close all iterators"""
        for iterator in self.iterators:
            iterator.close()

    def __enter__(self):
        return self

    def __exit__(self, exception_type, exception_value, traceback):
        return self.close()


class SortedStreamMergerClass(SequentialStreamMergerClass):
    """Read reads from multiple files in sorted order, requiring all input files to be sorted.
    ...doctest:
        >>> mreader = SortedStreamMergerClass(['a', 'b'], [iter([1,2,3,10,200,300]), iter([-10, 4, 7, 201, 1000])], [{}, {}])
        >>> [r for r in mreader]
        [-10, 1, 2, 3, 4, 7, 10, 200, 201, 300, 1000]
    """
    def __init__(self, ids, iterators, headers):
        super(SortedStreamMergerClass, self).__init__(ids, iterators, headers)
        # minimum heapq of records from multiple iterators.
        self.minh = next_all_iterators(minh=[], iterator_ids=self.ids, iterator_objs=self.iterators)

    def __iter__(self):
        """Iterators over records in multiple files in sorted order."""
        while len(self.minh) != 0:
            p = heapq.heappop(self.minh) # pop the smallest element
            try:
                #new_aln = next(self.iterators[p.id].__iter__()) # get next from reader of p
                #heapq.heappush(self.minh, IdObj(id=p.id, obj=new_aln)) # push the next to minimum heapq
                new_aln = next(self.id2iterator[p.id].__iter__()) # get next from reader of p
                heapq.heappush(self.minh, IdObj(id=p.id, obj=new_aln)) # push the next to minimum heapq
            except StopIteration as e:
                pass # OK if a reader has no more read
            #yield p # e.g., yield IdObj(1, AlignedSegment)
            yield p.obj # e.g., yield AlignedSegment
        raise StopIteration


def next_all_iterators(minh, iterator_ids, iterator_objs):
    """
    For each iterator in iterator_objs, get the very next data in each iterator, and
    append them to minh.
    Return [IdObj(iterator_id, next_data_of_iterator), ..., IdObj(iterator_id, next_data_of_iterator)]

    minh --- a minimum heapq IdObj(iterator_id, data_of_iterator) objects
    iterator_ids --- iterator ids
    iterator_objs --- a list of iterators
    ...doctest:
        >>> h = next_all_iterators(minh=[], iterator_ids=['a', 'b', 'c', 'e'], iterator_objs=[[1,3,5], [2,4,6], [-10], []])
        >>> [heapq.heappop(h) for i in range(len(h))]
        [('c', -10), ('a', 1), ('b', 2)]
    """
    assert len(iterator_ids) == len(iterator_objs)
    for iterator_id, iterator_obj in zip(iterator_ids, iterator_objs):
        try:
            obj = next(iterator_obj.__iter__())
            idobj = IdObj(id=iterator_id, obj=obj)
            heapq.heappush(minh, idobj)
        except StopIteration: # OK if an iterator reaches its end
            pass
    return minh


def _open_sam(fn, fmt):
    """Open SAM | BAM file as AlignmentFile"""
    assert fmt == 'sam' or fmt == 'bam'
    mode = read_mode_of_file(fn)
    return AlignmentFile(fn, mode, check_sq=False)

def _open_subreadset(fn, fmt):
    assert fmt == 'subreadset.xml'
    return SubreadSet(fn)

#def _open_alignmentset(fn, fmt):
#    assert fmt == 'alignmentset.xml'
#    return AlignmentSet(fn)

def _open_stdin(fn, fmt=None):
    assert fn in ['-', 'stdin', '/dev/stdin']
    return AlignmentFile('-', 'r', check_sq=False)

def _open(fn, fmt):
    if fmt == 'sam' or fmt == 'bam':
        return _open_sam(fn, fmt)
    elif fmt == 'subreadset.xml': # fmt == 'alignmentset.xml'
        return _open_subreadset(fn, fmt) # return _open_alignmentset(fn, fmt)
    elif fn in ['-', 'stdin', '/dev/stdin']:
        return _open_stdin(fn)


def get_chr_to_index_dict_from_bam(aln_bam):
    """Return {reference_name: reference_id} from bam/sam header"""
    f = SingleFileOpener(aln_bam).alignfile
    d = dict(zip([name.split(' ')[0] for name in  f.references], range(0, f.nreferences))) # pylint: disable=no-member
    f.close()
    return d


def iterator_of_alignments_in_ref_regions(alignfile, ref_regions):
    """Iterate alignments within reference regions if reference regions
    are not None or empty. Otherwise, iterate over all alignments.
    """
    filename = alignfile.filename if hasattr(alignfile, 'filename') else str(alignfile)
    if not ref_regions: # no reference regions specified, iter all
        for aln in alignfile:
            yield aln
    else:
        #assert isinstance(alignfile, AlignmentFile)
        ref_lens_d = dict(zip(alignfile.references, alignfile.lengths))
        for ref_region in ref_regions:
            if ref_region.name not in ref_lens_d:
                raise ValueError("Could not find reference %s in alignment file: %s" % (ref_region.name, filename))
        for ref_region in ref_regions:
            ref_region = ref_region.slop(0, max_ref_len=ref_lens_d[ref_region.name])
            for aln in alignfile.fetch(ref_region.name, ref_region.start, ref_region.end):
                yield aln


class SingleFileOpener(object):
    valid_infmt = ('sam', 'bam', 'subreadset.xml') # alignmentset
    def __init__(self, fn):
        self.fn, self.fmt = autofmt(fn, self.valid_infmt, None)
        self.alignfile = _open(self.fn, self.fmt)


class SingleAlignFileIterator(SingleFileOpener, TransformerIterator):
    """
    Iterate over alignments within specified reference regions, yield as AlignmentSegment.
    fn -- input file, can be sam, bam, subreadset.xml
    ref_regions --- if ref_regions is not None, only yield alignments in sepcified reference regions
    """
    def __init__(self, fn, ref_regions):
        if ref_regions is not None and (not is_bam(fn) or not has_bai_of_bam(fn)):
            raise ValueError('Fetching alignments by reference regions requires input file to be sorted BAM with index: %s' % (fn))
        SingleFileOpener.__init__(self, fn)
        _iterator = iterator_of_alignments_in_ref_regions(alignfile=self.alignfile, ref_regions=ref_regions)
        TransformerIterator.__init__(self, _iterator, transformer=to_alignedseg)


    @property
    def header(self):
        return get_bam_header(self.fn)


def fofn_to_fns(fn):
    """Return a list of file names of files in input fofn filehandler.
    ...doctest:
        >>> fn = op.join(op.dirname(op.dirname(op.dirname(__file__))), 'tests', 'data', 'bams.fofn')
        >>> fofn_to_fns(fn)
        ['align_p.bam', 'align_m.bam', 'align_f.bam']
    """
    return [f.strip() for f in open(fn, 'r') if len(f.strip()) != 0 and not f.startswith('#')]


def fofn_to_alignment_iterators(fn, ref_regions):
    """Return iterators of bam files in fofn. If ref_regions is not None, the iterators should only
    yield alignments in reference regions.
    fns --- a list of bam files.
    ref_regions --- Only read on the specified region
    """
    assert is_fofn(fn)
    fns = fofn_to_fns(fn) # expand a fofn to multiple files
    return [SingleAlignFileIterator(fn=bam_fn, ref_regions=ref_regions) for bam_fn in fns]


def json_to_fns_and_samples(fn):
    """
    Input json must contain a list of tuples [(a0, b0), (a1, b1), ...], where (a, b) represents a filename `a`
    and its sample tag `b`.
    return ([a0, a1, ...], [b0, b1, ...])

    fn -- json file containing a list of tuples, e.g., [(a0, b0), ...,]
    ...doctest:
        >>> fn = op.join(op.dirname(op.dirname(op.dirname(__file__))), 'tests', 'out', 'fn2sample.json')
        >>> d = [['align_m.bam', 'm'], ['align_f.bam', 'f'], ['align_p.bam', 'p']]
        >>> json.dump(d, open(fn, 'w'))
        >>> json_to_fns_and_samples(fn)
        (['align_m.bam', 'align_f.bam', 'align_p.bam'], ['m', 'f', 'p'])
    """
    tuples = json.load(open(fn, 'r')) # load [(a0, b0), ..., ]
    try:
        return [str(t[0]) for t in tuples], [str(t[1]) for t in tuples]
    except IndexError as e:
        raise ValueError('Json file must only contain a list of (filename, sample) tuples: %s, %s' % (fn, str(e)))


def json_to_key_val_dict(fn, key_name, val_name):
    """Input json must contain a list of tuples [(a0, b0), (a1, b1), ...],
    return ([a0, a1, ...], [b0, b1, ...])
    fn --- json file containing a list of (key, value) tuples, e.g., [(key0, val0), ...]
    """
    tuples = json.load(open(fn, 'r')) # load [(a0, b0), ..., ]
    try:
        keys, vals = [str(t[0]) for t in tuples], [str(t[1]) for t in tuples]
    except IndexError as e:
        raise ValueError('Json file must only contain a list of (%s, %s) tuples: %s, %s' % (key_name, val_name, fn, str(e)))
    if len(set(keys)) != len(keys):
        raise ValueError('{k} objects in json file {f} are not unique: {keys}.'.format(k=key_name, f=fn, keys=keys))
    return dict(zip(keys, vals))


def json_to_moviename_sample_dict(fn):
    """where (a, b) represents a movie `a` and its sample tag `b`."""
    return json_to_key_val_dict(fn, 'movie', 'sample')

def json_to_readgroup_sample_dict(fn):
    return json_to_key_val_dict(fn, 'readgroup', 'sample')

def get_or_generate_movienames2samples(json_fn, movienames):
    """Get or generate a dict {moviename: random_sample}.
    If json file is not None, get dict from json file; otherwise, generates a dict
    which assigns a random sample str to all movies.
    """
    if not json_fn: # no json dict {moviename: sample}, assign a random sample str to all movies
        sample_str = generate_random_sample_str()
        samples = [sample_str for dummy in range(0, len(movienames))] # same sample for all
        return dict(zip(movienames, samples))
    else:
        return json_to_moviename_sample_dict(json_fn) # {moviename: samples}


def json_to_alignment_iterators(fn, ref_regions):
    assert is_json(fn)
    fns, samples = json_to_fns_and_samples(fn)
    return [SingleAlignFileToAnnotatedAlignIterator(fn=fn, ref_regions=ref_regions, sample=sample)
            for fn, sample in zip(fns, samples)]


class SingleAlignFileToAnnotatedAlignIterator(SingleAlignFileIterator):
    def __init__(self, fn, ref_regions, sample):
        def _transformer(obj):
            """Input obj can either be BamAlignment or AlignedSegment. If sample is not None or empty,
            modify sample tag of AlignedSegment, and return AlignedSegment."""
            if sample: # sample specified, override or create the sample tag
                return modify_alignedseg_sample_tag(record=to_alignedseg(obj), sample_str=sample)
            else: # no sample specified
                return to_alignedseg(obj)
        super(SingleAlignFileToAnnotatedAlignIterator, self).__init__(fn=fn, ref_regions=ref_regions)
        self.transformer = _transformer

def __header_dict(header):
    if isinstance(header, dict):
        return header
    elif isinstance(header, AlignmentHeader):
        return header.to_dict()

def _get_bam_header(fn):
    """Return SAM BAM header as a dict.
    If input is bam or sam, return header directly. If input is subreadset, return merged header
    of all resources.
    """
    valid_infmt = ('sam', 'bam', 'subreadset.xml')
    fn, fmt = autofmt(fn, valid_infmt, None)
    alignfile = _open(fn, fmt)
    if isinstance(alignfile, AlignmentFile):
        header = copy.deepcopy(dict(__header_dict(alignfile.header))) # pylint: disable=no-member
    elif isinstance(alignfile, SubreadSet): # or isinstance(self.alignfile, AlignmentSet):
        header = copy.deepcopy(dict(merge_sam_headers([__header_dict(rr.peer.header) for rr in alignfile.resourceReaders()])))
    else:
       raise ValueError('Could not get BAM header from {fn}'.format(fn=fn))
    alignfile.close()
    return header


def get_refname_to_indices_from_header(header_d):
    """
    header_d --- a header dictionary, e.g., output from get_bam_header
    ...doctest:
        >>> header = {'SQ': [{'SN': 'chr3', 'LN': 100}, {'SN': 'chr2'}]}
        >>> get_refname_to_indices_from_header(header)
        {'chr3': 0, 'chr2': 1}
    """
    return dict(zip([sq['SN'] for sq in header_d['SQ']], range(0, len(header_d['SQ']))))


def sanitize_ref_regions(ref_regions, slop_size, align_bam):
    """
    Returns a list of sanitized reference regions, which are
    * slopped (extended) to both ends by slop_size,
    * consolidated by merging mergable reference regions
    * sorted by reference presented in align_bam
    """
    if not ref_regions:
        return []
    # dict {reference name: index} in aligned.bam
    refname_to_indices_d = get_refname_to_indices_from_header(get_bam_header(align_bam))
    def name_to_id_f(name):
        return refname_to_indices_d[name]
    return consolidate_ref_regions(ref_regions=[r.slop(slop_size) for r in ref_regions],
                                   name_to_id_f=name_to_id_f)


def get_bam_header(fn):
    """Return SAM BAM header as a dict, also support input is fofn."""
    if isinstance(fn, AlignmentFile):
        return copy.deepcopy(__header_dict(fn.header)) # pylint: disable=no-member
    if is_fofn(fn):
        return merge_sam_headers([_get_bam_header(bam_fn) for bam_fn in fofn_to_fns(fn)])
    if is_json(fn):
        return merge_sam_headers([_get_bam_header(bam_fn) for bam_fn in json_to_fns_and_samples(fn)[0]])
    return _get_bam_header(fn)


def header_is_sorted_by_coordinate(header):
    """Return True if bam header indicates that this file is sorted by coordinate.
    """
    return 'HD' in header and 'SO' in header['HD'] and header['HD']['SO'].lower() == 'coordinate'


def _check_files_are_sorted_indexed_bam(fns, headers):
    """Sanity check input files, must be sorted BAM with index."""
    for fn, header in zip(fns, headers):
        if not is_bam(fn) or not has_bai_of_bam(fn) or not header_is_sorted_by_coordinate(header):
            errmsg = 'Fetching alignments by reference regions or in sorted order require input files to be sorted BAM with index: %s' % (fn)
            raise ValueError(errmsg)


def _check_headers_have_same_sq(headers):
    """Check that headers of all files must have the same SQ groups with the same order.
    This is required so that alignments can be compared by (reference_id, reference_start).
    """
    if len(headers) == 0:
        return
    if not all(['SQ' in header for header in headers]):
        raise ValueError("All BAM headers must have SQ groups")
    sq = headers[0]['SQ']
    for header in headers[1:]:
        if repr(sq) != repr(header['SQ']):
            raise ValueError("All BAM headers must have the same SQ groups")


def BamStream(fn, ref_regions, require_sorted):
    """
    BamStream accepts an input file of type: sam, bam, subreadset.xml, fofn, or json.

    If reference regions (ref_regions) is specified, all input files (including expanded files from fofn or json)
    must be sorted and indexed BAM. BamStream should only iterates over AlignedSegment objects within reference regions.

    If require_sorted is True, all input files (including expanded files from fofn or json) must be sorted and indexed
    BAM. BamStream should read alignments in sorted order. Otherwise, read alignments sequentially.
    * sequentially: read all alignments in the first file, then all alignments in the next file, ..., till all alignments in the last file.
    * in sorted order: read as if all alignments come from a merged, sorted by coordinate and indexed BAM file.

    Define BamStream based on input file type:
    * SAM: BamStream iterates reads or alignments in SAM; ref_regions must be None.
           If require_sorted, input SAM file must be sorted by coordinate.
    * BAM, iterate reads or alignments in BAM; if ref_regions is not None, or require_sorted, input BAM file must sorted by coordinate and indexed;
      If ref_regions is not None, will iterate only reads or alignments within reference regions.
    * SubreadSet, iterate reads in SubreadSet; ref_regions must be None; require_sorted must be False.
    * fofn, expand to a list files, which must be BAM or SAM; If ref_regions is not None or require_sorted, all expanded files must
      be sorted by coordinate and indexed BAM. If ref_regions is not None, iterator alignments within reference regions.
      If require_sorted is False, read alignments sequentially; otherwise, read alignments in sorted order.
    * json, contains a list of (BAM, sample) tuples. Iterate reads or alignments and modify the sample 'sm'
      tag of all reads and alignments.
    """
    # Choose StreamMergerClass
    merger_cls = SortedStreamMergerClass if require_sorted else SequentialStreamMergerClass

    # Define ids=filenames, iterators, bam headers.
    ids, iterators, headers = None, None, None
    if is_bam(fn) or is_sam(fn) or is_subreadset(fn): # or is_alignmentset(fn)
        iterator = SingleAlignFileIterator(fn=fn, ref_regions=ref_regions)
        ids, iterators, headers= [fn], [iterator], [(iterator.header)]
    elif is_fofn(fn):
        ids = fofn_to_fns(fn)
        iterators = fofn_to_alignment_iterators(fn=fn, ref_regions=ref_regions)
        headers = [get_bam_header(bam_fn) for bam_fn in fofn_to_fns(fn)]
    else:
        raise ValueError("BamStream dose not support read alignments from file: %s" % (fn))

    if ref_regions or require_sorted:
        _check_files_are_sorted_indexed_bam(fns=ids, headers=headers) # sanity check all input files are sorted BAM with index
    if require_sorted:
        _check_headers_have_same_sq(headers) # merge sort requires all BAM files share the same SQ groups

    return merger_cls(ids=ids, iterators=iterators, headers=headers) # return reader


class AnnotateAlignedSegSampleByFunctionIterator(TransformerIterator):
    """Annotate sample ('sm') tag of AlignedSegment objects, where sample = obj2sample_func(obj).
    """
    def __init__(self, iterator, obj2sample_func): # obj2key_func, keys2samples):
        def _transformer(obj):
            """Input an AlignedSeg object, add sample to this object and return."""
            return modify_alignedseg_sample_tag(record=obj, sample_str=obj2sample_func(obj))
        super(AnnotateAlignedSegSampleByFunctionIterator, self).__init__(iterator=iterator, transformer=_transformer)
        self.header = iterator.header

def alignedseg_to_moviename_to_sample(alignedseg, movies2samples):
    """Get movie name of a given AlignedSegment, and return movies2samples[moviename]"""
    moviename = get_moviename_from_alignedseg(alignedseg)
    try:
        return movies2samples[moviename]
    except Exception as e:
        raise ValueError("Movie name {mn} is not in movies2samples dictionary {d}.".format(mn=moviename, d=movies2samples))


def alignedseg_to_alignedseg_with_sample(alignedseg, readgroups2samples):
    """Get read group of a given AlignedSegment and return readgroups2samples[readgroup]."""
    readgroup = get_readgroup_from_alignedseg(alignedseg)
    try:
        return  readgroups2samples[readgroup]
    except Exception as e:
        raise ValueError("Read group {rg} is not in readgroups2samples dictionary {d}.".format(rg=readgroup, d=readgroups2samples))


def AnnotateSampleByMovieName_BamStream(fn, ref_regions, require_sorted, movies2samples):
    """Accepts an input file of type: sam, bam, subreadset.xml, fofn.
    movie2samples --- a dict {movie: sample}
    Like BamStream, except that this function annotates sample ('sm') tag of all alignments based
    on {movie: sample}.
    """
    iterator = BamStream(fn=fn, ref_regions=ref_regions, require_sorted=require_sorted)
    def f(obj):
        return alignedseg_to_alignedseg_with_sample(obj, movies2samples)
    return AnnotateAlignedSegSampleByFunctionIterator(iterator=iterator, obj2sample_func=f)


def AnnotateSampleByReadGroup_BamStream(fn, ref_regions, require_sorted, readgroups2samples):
    """Accepts an input file of type: sam, bam, subreadset.xml, fofn.
    movie2samples --- a dict {movie: sample}
    Like BamStream, except that this function annotates sample ('sm') tag of all alignments based
    on {readgroup: sample}.
    """
    iterator = BamStream(fn=fn, ref_regions=ref_regions, require_sorted=require_sorted)
    def f(obj):
        return alignedseg_to_alignedseg_with_sample(obj, readgroups2samples)
    return AnnotateAlignedSegSampleByFunctionIterator(iterator=iterator, obj2sample_func=f)


def AnnotateSampleByFile_BamStream(fn, ref_regions, require_sorted):
    """Only accepts an input file of type: json. Json input file must contain a list of
    (filename, sample) tuples. For each filename, annotate the sample 'sm' tag of all alignments
    in this file with `sample`.
    """
    # Choose StreamMergerClass
    merger_cls = SortedStreamMergerClass if require_sorted else SequentialStreamMergerClass

    if is_json(fn):
        iterators = json_to_alignment_iterators(fn=fn, ref_regions=ref_regions)
        fns = json_to_fns_and_samples(fn)[0]
        headers = [get_bam_header(bam_fn) for bam_fn in fns]

        if ref_regions or require_sorted: # sanity check all input files are sorted BAM with index
            _check_files_are_sorted_indexed_bam(fns=fns, headers=headers)
        if require_sorted: # merge sort requires all BAM files share the same SQ groups
            _check_headers_have_same_sq(headers)

        return merger_cls(ids=fns, iterators=iterators, headers=headers) # return reader
    else:
        raise ValueError("AnnotateSampleByFile_BamStream dose not support read alignments from file: %s" % (fn))


def get_readgroups2samples_from_header(header):
    """Given an input BAM header dict, return a dict {readgroup: sample}.
    ...doctest:
        >>> header = {'RG':[{'ID': 'myid', 'SM': 'mysample'}, {'ID': 'myid2', 'SM': 'mysample2'}]}
        >>> d  = get_readgroups2samples_from_header(header)
        >>> d['myid']
        'mysample'
        >>> d['myid2']
        'mysample2'
    """
    readgroups2samples = {}
    if 'RG' not in header or not all('ID' in rg and 'SM' in rg for rg in header['RG']):
        raise ValueError("BAM header must have RG read group and each read group must contain SM tag.")
    for rg in header['RG']:
        if rg['ID'] in readgroups2samples:
            raise ValueError("BAM header read group %s must be unique." % (rg['ID']))
        readgroups2samples[rg['ID']] = rg['SM']
    return readgroups2samples


def get_samples_from_header(header):
    """Given an input BAM header dict, return a list of samples.
    ...doctest:
        >>> header = {'RG':[{'ID': 'myid', 'SM': 'mysample'}, {'ID': 'myid2', 'SM': 'mysample2'}, {'ID': 'myid3', 'SM': 'mysample2'}]}
        >>> get_samples_from_header(header)
        ['mysample', 'mysample2']
    """
    return sorted(set(get_readgroups2samples_from_header(header).values()))


def get_movienames2samples_from_header(header):
    """Given an input BAM header dict, return a dict {moviename: sample}.
    ...doctest:
        >>> header = {'RG':[{'ID': 'myid', 'PU': 'mymovie', 'SM': 'mysample'}, {'ID': 'myid2', 'PU':'mymovie2',  'SM': 'mysample2'}]}
        >>> d  = get_movienames2samples_from_header(header)
        >>> d['mymovie']
        'mysample'
        >>> d['mymovie2']
        'mysample2'
    """
    movienames2samples = {}
    if 'RG' not in header or not all('PU' in rg and 'SM' in rg for rg in header['RG']):
        raise ValueError("BAM header must have RG read group and each read group must contain PU and SM tag.")
    for rg in header['RG']:
        if rg['PU'] in movienames2samples and movienames2samples[rg['PU']] != rg['SM']:
            raise ValueError("BAM header movie %s have two different SM tag %s and %s" % (rg['PU'], movienames2samples[rg['PU']], rg['SM']))
        movienames2samples[rg['PU']] = rg['SM']
    return movienames2samples


def get_movienames2readgroups_from_header(header):
    """Given an input BAM header dict, return a dict {moviename: readgroup}.
    Note that although as of SA 5.0, each movie should be associated with an unique read group ID,
    however, it may not be true in the future for barcoded samples, when each read group should be
    determined by (smrtcell, biological sample, read type).

    This function is designed to link reads with biological samples for joint-calling, assuming that
    build is SA 5.0 and each movie is associated with an unique read group.

    ...doctest:
        >>> header = {'RG':[{'ID': 'myid', 'PU': 'mymovie', 'SM': 'mysample'}, {'ID': 'myid2', 'PU':'mymovie2',  'SM': 'mysample2'}]}
        >>> d  = get_movienames2readgroups_from_header(header)
        >>> d['mymovie']
        'myid'
        >>> d['mymovie2']
        'myid2'
    """
    movienames2readgroups = {}
    if 'RG' not in header or not all('PU' in rg and 'ID' in rg for rg in header['RG']):
        raise ValueError("BAM header must have RG read group and each read group must contain PU and ID tag.")
    for rg in header['RG']:
        if rg['PU'] in movienames2readgroups and movienames2readgroups[rg['PU']] != rg['ID']:
            raise ValueError("BAM header movie %s have two different ID tag %s and %s" % (rg['PU'], movienames2readgroups[rg['PU']], rg['ID']))
        movienames2readgroups[rg['PU']] = rg['ID']
    return movienames2readgroups


def AnnotateSampleBamStream(fn, ref_regions, require_sorted):
    if is_json(fn):
        return AnnotateSampleByFile_BamStream(fn=fn, ref_regions=ref_regions, require_sorted=require_sorted)
    elif is_bam(fn) or is_sam(fn) or is_subreadset(fn) or is_fofn(fn):
        readgroups2samples = get_readgroups2samples_from_header(header=get_bam_header(fn))
        return AnnotateSampleByReadGroup_BamStream(fn=fn, ref_regions=ref_regions, require_sorted=require_sorted, readgroups2samples=readgroups2samples)


def is_good_mapped_alignment(aln, allow_secondary, min_mapq, allow_qcfail, allow_duplicate):
    """Return True if input alignment aln meets all below criteria:
    * is mapped,
    * its mapping QV is greater than or equal to min_mapq
    * is parimary alignment or secondary alignment is allowed, and
    * has good qc (qc not fail) or allow qcfail
    * is not duplicated or allow duplicated
    """
    return not aln.is_unmapped and \
            (aln.mapping_quality >= min_mapq) and \
            (not aln.is_secondary or allow_secondary) and \
            (not aln.is_qcfail or allow_qcfail) and \
            (not aln.is_duplicate or allow_duplicate)


def yield_good_obj(yield_obj_f, is_good_obj_f):
    """
    Silently drop bad objects yielded from input yield_obj_f and only yield good objects.
    is_good_obj_f -- a function, is_good_obj_f(obj) returns True if obj should be kept,
                        False, if this obj needs to be dropped.
    ..doctest:
        >>> def f(a): return a >= 5
        >>> next(yield_good_obj(range(0, 7), f))
        5
        >>> [obj for obj in yield_good_obj(range(0, 8), f)]
        [5, 6, 7]
    """
    for obj in yield_obj_f:
        if is_good_obj_f(obj):
            yield obj


def yield_grouped_objs(yield_obj_f, obj_to_group_f):
    """Group input objects by their keys and yield a list of objects in each group sequentially.
    ...doctest:
        >>> def f(i): return i
        >>> [r for r in yield_grouped_objs(yield_obj_f=[4,4,4,2,2,2,3,1,1], obj_to_group_f=f)]
        [[4, 4, 4], [2, 2, 2], [3], [1, 1]]
    """
    objs = []
    prev_group = None
    for obj in yield_obj_f:
        group = obj_to_group_f(obj)
        if prev_group is None:
            prev_group = group
            objs.append(obj)
        else:
            if prev_group != group:
                yield objs
                objs = []
                prev_group = group
            objs.append(obj)
    yield objs


def aln_to_md5_hexdigest(aln_obj, salt):
    """Hash (an alignedseg obj, salt) to a md5 hexdigest string.
    salt --- additional input string
    """
    input_elements = [salt, aln_obj.query_name, aln_obj.reference_name,
                      aln_obj.query_alignment_start, aln_obj.reference_start,
                      aln_obj.flag, aln_obj.cigarstring]
    input_str = '$'.join([str(e) for e in input_elements])
    return hashlib.md5(input_str).hexdigest()


from ..independent.utils import heapq_sampling_one_move
def yield_sampled_aln(yield_obj_f, is_good_obj_f, obj_to_group_f, obj_to_key_f, K, allowed_keys,
        downsample_randomly=True, downsample_randomseed=0):
    """Yield at most K sampled objects per group per key, while keep order of objects unchanged.

    is_good_obj_f --- is_good_obj_f(obj) return True if this obj is good.
    obj_to_group_f --- obj_to_group_f(aln) return group id of this obj
    K --- yield at most K objects per group per key
    allowed_keys --- objects' keys must be in allowed_keys
    downsample_randomly --- if True, use random sampling of K items per group per key. False, use top K items.
    """
    def alnhex_score_f(aln, i): # aln objs scored by md5 hex digest strings
        return aln_to_md5_hexdigest(aln, downsample_randomseed)
    def fifo_score_f(aln, i): # aln objs first in first out
        return -i
    score_f = alnhex_score_f if downsample_randomly else fifo_score_f
    return _yield_sampled_obj(yield_obj_f, is_good_obj_f, obj_to_group_f, obj_to_key_f, K, allowed_keys, score_f)


def _yield_sampled_obj(yield_obj_f, is_good_obj_f, obj_to_group_f, obj_to_key_f, K, allowed_keys, score_f):
    """Yield at most K sampled objects per group per key, while keep order of objects unchanged.

    is_good_obj_f --- is_good_obj_f(obj) return True if this obj is good.
    obj_to_group_f --- obj_to_group_f(aln) return group id of this obj
    K --- yield at most K objects per group per key
    allowed_keys --- objects' keys must be in allowed_keys
    score_f --- score_f(obj, i) returns score of obj whose index is i

    ..doctest:
        >>> def f(t): return t[2] >= 0
        >>> def g(t): return t[0] # group
        >>> def k(t): return t[1] # key
        >>> def s(obj, i): return -i # score of obj
        >>> items = [(1, 2, -1), (1, 3, 0), (1, 1, 4), (2, 2, 3), (2, 3, 4), (10, 2, 3), (10, 2, 4)]
        >>> [r for r in _yield_sampled_obj(items, is_good_obj_f=f, obj_to_group_f=g, obj_to_key_f=k, K=1, allowed_keys=[3, 2, 1], score_f=s)]
        [(1, 3, 0), (1, 1, 4), (2, 2, 3), (2, 3, 4), (10, 2, 3)]
        >>> # group 0: [BAD(1, 2, -1), (1, 3, 0), (1, 1, 4)] --> yield (1, 3, 0), (1, 1, 4)
        >>> # group 1: [(2, 2, 3), (2, 3, 4)] --> yield (2, 3, 4), (2, 2, 3)
        >>> # group 2: [(10, 2, 3), (10, 2, 4)] --> yield (10, 2, 3)
    """
    y0 = yield_good_obj(yield_obj_f=yield_obj_f, is_good_obj_f=is_good_obj_f)
    for objs_in_group in yield_grouped_objs(yield_obj_f=y0, obj_to_group_f=obj_to_group_f):
        for obj in yield_obj_cap_by_count_per_key(yield_obj_f=objs_in_group,
            obj_to_key_f=obj_to_key_f, K=K, allowed_keys=allowed_keys,
            score_f=score_f):
            yield obj


def yield_obj_cap_by_count_per_key(yield_obj_f, obj_to_key_f, K, allowed_keys, score_f):
    """
    This function will downsample and yield objects from input yield function `yield_obj_f`.

    * Input yield_obj_f yields arbitrary but limited number of objects in order
    * Each input object must have a key and the key must be exist in input variable `keys`
    * Must yield up to K output objects per key.
    * Must yield output objects in order, aka keep relative order of objects between input and output the same

    yield_obj_f -- a input yielder function which yields arbitrary but limited number of objects in order
    obj_to_key_f -- a function returns key of an object
    K -- max count of objects to output yield per key
    allowed_keys -- objects' keys must in `allowed_keys`, raises a ValueError otherwise
    score_f --- score_f(obj, i) returns score of obj whose index is i

    test random sampling of K items
    ..doctest:
        >>> def f(i): return i
        >>> def s(obj, i): return -i # score of obj is -index, fifo
        >>> y = yield_obj_cap_by_count_per_key([1, 2, 2, 1, 2, 1, 3, 4, 4], obj_to_key_f=f, K=2, allowed_keys=[4, 3, 2, 1], score_f=s)
        >>> [r for r in y]
        [1, 2, 2, 1, 3, 4, 4]

    test sampling of top K items
    ..doctest:
        >>> def f(i): return i[0] # the first element as key
        >>> def s(obj, i): return obj[1] # obj[1] as score
        >>> y = yield_obj_cap_by_count_per_key([(1, 'a'), (2, 'a'), (2, 'b'), (1, 'b')], obj_to_key_f=f, K=1, allowed_keys=[4, 3, 2, 1], score_f=s)
        >>> [r for r in y]
        [(2, 'b'), (1, 'b')]

        >>> def s(obj, i): return -i # score of obj is -index, fifo
        >>> y = yield_obj_cap_by_count_per_key([(1, 'a'), (2, 'a'), (2, 'b'), (1, 'b')], obj_to_key_f=f, K=1, allowed_keys=[4, 3, 2, 1], score_f=s)
        >>> [r for r in y]
        [(1, 'a'), (2, 'a')]
    """
    # key_to_R_dict -- key_to_R_dict[key] is a heapq list of output (score_of_obj, obj) tuples to yield, where objects' key=key.
    key_to_R_dict = defaultdict(list)
    # key_to_S_count_dict -- key_to_S_count_dict[key] is total number of previously yielded input objects whose key=key,
    # in another word, it is the 0 based index of this obj in the list of all input obj which share the same key
    # as this obj
    key_to_S_count_dict = defaultdict(lambda: 0)

    for global_id, obj in enumerate(yield_obj_f): # read all objects from input S
        key = obj_to_key_f(obj)
        if not key in allowed_keys: # no tolerance to a bad key
            raise ValueError('Object {obj} its key {key} must be in keys {keys}'.format(obj=obj, key=key, keys=allowed_keys))
        # key_to_R_dict[key] is a list of obj sharing the same key in output bucket

        # heapq_sampling_one_move may either append s_i=(score_of_obj, obj) to the output heapq,
        # or replace an existing item in output heapq with s_i, or drop s_i without changing output,
        # while the maximum size of output heapq must be capped by K.
        i = key_to_S_count_dict[key]
        heapq_sampling_one_move(i=i, s_i=(score_f(obj, i), IdObj(id=global_id, obj=obj)),
                                K=K, R=key_to_R_dict[key])
        key_to_S_count_dict[key] += 1 # index of this obj in all obj which share the same key as this obj

    # yield output objects while preserve their relative order in input
    idobjs = [] # all output [(global_id, alignedseg), ...]
    for key in allowed_keys:
        idobjs.extend([r[1] for r in key_to_R_dict[key]]) # drop scores, merge all idobjs regardless of keys
    idobjs = sorted(idobjs, key=lambda idobj:idobj.id)  # sort by obj's global_id
    for idobj in idobjs:
        yield idobj.obj # yield objs
