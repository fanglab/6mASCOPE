#! python

"""
Scatter inputs of pbsvtools.tasks.align_json_to_svsig

align takes two inputs:
    Input: idx 0 - datastore.json containing a list of AlignmentSet files,
                   or a list of ConsensusAlignmentSet files.
           idx 1 - TRF bed file
    Output: idx 0 - FOFN of svsig.gz files
"""
from __future__ import absolute_import

import logging
import sys
import os.path as op
from collections import defaultdict

from pbcommand.pb_io import write_pipeline_chunks
from pbcommand.models import FileTypes, PipelineChunk, DataStore, DataStoreFile

from pbcoretools.datastore_utils import dataset_to_datastore, datastore_to_datastorefile_objs
from .merge_alignments_by_sample import Constants as BC
from .merge_alignments_by_sample import get_only_sample_from_bam
from ..basic import scatter_main, TCP_INPUT, TCP_OUTPUT, BaseScatterConstants


log = logging.getLogger(__name__)


class Constants(BaseScatterConstants):
    """Constants must be identical to align_json_to_svsig
    Input: idx 0 - DataStore json of AlignmentSet (or ConsensusAlignmentSet)
    Output: idx 0 - FOFN of svsig.gz
    """
    TOOL_NAME = 'scatter_{}'.format(BC.TOOL_NAME)
    DESCRIPTION = "Scatter inputs for pbsvtools.tasks.{}".format(TOOL_NAME)
    CHUNK_KEYS = ('$chunk.datastore_id', )
    INPUT_FILES = BC.INPUT_FILES
    OUTPUT_FILES = [
        TCP_OUTPUT(FileTypes.CHUNK, "cjson_out", "Chunk merge_alignments_by_sample datastore JSON",
                   "Chunk merge_alignments_by_sample datastore JSON",
                   "merge_alignments_by_sample.datastore.chunked")
    ]
    ALLOWED_TYPES = BC.ALLOWED_TYPES


def run_main(i_datastore_fn, o_json_fn, max_nchunks):
    """
    Parameters:
      i_datastore_fn --- DataStore json containing BAM files, chunk by sample
      o_json_fn -- Output json file
    """
    datastorefile_objs, _, readcls, _ = datastore_to_datastorefile_objs(
        i_datastore_fn, allowed_types=Constants.ALLOWED_TYPES)

    # Assumption: each DataStoreFile object must be associated with exactly one sample,
    # while one sample may have multiple DataStoreFile objs associated with it
    sample_to_datastorefile_objs = defaultdict(list)  # sample --> [datastorefile_objs]
    for obj in datastorefile_objs:
        dset_file = obj.path
        dset = readcls(dset_file)
        bam_files = dset.toExternalFiles()
        if len(bam_files) != 1:
            raise ValueError("Expected alignment set {} to contain exactly one external BAM file!"
                             .format(dset_file))
        bam_file = bam_files[0]
        # Check input bam files, each must associated with exactly one sample
        sample = get_only_sample_from_bam(bam_file)
        sample_to_datastorefile_objs[sample].append(obj)  # Associate datastore obj with sample

    # Split by sample, capped by max_nchunks.
    # Each item in boxes_of_objs is a list of DataStoreFile objects.
    # Must put DataStoreFile objects of a sample in one box.
    # If total number samples is greater than total number of chunks, allow multiple samples in one box.
    n_chunks = max(1, min(max_nchunks, len(sample_to_datastorefile_objs.keys())))
    boxes_of_objs = [[] for _ in range(n_chunks)]
    for idx, sample in enumerate(sample_to_datastorefile_objs.keys()):
        objs = sample_to_datastorefile_objs[sample]
        boxes_of_objs[idx % n_chunks].extend(objs)

    # Chunk input datastore json, generate multiple chunked datastore.json, and
    # generate pbcommand.models.PipelineChunk objects
    output_dir = op.dirname(o_json_fn)
    basename = 'chunk'
    chunks = []
    for i, objs in enumerate(boxes_of_objs):
        out_json = op.join(output_dir, '{}.{}.{}'.format(basename, i, 'datastore.json'))
        DataStore(objs).write_json(out_json)
        # Create a chunk: get $chunk.datastore_id from chunk,
        d = {Constants.CHUNK_KEYS[0]: out_json}
        chunk_id = Constants.TOOL_NAME+'_chunk_{}'.format(i)  # chunks MUST have unique IDs
        chunk = PipelineChunk(chunk_id, **d)
        chunks.append(chunk)

    log.info("Writing chunk.json to %s", o_json_fn)
    write_pipeline_chunks(chunks, o_json_fn, "created by %s" % Constants.TOOL_ID())
    return 0


def rtc_runner(rtc):
    """Resolved tool contract runner."""
    max_nchunks = rtc.task.max_nchunks if hasattr(
        rtc.task, 'max_nchunks') else Constants.DEFAULT_NCHUNKS
    return run_main(i_datastore_fn=rtc.task.input_files[0],
                    o_json_fn=rtc.task.output_files[0],
                    max_nchunks=int(max_nchunks))


if __name__ == '__main__':
    sys.exit(scatter_main(
        args=sys.argv[1:], const=Constants, rtc_runner=rtc_runner, alog=log))
