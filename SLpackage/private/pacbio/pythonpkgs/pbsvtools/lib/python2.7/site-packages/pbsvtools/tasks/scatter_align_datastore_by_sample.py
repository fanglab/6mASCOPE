#! python

"""
Split alignment bam in input alignment datastore file by Biosample.
"""
from __future__ import absolute_import

import logging
import sys
import os.path as op
from collections import defaultdict
import uuid

from pbcommand.cli import (pacbio_args_runner,
                           get_default_argparser_with_base_opts)
from pbcommand.utils import setup_log
from pbcommand.models import FileTypes, DataStore, DataStoreFile

from pbcoretools.datastore_utils import datastore_to_datastorefile_objs
from .merge_alignments_by_sample import get_only_sample_from_bam


log = logging.getLogger(__name__)
__version__ = "0.1.0"


def scatter_align_json_by_sample(i_datastore_fn, output_dir, max_nchunks):
    """
    Parameters:
      i_datastore_fn --- DataStore json of AlignmentSet or ConsensusAlignmentSet to separate by sample.
      output_dir --- Output directory.
      max_nchunks --- Split input datastore into at most max_nchunks chunks.
    """
    ALLOWED_TYPES = (FileTypes.DS_ALIGN, FileTypes.DS_ALIGN_CCS)

    datastorefile_objs, file_type_id, readcls, _ = datastore_to_datastorefile_objs(
        i_datastore_fn, allowed_types=ALLOWED_TYPES)

    # Assumption: each BAM file must be assigned exactly one sample.
    # while one sample may have multiple BAM file associated with it
    sample_to_bam_files = defaultdict(list)  # sample --> [bam_files]
    for obj in datastorefile_objs:
        dset_file = obj.path
        dset = readcls(dset_file)
        bam_files = dset.toExternalFiles()
        for bam_file in bam_files:
            sample = get_only_sample_from_bam(bam_file)
            sample_to_bam_files[sample].append(bam_file)  # Associate bam file with sample

    # Split by sample, capped by max_nchunks.
    # Each item in boxes_of_objs is a bam file.
    # Must put bam files of a sample in one box.
    # If total number samples is greater than total number of chunks, allow multiple samples in one box.
    samples = sample_to_bam_files.keys()
    n_chunks = max(1, min(max_nchunks, len(samples)))
    boxes_of_objs = [[] for _ in range(n_chunks)]
    for idx, sample in enumerate(samples):
        bam_files = sample_to_bam_files[sample]
        boxes_of_objs[idx % n_chunks].extend(bam_files)

    # Chunk input datastore json, generate multiple chunked datastore.json, and
    # generate pbcommand.models.PipelineChunk objects
    basename = 'chunk_by_sample'
    source_id = "scatter_align_datastore_by_sample"
    for i, bam_files in enumerate(boxes_of_objs):
        # WARNING: if output json filename is changed, must update
        # chunk name to collect in sv.wdl.
        out_json = op.join(output_dir, '{}.{}.{}'.format(basename, i, 'datastore.json'))
        ds_files = [DataStoreFile(uuid.uuid4(), source_id, file_type_id, bam_file) for bam_file in bam_files]
        DataStore(ds_files).write_json(out_json)
    return 0


def run_args(args):
    return scatter_align_json_by_sample(i_datastore_fn=args.i_datastore_fn,
                                        output_dir=args.output_dir,
                                        max_nchunks=args.max_nchunks)


def _get_parser():
    p = get_default_argparser_with_base_opts(
        version=__version__,
        description=__doc__,
        default_level="INFO")
    p.add_argument("i_datastore_fn", type=str,
                   help="datastore.json containing a list of AlignmentSet files or a list of ConsensusAlignmentSet files.")
    p.add_argument("output_dir", type=str, help="Output directory")
    p.add_argument("max_nchunks", type=int, help="Split input into at most max_nchunks chunks.")
    return p


def main(argv=sys.argv):
    return pacbio_args_runner(
        argv=argv[1:],
        parser=_get_parser(),
        args_runner_func=run_args,
        alog=log,
        setup_log_func=setup_log)


if __name__ == "__main__":
    sys.exit(main(sys.argv))
