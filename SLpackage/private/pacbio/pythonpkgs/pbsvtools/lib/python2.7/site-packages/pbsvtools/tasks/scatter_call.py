#! python

"""
Scatter inputs of pbsvtools.call.

    Input: idx 0 - pbsv.cfg
           idx 1 - sorted_chained.bam
           idx 2 - referenceset.xml
"""
from __future__ import absolute_import

import logging
import sys
import os.path as op

from pbcommand.pb_io.common import load_pipeline_chunks_from_json
from pbcommand.pb_io import write_pipeline_chunks
from pbcommand.models import  FileTypes, PipelineChunk

from pbsv1.libs import AlignmentFile
import pbcoretools.chunking.chunk_utils as CU
from pbcoretools.chunking.gather import get_datum_from_chunks_by_chunk_key

from pbsvtools.basic import scatter_main, TCP_INPUT, TCP_OUTPUT, BaseScatterConstants

log = logging.getLogger(__name__)


class Constants(BaseScatterConstants):
    """Constants used in pbsvtools.tasks.scatter_call"""
    TOOL_NAME = 'scatter_call'
    DESCRIPTION = "Scatter aligned reads by reference for pbsvtools.tasks.call, virutally"
    CHUNK_KEYS = ('$chunk.config_id', '$chunk.bam_id', '$chunk.ref_id', '$chunk.txt_id')

    INPUT_FILES = [
        TCP_INPUT(FileTypes.CFG, 'pbsv_config_in', 'pbsv Config In', 'pbsv Config'),
        TCP_INPUT(FileTypes.BAM, 'sorted_chained_bam', 'BAM ALIGNMENTS In', 'Sorted chained alignments'),
        TCP_INPUT(FileTypes.DS_REF, 'referenceset_in', 'Reference Set In', 'PacBio ReferenceSet'),
        TCP_INPUT(FileTypes.TXT, 'reference_regions', 'Reference Regions In', 'Reference Regions TXT')
    ]

    OUTPUT_FILES = [
        TCP_OUTPUT(FileTypes.CHUNK, "cjson_out", "Chunk JSON sv call tasks", "Chunk JSON sv call tasks", "call.chunked")
    ]

def put_items_to_boxes(items, weights, n, cutoff):
    """Put seqs into n boxes, where stop to put more items when sum of weight in a box is greater than or equal to a cutoff
    items --- a list of strings
    weights --- item weights
    n --- number of boxes
    cutoff --- sum of weight cutoff to stop putting items to a box
    ..doctest::
        >>> put_items_to_boxes(['1', '2'], [1, 3], 1, 100)
        [['1', '2']]
        >>> put_items_to_boxes(['1', '2'], [1, 3], 2, 100)
        [['1', '2']]
        >>> put_items_to_boxes(['2', '2'], [1, 3], 2, 2)
        [['2'], ['2']]
        >>> put_items_to_boxes(['1', '2'], [1, 3], 100, 1)
        [['1'], ['2']]
    """
    weight_d = dict(zip(items, weights))
    assert sum(weights) <= n * cutoff
    boxes = [[] for dummpy in range(n)]
    idx = 0
    for item in items:
        if sum([weight_d[i] for i in boxes[idx]]) >= cutoff:
            idx += 1
        boxes[idx].append(item)
    return [box for box in boxes if len(box) != 0]


def chunk_aln_references(i_aln_fn, max_nchunks):
    """Chunk references in i_aln_fn into no more than max_nchunks chunks based on length."""
    # Chunk i_aln_fn BAM by reference based on length
    alnfile = AlignmentFile(i_aln_fn, 'r', check_sq=False)
    n_chunks = max(1, min(max_nchunks, alnfile.nreferences))
    len_cutoff = int(sum(alnfile.lengths) / n_chunks) + 1
    ref_chunks = put_items_to_boxes(items=alnfile.references, weights=alnfile.lengths, n=n_chunks, cutoff=len_cutoff)
    alnfile.close()
    return ref_chunks

def run_main(i_cfg_fn, i_aln_fn, i_ref_fn, i_ref_regions_fn, o_json_fn, max_nchunks):
    """
    Parameters:
      i_cfg_fn -- pbsv config file
      i_aln_fn --- aligned BAM to chunk
      i_ref_fn -- referenceset
      i_ref_regions_fn -- reference regions in txt, e.g., 'chr1;chr2' or an empty string meaning all
      o_json_fn -- chunk.json
    """
    o_aln_chunk_json = o_json_fn + ".aln.json"
    output_dir = op.dirname(o_json_fn)
    ref_chunks = chunk_aln_references(i_aln_fn=i_aln_fn, max_nchunks=max_nchunks)
    # Writing chunk.json
    chunks = []
    for i, ref_chunk in enumerate(ref_chunks):
        chunk_id = "_".join(["call_chunk", str(i)])
        # write ref regions in ref_chunk to a file
        ref_regions_chunk_fn = op.join(output_dir, chunk_id + '.txt')
        with open(ref_regions_chunk_fn, 'w') as writer:
            writer.write(';'.join(ref_chunk))
        # make dict
        d = {Constants.CHUNK_KEYS[0]: i_cfg_fn,
             Constants.CHUNK_KEYS[1]: i_aln_fn,
             Constants.CHUNK_KEYS[2]: i_ref_fn,
             Constants.CHUNK_KEYS[3]: ref_regions_chunk_fn}
        c = PipelineChunk(chunk_id, **d)
        chunks.append(c)

    log.info("Writing chunk.json to %s", o_json_fn)
    write_pipeline_chunks(chunks, o_json_fn, "created by %s" % Constants.TOOL_ID())
    return 0


def rtc_runner(rtc):
    """Resolved tool contract runner."""
    max_nchunks = rtc.task.max_nchunks if hasattr(rtc.task, 'max_nchunks') else Constants.DEFAULT_NCHUNKS
    return run_main(i_cfg_fn=rtc.task.input_files[0],
                    i_aln_fn=rtc.task.input_files[1],
                    i_ref_fn=rtc.task.input_files[2],
                    i_ref_regions_fn=rtc.task.input_files[3],
                    o_json_fn=rtc.task.output_files[0],
                    max_nchunks=int(max_nchunks))


if __name__ == '__main__':
    sys.exit(scatter_main(args=sys.argv[1:], const=Constants, rtc_runner=rtc_runner, alog=log))
