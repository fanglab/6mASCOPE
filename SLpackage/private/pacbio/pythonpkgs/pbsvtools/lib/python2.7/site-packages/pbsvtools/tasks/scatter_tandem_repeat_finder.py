#! python

"""
Scatter inputs of pbsvtools.call.

    Input: idx 0 - pbsv.cfg
           idx 1 - sorted_chained.bam
           idx 2 - referenceset.xml

"""
from __future__ import absolute_import

import logging
import sys
import os.path as op
import csv

from pbcommand.pb_io.common import load_pipeline_chunks_from_json
from pbcommand.pb_io import write_pipeline_chunks
from pbcommand.models import  FileTypes, PipelineChunk

from pbsv1.libs import AlignmentFile
import pbcoretools.chunking.chunk_utils as CU
from pbcoretools.chunking.gather import get_datum_from_chunks_by_chunk_key

from ..basic import scatter_main, TCP_INPUT, TCP_OUTPUT, BaseScatterConstants

from .scatter_call import put_items_to_boxes
from .split_ref_to_chrs import chrs_lens_to_csv
from .tandem_repeat_finder import csv_to_chrs_lens_dict

log = logging.getLogger(__name__)


class Constants(BaseScatterConstants):
    """Constants used for pbsvtools.tasks.tandem_repeat_finder
    Input: idx 0 - ReferenceSet XML
           idx 1 - csv with columns: chr,size
    Output: idx 0 - csv_id
    """

    TOOL_NAME = 'scatter_tandem_repeat_finder'
    DESCRIPTION = "Scatter tandem repeats finder per chromsome"
    CHUNK_KEYS = ('$chunk.ref_id', '$chunk.csv_id')

    INPUT_FILES = [
        TCP_INPUT(FileTypes.DS_REF, 'referenceset_in', 'Reference Set In', 'PacBio ReferenceSet'),
        TCP_INPUT(FileTypes.CSV, 'chrs_in', 'Chromosomes In', 'Chromosomes to apply TandemRepeatFinder'),
    ]

    OUTPUT_FILES = [
        TCP_OUTPUT(FileTypes.CHUNK, "cjson_out", "Chunk JSON tandem repeat finder tasks", "Chunk JSON tandem repeat finder tasks", "tandem_repeat_finder.chunked")
    ]


def chunk_chr_csv(i_csv_fn, max_nchunks):
    """Chunk reference chromosomes in i_csv_fn into no more than max_nchunks chunks based on length."""
    chrs = [r['chr'] for r in csv.DictReader(open(i_csv_fn, 'r'))]
    lens = [int(r['size']) for r in csv.DictReader(open(i_csv_fn, 'r'))]
    n_chunks = max(1, min(max_nchunks, len(chrs)))
    len_cutoff = int(sum(lens) / n_chunks) + 1
    chr_chunks = put_items_to_boxes(items=chrs, weights=lens, n=n_chunks, cutoff=len_cutoff)
    return chr_chunks


def run_main(i_ref_fn, i_csv_fn, o_json_fn, max_nchunks):
    """
    Parameters:
      i_ref_fn -- referenceset
      i_csv_fn -- csv with two columns 'chr,size'
      o_json_fn -- chunk.json
    """
    o_trf_chunk_json = o_json_fn + ".tandem_repeat_finder.json"
    output_dir = op.dirname(o_json_fn)
    chr_chunks = chunk_chr_csv(i_csv_fn=i_csv_fn, max_nchunks=max_nchunks)
    chrs_lens_d = csv_to_chrs_lens_dict(i_csv_fn=i_csv_fn)
    # Writing chunk.json
    chunks = []
    for i, chr_chunk in enumerate(chr_chunks):
        # each chr_chunk is a list of chrs., e.g., ['chr1', 'chr2']
        chunk_id = "_".join(["Tandem Repeat Finder chunk", str(i)])
        # write ref regions in ref_chunk to a file
        csv_chunk_fn = op.join(output_dir, chunk_id + '.csv')
        len_chunk = [chrs_lens_d[chrom] for chrom in chr_chunk]
        chrs_lens_to_csv(chrs=chr_chunk, lens=len_chunk, o_csv_fn=csv_chunk_fn)

        # make dict
        d = {Constants.CHUNK_KEYS[0]: i_ref_fn,
             Constants.CHUNK_KEYS[1]: csv_chunk_fn}
        c = PipelineChunk(chunk_id, **d)
        chunks.append(c)

    log.info("Writing chunk.json to %s", o_json_fn)
    write_pipeline_chunks(chunks, o_json_fn, "created by %s" % Constants.TOOL_ID())
    return 0


def rtc_runner(rtc):
    """Resolved tool contract runner."""
    max_nchunks = rtc.task.max_nchunks if hasattr(rtc.task, 'max_nchunks') else Constants.DEFAULT_NCHUNKS
    return run_main(i_ref_fn=rtc.task.input_files[0],
                    i_csv_fn=rtc.task.input_files[1],
                    o_json_fn=rtc.task.output_files[0],
                    max_nchunks=int(max_nchunks))


if __name__ == '__main__':
    sys.exit(scatter_main(args=sys.argv[1:], const=Constants, rtc_runner=rtc_runner, alog=log))
