#! python

"""
Scatter inputs of pbsvtools.align.

align takes two inputs:
    Input: idx 0 - pbsv.cfg
           idx 1 - subreadset.xml
           idx 2 - referenceset.xml
scatter_align chunks subreadset.
"""
from __future__ import absolute_import

import logging
import sys
import os.path as op

from pbcommand.pb_io.common import load_pipeline_chunks_from_json
from pbcommand.pb_io import write_pipeline_chunks
from pbcommand.models import  FileTypes, PipelineChunk

import pbcoretools.chunking.chunk_utils as CU
from pbcoretools.chunking.gather import get_datum_from_chunks_by_chunk_key

from pbsvtools.basic import scatter_main, TCP_INPUT, TCP_OUTPUT, BaseScatterConstants

log = logging.getLogger(__name__)


class Constants(BaseScatterConstants):
    """Constants used in pbsvtools.tasks.scatter_align"""
    TOOL_NAME = 'scatter_align'
    DESCRIPTION = "Scatter subreads for pbsvtools.tasks.align"
    CHUNK_KEYS = ('$chunk.config_id', '$chunk.subreadset_id', '$chunk.ref_id', '$chunk.json_id')

    INPUT_FILES = [
        TCP_INPUT(FileTypes.CFG, 'pbsv_config_in', 'pbsv Config In', 'pbsv Config'),
        TCP_INPUT(FileTypes.DS_SUBREADS, 'subreads_in', 'SubreadSet In', 'PacBio SubreadSet'),
        TCP_INPUT(FileTypes.DS_REF, 'referenceset_in', 'Reference Set In', 'PacBio ReferenceSet'),
        TCP_INPUT(FileTypes.JSON, 'movienames2samples_in', 'Movie names to samples JSON In', 'Link movie names to samples')
    ]
    OUTPUT_FILES = [
        TCP_OUTPUT(FileTypes.CHUNK, "cjson_out", "Chunk JSON sv align tasks", "Chunk JSON sv align tasks", "align.chunked")
    ]


def run_main(i_cfg_fn, i_sr_fn, i_ref_fn, i_m2s_json_fn, o_json_fn, max_nchunks):
    """
    Parameters:
      i_cfg_fn --- pbsv config
      i_sr_fn --- subreads to chunk
      i_ref_fn --- reference
      i_m2s_json_fn --- input json file containing a list of (movie, sample) tuples
      o_json_fn -- chunk.json
    """
    # Chunk subreads
    o_sr_chunk_json = o_json_fn + ".sr.json"
    output_dir = op.dirname(o_json_fn)

    CU.write_subreadset_zmw_chunks_to_file(
        chunk_file=o_sr_chunk_json, dataset_path=i_sr_fn,
        max_total_chunks=max_nchunks, dir_name=output_dir,
        chunk_base_name="chunk_subreads", chunk_ext=FileTypes.DS_SUBREADS.ext)

    # get subreadset chunks from o_sr_chunk_json
    sr_chunks = load_pipeline_chunks_from_json(o_sr_chunk_json)
    sr_fns = get_datum_from_chunks_by_chunk_key(sr_chunks, "$chunk.subreadset_id")
    log.debug("Chunked subreads files are %s.", (', '.join(sr_fns)))

    # Writing chunk.json
    chunks = []
    for i, sr_fn in enumerate(sr_fns):
        chunk_id = "_".join(["align_chunk", str(i)])
        d = {Constants.CHUNK_KEYS[0]: i_cfg_fn,
             Constants.CHUNK_KEYS[1]: sr_fn,
             Constants.CHUNK_KEYS[2]: i_ref_fn,
             Constants.CHUNK_KEYS[3]: i_m2s_json_fn}
        c = PipelineChunk(chunk_id, **d)
        chunks.append(c)

    log.info("Writing chunk.json to %s", o_json_fn)
    write_pipeline_chunks(chunks, o_json_fn,
                          "created by %s" % Constants.TOOL_ID())
    return 0


def rtc_runner(rtc):
    """Resolved tool contract runner."""
    max_nchunks = rtc.task.max_nchunks if hasattr(rtc.task, 'max_nchunks') else Constants.DEFAULT_NCHUNKS
    return run_main(i_cfg_fn=rtc.task.input_files[0],
                    i_sr_fn=rtc.task.input_files[1],
                    i_ref_fn=rtc.task.input_files[2],
                    i_m2s_json_fn=rtc.task.input_files[3],
                    o_json_fn=rtc.task.output_files[0],
                    max_nchunks=int(max_nchunks))


if __name__ == '__main__':
    sys.exit(scatter_main(args=sys.argv[1:], const=Constants, rtc_runner=rtc_runner, alog=log))
