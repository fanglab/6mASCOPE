#! python
"""
split the alignments in datastore json.
"""
from __future__ import absolute_import

import sys
import os
import copy
import uuid
import logging
from collections import defaultdict

from pbcommand.models import FileTypes, DataStore, DataStoreFile

from pbsv1.independent.utils import execute, _index_bam_cmd, realpath, _fns2fofn

from .scatter_align_json_to_svsig import datastore_to_bam_files
from ..basic import main, TCP_INPUT, TCP_OUTPUT, BaseConstants

log = logging.getLogger(__name__)


class Constants(BaseConstants):
    """Required task definitions
    Input: BAM Datastore
    Output: One aligned BAM
    """
    TOOL_NAME = 'merge_alignments_by_sample'
    DESCRIPTION = "For each sample, split all associated BAM files into a separate BAM file"
    INPUT_FILES = [
        TCP_INPUT(FileTypes.JSON, 'in_aln_json', 'Input datastore of Alignment Bam', 'align')
    ]
    OUTPUT_FILES = [
        TCP_OUTPUT(FileTypes.DATASTORE, 'out_aln_by_sample_json',
                   'Alignment set by sample',
                   'Datastore containing set of alignment files, one for each sample',
                   'alignments_by_sample')
    ]
    ALLOWED_TYPES = (FileTypes.DS_ALIGN, FileTypes.DS_ALIGN_CCS)


def dfile(sample, fname):
    """Convenience function for creating DataStoreFile for bam"""
    return DataStoreFile(uuid.uuid4(), Constants.TOOL_ID() + "-out-1",
                         FileTypes.BAM.file_type_id,
                         fname, name="Aligned reads (%s)" % sample,
                         description="Aligned reads for Sample - %s" % sample)


def bai_dfile(sample, fname):
    """Convenience function for creating DataStoreFile for bai"""
    return DataStoreFile(uuid.uuid4(), Constants.TOOL_ID() + "-out-2",
                         FileTypes.BAMBAI.file_type_id,
                         fname, name="Index of aligned reads (%s)" % sample,
                         description="Index of aligned reads for Sample - %s" % sample)


def check_or_make_bai(bam_file, nproc):
    """
    Check bai index of bam_file exists or not, if bai index file exists, return; if bai
    index file does not exist, generate bai index.
    """
    bai_file = bam_file + '.bai'
    if os.path.exists(bai_file):
        log.info("Bai index file {} already exist.".format(bai_file))
        return
    log.info("Generating bai index file {}.".format(bai_file))
    execute(_index_bam_cmd(bam_file, nproc))


def run_main(i_align_datastore, o_datastore_json, nproc):
    """
    Assumption: input i_align_datastore may contain multiple bam files, each must
    be associated with exactly one sample.

    For each sample, split assocaited alignments into a separate BAM.
    Creates a JSON DataStore file for consumption by the UI.
    """
    out_dir = os.path.dirname(realpath(o_datastore_json))
    out_prefix = os.path.join(out_dir, 'sample')
    if not os.access(out_dir, os.W_OK):
        log.error("Can't write to path: %r, failing split-by-sample", out_dir)
        return
    bam_files = datastore_to_bam_files(i_align_datastore)  # external bam files.

    # check input bam files, each must associated with exactly one sample, make bai
    sample_to_bam_files = defaultdict(list)
    for bam_file in bam_files:
        sample = get_only_sample_from_bam(bam_file)
        check_or_make_bai(bam_file, nproc)
        sample_to_bam_files[sample].append(bam_file)

    # Merge bam files by sample
    ds = []  # Create one bam per each sample
    for idx, sample in enumerate(sample_to_bam_files.keys()):
        # XXX Hacky workaround for Cromwell workflow output file names
        if "PB_WORKFLOW_CHUNK_ID" in os.environ:
            idx = "{}-{}".format(os.environ["PB_WORKFLOW_CHUNK_ID"], idx)
        o_bam_fn = '{}.{}.bam'.format(out_prefix, idx)
        o_fofn = o_bam_fn + '.fofn'
        _fns2fofn(sample_to_bam_files[sample], o_fofn)  # create bam fofn for samtools merge
        cmd = 'samtools merge -1 -c -b %s %s -f --threads %s' % (o_fofn, o_bam_fn, int(nproc))
        execute(cmd)
        check_or_make_bai(o_bam_fn, nproc)  # make bai index for output bam
        # create DataStoreFile objects
        ds.append(dfile(sample, o_bam_fn))
        ds.append(bai_dfile(sample, o_bam_fn + '.bai'))
    # create datastore referencing written output
    DataStore(ds).write_json(o_datastore_json)
    return 0


def get_only_sample_from_bam(i_bam):
    """Return a list of samples from bam by parsing pbindex-ed bam file"""
    from pbcore.io import IndexedBamReader
    import numpy as np
    reader = IndexedBamReader(i_bam)
    qids = list(np.unique(reader.index.qId))
    samples = set([rg.SampleName for rg in reader.readGroupTable if rg.ID in qids])
    if len(samples) > 1:
        raise ValueError("Bam file {!r} has more than one samples: {!r}".format(i_bam, samples))
    elif len(samples) == 0:  # fall back
        return 'UnnamedSample'
    return list(samples)[0]


def rtc_runner(rtc):
    return run_main(rtc.task.input_files[0], rtc.task.output_files[0], rtc.task.nproc)


def args_runner(args):
    return run_main(args.in_aln_json, args.out_aln_by_sample_json, args.nproc)


if __name__ == '__main__':
    sys.exit(main(args=sys.argv[1:], const=Constants, rtc_runner=rtc_runner, alog=log, args_runner=args_runner))
